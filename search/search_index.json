{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to probatus documentation! \u00b6 Probatus is a Python library that allows to analyse binary classification models as well as the data used to develop them. The main features assess the metric stability and analyse differences between two data samples e.g. shift between train and test splits. Installation \u00b6 In order to install Probatus you need to use Python 3.6 or higher. Install probatus via pip with: pip install probatus Alternatively you can fork/clone and run: git clone https://gitlab.com/ing_rpaa/probatus.git cd probatus pip install . For packages required for development, please refer to requirements.txt . Licence \u00b6 Probatus is created under MIT License, see more in LICENCE file .","title":"Index"},{"location":"index.html#welcome-to-probatus-documentation","text":"Probatus is a Python library that allows to analyse binary classification models as well as the data used to develop them. The main features assess the metric stability and analyse differences between two data samples e.g. shift between train and test splits.","title":"Welcome to probatus documentation!"},{"location":"index.html#installation","text":"In order to install Probatus you need to use Python 3.6 or higher. Install probatus via pip with: pip install probatus Alternatively you can fork/clone and run: git clone https://gitlab.com/ing_rpaa/probatus.git cd probatus pip install . For packages required for development, please refer to requirements.txt .","title":"Installation"},{"location":"index.html#licence","text":"Probatus is created under MIT License, see more in LICENCE file .","title":"Licence"},{"location":"api/feature_elimination.html","text":"Features Elimination \u00b6 This module allows to apply features elimination. \u00b6 feature_elimination \u00b6 ShapRFECV \u00b6 This class performs Backwards Recursive Feature Elimination, using SHAP features importance. At each round, for a given features set, starting from all available features, a model is optimized (e.g. using RandomSearchCV) and trained. At the end of each round, the n lowest SHAP feature importance features are removed and the model results are stored. The user can plot the performance of the model for each round, and select the optimal number of features and the features set. The functionality is similar to sklearn.feature_selection.RFECV , yet, it removes the lowest importance features based on SHAP features importance and optimizes the hyperparameters of the model at each round. We recommend using LightGBM model, because by default it handles missing values and categorical features. In case of other models, make sure to handle these issues for your dataset and consider impact it might have on features importance. Examples: from probatus.feature_elimination import ShapRFECV from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split import numpy as np import pandas as pd import lightgbm feature_names = [ 'f1_categorical' , 'f2_missing' , 'f3_static' , 'f4' , 'f5' , 'f6' , 'f7' , 'f8' , 'f9' , 'f10' , 'f11' , 'f12' , 'f13' , 'f14' , 'f15' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , n_features = 15 , random_state = 0 , n_redundant = 10 ) X = pd . DataFrame ( X , columns = feature_names ) X [ 'f1_categorical' ] = X [ 'f1_categorical' ] . apply ( lambda x : str ( np . round ( x * 10 ))) X [ 'f2_missing' ] = X [ 'f2_missing' ] . apply ( lambda x : x if np . random . rand () < 0.8 else np . nan ) X [ 'f3_static' ] = 0 # Prepare model and parameter search space clf = lightgbm . LGBMClassifier ( max_depth = 5 , class_weight = 'balanced' ) param_grid = { 'n_estimators' : [ 5 , 7 , 10 ], 'num_leaves' : [ 3 , 5 , 7 ], } # Run feature elimination shap_elimination = ShapRFECV ( clf = clf , search_space = param_grid , search_schema = 'grid' , step = 0.2 , cv = 20 , scoring = 'roc_auc' , n_jobs = 3 , random_state = 42 ) report = shap_elimination . fit_compute ( X , y ) # Make plots shap_elimination . plot ( 'performance' ) shap_elimination . plot ( 'parameter' , param_names = [ 'n_estimators' , 'num_leaves' ]) # Get final features set final_features_set = shap_elimination . get_reduced_features_set ( num_features = 2 ) __init__ ( self , clf , search_space , search_schema = 'random' , step = 1 , min_features_to_select = 1 , random_state = None , ** search_kwargs ) special \u00b6 This method initializes the class: Parameters: Name Type Description Default clf binary classifier A model that will be optimized and trained at each round of features elimination. The recommended model is LightGBM, because it by default handles the missing values and categorical variables. required search_space dict of sklearn.ParamGrid Parameter search space, which will be explored during the hyperparameter search. In case grid search_schema, it is passed to GridSearchCV as param_grid , in case of random search_schema, then this value is passed to RandomSearchCV as param_distributions parameter. required search_schema Optional, str The hyperparameter search algorithm that should be used to optimize the model. It can be one of the following: random : RandomSearchCV which randomly selects hyperparameters from the prowided param_grid, and performs optimization using Cross-Validation. It is recommended option, when you optimize a large number of hyperparameters. grid : GridSearchCV which searches through all permutations of hyperparameters values from param_grid, and performs optimization using Cross-Validation. It is recommended option, for low number of hyperparameters. 'random' step Optional, int or float Number of lowest importance features removed each round. If it is an int, then each round such number of features is discarded. If float, such percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise towards less features. Note: the last round may remove fewer features in order to reach min_features_to_select. 1 min_features_to_select Optional, unt Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. 1 random_state Optional, int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None **search_kwargs The keywords arguments passed to a given search schema, during initialization. Please refer to the parameters of a given search schema. {} Source code in probatus/feature_elimination/feature_elimination.py def __init__ ( self , clf , search_space , search_schema = 'random' , step = 1 , min_features_to_select = 1 , random_state = None , ** search_kwargs ): \"\"\" This method initializes the class: Args: clf (binary classifier): A model that will be optimized and trained at each round of features elimination. The recommended model is LightGBM, because it by default handles the missing values and categorical variables. search_space (dict of sklearn.ParamGrid): Parameter search space, which will be explored during the hyperparameter search. In case `grid` search_schema, it is passed to GridSearchCV as `param_grid`, in case of `random` search_schema, then this value is passed to RandomSearchCV as `param_distributions` parameter. search_schema (Optional, str): The hyperparameter search algorithm that should be used to optimize the model. It can be one of the following: - `random`: [RandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) which randomly selects hyperparameters from the prowided param_grid, and performs optimization using Cross-Validation. It is recommended option, when you optimize a large number of hyperparameters. - `grid`: [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) which searches through all permutations of hyperparameters values from param_grid, and performs optimization using Cross-Validation. It is recommended option, for low number of hyperparameters. step (Optional, int or float): Number of lowest importance features removed each round. If it is an int, then each round such number of features is discarded. If float, such percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise towards less features. Note: the last round may remove fewer features in order to reach min_features_to_select. min_features_to_select (Optional, unt): Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. random_state (Optional, int): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. **search_kwargs: The keywords arguments passed to a given search schema, during initialization. Please refer to the parameters of a given search schema. \"\"\" self . clf = clf if search_schema == 'random' : self . search_class = RandomizedSearchCV elif search_schema == 'grid' : self . search_class = GridSearchCV else : raise ( ValueError ( 'Unsupported search_schema, choose one of the following: \"random\", \"grid\".' )) self . search_space = search_space self . search_kwargs = search_kwargs if ( isinstance ( step , int ) or isinstance ( step , float )) and \\ step > 0 : self . step = step else : raise ( ValueError ( f \"The current value of step = { step } is not allowed. \" f \"It needs to be a positive integer or positive float.\" )) if isinstance ( min_features_to_select , int ) and min_features_to_select > 0 : self . min_features_to_select = min_features_to_select else : raise ( ValueError ( f \"The current value of min_features_to_select = { min_features_to_select } is not allowed. \" f \"It needs to be a positive integer.\" )) self . random_state = random_state self . report_df = pd . DataFrame ([]) self . fitted = False compute ( self ) \u00b6 Checks if fit() method has been run and computes the DataFrame with results of feature elimintation for each round. Returns: Type Description (pd.DataFrame) DataFrame with results of feature elimination for each round. Source code in probatus/feature_elimination/feature_elimination.py def compute ( self ): \"\"\" Checks if fit() method has been run and computes the DataFrame with results of feature elimintation for each round. Returns: (pd.DataFrame): DataFrame with results of feature elimination for each round. \"\"\" self . _check_if_fitted () return self . report_df fit ( self , X , y ) \u00b6 Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. At each step, it optimizes hyperparameters of the model, computes SHAP features importance and removes the lowest importance features. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required Source code in probatus/feature_elimination/feature_elimination.py def fit ( self , X , y ): \"\"\" Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. At each step, it optimizes hyperparameters of the model, computes SHAP features importance and removes the lowest importance features. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. \"\"\" # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) self . X = self . _preprocess_data ( X ) self . y = y remaining_features = current_features_set = self . X . columns . tolist () round_number = 0 while len ( current_features_set ) > self . min_features_to_select : round_number += 1 # Get current dataset info current_features_set = remaining_features current_X = self . X [ current_features_set ] # Optimize parameters # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) search = self . search_class ( self . clf , self . search_space , refit = True , return_train_score = True , ** self . search_kwargs ) search = search . fit ( current_X , y ) # Compute SHAP values shap_values = shap_calc ( search . best_estimator_ , current_X , suppress_warnings = True ) shap_importance_df = calculate_shap_importance ( shap_values , remaining_features ) # Get features to remove features_to_remove = self . _get_current_features_to_remove ( shap_importance_df ) remaining_features = list ( set ( current_features_set ) - set ( features_to_remove )) # Report results self . _report_current_results ( round_number = round_number , current_features_set = current_features_set , features_to_remove = features_to_remove , search = search ) print ( f 'Round: { round_number } , Current number of features: { len ( current_features_set ) } , ' f 'Current performance: Train { self . report_df . loc [ round_number ][ \"train_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"train_metric_std\" ] } , CV Validation ' f ' { self . report_df . loc [ round_number ][ \"val_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"val_metric_std\" ] } . \\n ' f 'Num of features left: { len ( remaining_features ) } . ' f 'Removed features at the end of the round: { features_to_remove } ' ) self . fitted = True fit_compute ( self , X , y ) \u00b6 Fits the object and computes the report. The algorithm starts with the entire dataset, and then sequentially eliminates features. At each step, it optimizes hyperparameters of the model, computes SHAP features importance and removes the lowest importance features. At the end, the report containing results from each iteration is computed and returned to the user. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required Returns: Type Description (pd.DataFrame) DataFrame containing results of feature elimination from each iteration. Source code in probatus/feature_elimination/feature_elimination.py def fit_compute ( self , X , y ): \"\"\" Fits the object and computes the report. The algorithm starts with the entire dataset, and then sequentially eliminates features. At each step, it optimizes hyperparameters of the model, computes SHAP features importance and removes the lowest importance features. At the end, the report containing results from each iteration is computed and returned to the user. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. Returns: (pd.DataFrame): DataFrame containing results of feature elimination from each iteration. \"\"\" self . fit ( X , y ) return self . compute () get_reduced_features_set ( self , num_features ) \u00b6 Gets the features set after the feature elimination process, for a given number of features. Parameters: Name Type Description Default num_features int Number of features in the reduced features set. required Returns: Type Description (list of str) Reduced features set. Source code in probatus/feature_elimination/feature_elimination.py def get_reduced_features_set ( self , num_features ): \"\"\" Gets the features set after the feature elimination process, for a given number of features. Args: num_features (int): Number of features in the reduced features set. Returns: (list of str): Reduced features set. \"\"\" self . _check_if_fitted () if num_features not in self . report_df . num_features . tolist (): raise ( ValueError ( f 'The provided number of features has not been achieved at any stage of the process. ' f 'You can select one of the following: { self . report_df . num_features . tolist () } ' )) else : return self . report_df [ self . report_df . num_features == num_features ][ 'features_set' ] . values [ 0 ] plot ( self , plot_type = 'performance' , param_names = None , show = True , ** figure_kwargs ) \u00b6 Generates plots that allow to analyse the results. Parameters: Name Type Description Default plot_type Optional, str String indicating the plot type: performance : Performance of the optimized model at each iteration. This plot allows to select the optimal features set. parameter : Plots the optimized hyperparameter's values at each iteration. This plot allows to analyse stability of parameters for different features set. In case large variability of optimal hyperparameters values is seen, consider reducing the search space. 'performance' param_names Optional, str, list of str Name or names of parameters that will be plotted in case of plot_type=\"parameter\" None show Optional, bool If True, the plots are showed to the user, otherwise they are not shown. True **figure_kwargs Keyword arguments that are passed to the plt.figure, at its initialization. {} Returns: Type Description Plot (plt.axis or list of plt.axis) Axis containing the target plot, or list of such axes. Source code in probatus/feature_elimination/feature_elimination.py def plot ( self , plot_type = 'performance' , param_names = None , show = True , ** figure_kwargs ): \"\"\" Generates plots that allow to analyse the results. Args: plot_type (Optional, str): String indicating the plot type: - `performance`: Performance of the optimized model at each iteration. This plot allows to select the optimal features set. - `parameter`: Plots the optimized hyperparameter's values at each iteration. This plot allows to analyse stability of parameters for different features set. In case large variability of optimal hyperparameters values is seen, consider reducing the search space. param_names (Optional, str, list of str): Name or names of parameters that will be plotted in case of `plot_type=\"parameter\"` show (Optional, bool): If True, the plots are showed to the user, otherwise they are not shown. **figure_kwargs: Keyword arguments that are passed to the plt.figure, at its initialization. Returns: Plot (plt.axis or list of plt.axis): Axis containing the target plot, or list of such axes. \"\"\" x_ticks = list ( reversed ( self . report_df [ 'num_features' ] . tolist ())) if plot_type == 'performance' : plt . figure ( ** figure_kwargs ) plt . plot ( self . report_df [ 'num_features' ], self . report_df [ 'train_metric_mean' ], label = 'Train Score' ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = 'coerce' ), self . report_df [ 'train_metric_mean' ] - self . report_df [ 'train_metric_std' ], self . report_df [ 'train_metric_mean' ] + self . report_df [ 'train_metric_std' ], alpha =. 3 ) plt . plot ( self . report_df [ 'num_features' ], self . report_df [ 'val_metric_mean' ], label = 'Validation Score' ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = 'coerce' ), self . report_df [ 'val_metric_mean' ] - self . report_df [ 'val_metric_std' ], self . report_df [ 'val_metric_mean' ] + self . report_df [ 'val_metric_std' ], alpha =. 3 ) plt . xlabel ( 'Number of features' ) plt . ylabel ( 'Performance' ) plt . title ( 'Backwards Feature Elimination using SHAP & CV' ) plt . legend ( loc = \"lower left\" ) ax = plt . gca () ax . invert_xaxis () ax . set_xticks ( x_ticks ) if show : plt . show () else : plt . close () elif plot_type == 'parameter' : param_names = assure_list_of_strings ( param_names , 'target_columns' ) ax = [] for param_name in param_names : plt . figure ( ** figure_kwargs ) plt . plot ( self . report_df [ 'num_features' ], self . report_df [ f 'param_ { param_name } ' ], label = f ' { param_name } optimized value' ) plt . xlabel ( 'Number of features' ) plt . ylabel ( f 'Optimizal { param_name } value' ) plt . title ( f 'Optimization of { param_name } for different numbers of features' ) plt . legend ( loc = \"lower left\" ) current_ax = plt . gca () current_ax . invert_xaxis () current_ax . set_xticks ( x_ticks ) ax . append ( current_ax ) if show : plt . show () else : plt . close () else : raise ( ValueError ( 'Wrong value of plot_type. Select from \"performance\" or \"parameter\"' )) if isinstance ( ax , list ) and len ( ax ) == 1 : ax = ax [ 0 ] return ax","title":"probatus.feature_elimination"},{"location":"api/feature_elimination.html#features-elimination","text":"This module allows to apply features elimination.","title":"Features Elimination"},{"location":"api/feature_elimination.html#probatus.feature_elimination","text":"","title":"probatus.feature_elimination"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination","text":"","title":"feature_elimination"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV","text":"This class performs Backwards Recursive Feature Elimination, using SHAP features importance. At each round, for a given features set, starting from all available features, a model is optimized (e.g. using RandomSearchCV) and trained. At the end of each round, the n lowest SHAP feature importance features are removed and the model results are stored. The user can plot the performance of the model for each round, and select the optimal number of features and the features set. The functionality is similar to sklearn.feature_selection.RFECV , yet, it removes the lowest importance features based on SHAP features importance and optimizes the hyperparameters of the model at each round. We recommend using LightGBM model, because by default it handles missing values and categorical features. In case of other models, make sure to handle these issues for your dataset and consider impact it might have on features importance. Examples: from probatus.feature_elimination import ShapRFECV from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split import numpy as np import pandas as pd import lightgbm feature_names = [ 'f1_categorical' , 'f2_missing' , 'f3_static' , 'f4' , 'f5' , 'f6' , 'f7' , 'f8' , 'f9' , 'f10' , 'f11' , 'f12' , 'f13' , 'f14' , 'f15' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , n_features = 15 , random_state = 0 , n_redundant = 10 ) X = pd . DataFrame ( X , columns = feature_names ) X [ 'f1_categorical' ] = X [ 'f1_categorical' ] . apply ( lambda x : str ( np . round ( x * 10 ))) X [ 'f2_missing' ] = X [ 'f2_missing' ] . apply ( lambda x : x if np . random . rand () < 0.8 else np . nan ) X [ 'f3_static' ] = 0 # Prepare model and parameter search space clf = lightgbm . LGBMClassifier ( max_depth = 5 , class_weight = 'balanced' ) param_grid = { 'n_estimators' : [ 5 , 7 , 10 ], 'num_leaves' : [ 3 , 5 , 7 ], } # Run feature elimination shap_elimination = ShapRFECV ( clf = clf , search_space = param_grid , search_schema = 'grid' , step = 0.2 , cv = 20 , scoring = 'roc_auc' , n_jobs = 3 , random_state = 42 ) report = shap_elimination . fit_compute ( X , y ) # Make plots shap_elimination . plot ( 'performance' ) shap_elimination . plot ( 'parameter' , param_names = [ 'n_estimators' , 'num_leaves' ]) # Get final features set final_features_set = shap_elimination . get_reduced_features_set ( num_features = 2 )","title":"ShapRFECV"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.__init__","text":"This method initializes the class: Parameters: Name Type Description Default clf binary classifier A model that will be optimized and trained at each round of features elimination. The recommended model is LightGBM, because it by default handles the missing values and categorical variables. required search_space dict of sklearn.ParamGrid Parameter search space, which will be explored during the hyperparameter search. In case grid search_schema, it is passed to GridSearchCV as param_grid , in case of random search_schema, then this value is passed to RandomSearchCV as param_distributions parameter. required search_schema Optional, str The hyperparameter search algorithm that should be used to optimize the model. It can be one of the following: random : RandomSearchCV which randomly selects hyperparameters from the prowided param_grid, and performs optimization using Cross-Validation. It is recommended option, when you optimize a large number of hyperparameters. grid : GridSearchCV which searches through all permutations of hyperparameters values from param_grid, and performs optimization using Cross-Validation. It is recommended option, for low number of hyperparameters. 'random' step Optional, int or float Number of lowest importance features removed each round. If it is an int, then each round such number of features is discarded. If float, such percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise towards less features. Note: the last round may remove fewer features in order to reach min_features_to_select. 1 min_features_to_select Optional, unt Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. 1 random_state Optional, int Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. None **search_kwargs The keywords arguments passed to a given search schema, during initialization. Please refer to the parameters of a given search schema. {} Source code in probatus/feature_elimination/feature_elimination.py def __init__ ( self , clf , search_space , search_schema = 'random' , step = 1 , min_features_to_select = 1 , random_state = None , ** search_kwargs ): \"\"\" This method initializes the class: Args: clf (binary classifier): A model that will be optimized and trained at each round of features elimination. The recommended model is LightGBM, because it by default handles the missing values and categorical variables. search_space (dict of sklearn.ParamGrid): Parameter search space, which will be explored during the hyperparameter search. In case `grid` search_schema, it is passed to GridSearchCV as `param_grid`, in case of `random` search_schema, then this value is passed to RandomSearchCV as `param_distributions` parameter. search_schema (Optional, str): The hyperparameter search algorithm that should be used to optimize the model. It can be one of the following: - `random`: [RandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) which randomly selects hyperparameters from the prowided param_grid, and performs optimization using Cross-Validation. It is recommended option, when you optimize a large number of hyperparameters. - `grid`: [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) which searches through all permutations of hyperparameters values from param_grid, and performs optimization using Cross-Validation. It is recommended option, for low number of hyperparameters. step (Optional, int or float): Number of lowest importance features removed each round. If it is an int, then each round such number of features is discarded. If float, such percentage of remaining features (rounded down) is removed each iteration. It is recommended to use float, since it is faster for a large number of features, and slows down and becomes more precise towards less features. Note: the last round may remove fewer features in order to reach min_features_to_select. min_features_to_select (Optional, unt): Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By default the process stops when one feature is left. random_state (Optional, int): Random state set at each round of feature elimination. If it is None, the results will not be reproducible and in random search at each iteration a different hyperparameters might be tested. For reproducible results set it to integer. **search_kwargs: The keywords arguments passed to a given search schema, during initialization. Please refer to the parameters of a given search schema. \"\"\" self . clf = clf if search_schema == 'random' : self . search_class = RandomizedSearchCV elif search_schema == 'grid' : self . search_class = GridSearchCV else : raise ( ValueError ( 'Unsupported search_schema, choose one of the following: \"random\", \"grid\".' )) self . search_space = search_space self . search_kwargs = search_kwargs if ( isinstance ( step , int ) or isinstance ( step , float )) and \\ step > 0 : self . step = step else : raise ( ValueError ( f \"The current value of step = { step } is not allowed. \" f \"It needs to be a positive integer or positive float.\" )) if isinstance ( min_features_to_select , int ) and min_features_to_select > 0 : self . min_features_to_select = min_features_to_select else : raise ( ValueError ( f \"The current value of min_features_to_select = { min_features_to_select } is not allowed. \" f \"It needs to be a positive integer.\" )) self . random_state = random_state self . report_df = pd . DataFrame ([]) self . fitted = False","title":"__init__()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.compute","text":"Checks if fit() method has been run and computes the DataFrame with results of feature elimintation for each round. Returns: Type Description (pd.DataFrame) DataFrame with results of feature elimination for each round. Source code in probatus/feature_elimination/feature_elimination.py def compute ( self ): \"\"\" Checks if fit() method has been run and computes the DataFrame with results of feature elimintation for each round. Returns: (pd.DataFrame): DataFrame with results of feature elimination for each round. \"\"\" self . _check_if_fitted () return self . report_df","title":"compute()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.fit","text":"Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. At each step, it optimizes hyperparameters of the model, computes SHAP features importance and removes the lowest importance features. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required Source code in probatus/feature_elimination/feature_elimination.py def fit ( self , X , y ): \"\"\" Fits the object with the provided data. The algorithm starts with the entire dataset, and then sequentially eliminates features. At each step, it optimizes hyperparameters of the model, computes SHAP features importance and removes the lowest importance features. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. \"\"\" # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) self . X = self . _preprocess_data ( X ) self . y = y remaining_features = current_features_set = self . X . columns . tolist () round_number = 0 while len ( current_features_set ) > self . min_features_to_select : round_number += 1 # Get current dataset info current_features_set = remaining_features current_X = self . X [ current_features_set ] # Optimize parameters # Set seed for results reproducibility if self . random_state is not None : np . random . seed ( self . random_state ) search = self . search_class ( self . clf , self . search_space , refit = True , return_train_score = True , ** self . search_kwargs ) search = search . fit ( current_X , y ) # Compute SHAP values shap_values = shap_calc ( search . best_estimator_ , current_X , suppress_warnings = True ) shap_importance_df = calculate_shap_importance ( shap_values , remaining_features ) # Get features to remove features_to_remove = self . _get_current_features_to_remove ( shap_importance_df ) remaining_features = list ( set ( current_features_set ) - set ( features_to_remove )) # Report results self . _report_current_results ( round_number = round_number , current_features_set = current_features_set , features_to_remove = features_to_remove , search = search ) print ( f 'Round: { round_number } , Current number of features: { len ( current_features_set ) } , ' f 'Current performance: Train { self . report_df . loc [ round_number ][ \"train_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"train_metric_std\" ] } , CV Validation ' f ' { self . report_df . loc [ round_number ][ \"val_metric_mean\" ] } ' f '+/- { self . report_df . loc [ round_number ][ \"val_metric_std\" ] } . \\n ' f 'Num of features left: { len ( remaining_features ) } . ' f 'Removed features at the end of the round: { features_to_remove } ' ) self . fitted = True","title":"fit()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.fit_compute","text":"Fits the object and computes the report. The algorithm starts with the entire dataset, and then sequentially eliminates features. At each step, it optimizes hyperparameters of the model, computes SHAP features importance and removes the lowest importance features. At the end, the report containing results from each iteration is computed and returned to the user. Parameters: Name Type Description Default X pd.DataFrame Provided dataset. required y pd.Series Binary labels for X. required Returns: Type Description (pd.DataFrame) DataFrame containing results of feature elimination from each iteration. Source code in probatus/feature_elimination/feature_elimination.py def fit_compute ( self , X , y ): \"\"\" Fits the object and computes the report. The algorithm starts with the entire dataset, and then sequentially eliminates features. At each step, it optimizes hyperparameters of the model, computes SHAP features importance and removes the lowest importance features. At the end, the report containing results from each iteration is computed and returned to the user. Args: X (pd.DataFrame): Provided dataset. y (pd.Series): Binary labels for X. Returns: (pd.DataFrame): DataFrame containing results of feature elimination from each iteration. \"\"\" self . fit ( X , y ) return self . compute ()","title":"fit_compute()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.get_reduced_features_set","text":"Gets the features set after the feature elimination process, for a given number of features. Parameters: Name Type Description Default num_features int Number of features in the reduced features set. required Returns: Type Description (list of str) Reduced features set. Source code in probatus/feature_elimination/feature_elimination.py def get_reduced_features_set ( self , num_features ): \"\"\" Gets the features set after the feature elimination process, for a given number of features. Args: num_features (int): Number of features in the reduced features set. Returns: (list of str): Reduced features set. \"\"\" self . _check_if_fitted () if num_features not in self . report_df . num_features . tolist (): raise ( ValueError ( f 'The provided number of features has not been achieved at any stage of the process. ' f 'You can select one of the following: { self . report_df . num_features . tolist () } ' )) else : return self . report_df [ self . report_df . num_features == num_features ][ 'features_set' ] . values [ 0 ]","title":"get_reduced_features_set()"},{"location":"api/feature_elimination.html#probatus.feature_elimination.feature_elimination.ShapRFECV.plot","text":"Generates plots that allow to analyse the results. Parameters: Name Type Description Default plot_type Optional, str String indicating the plot type: performance : Performance of the optimized model at each iteration. This plot allows to select the optimal features set. parameter : Plots the optimized hyperparameter's values at each iteration. This plot allows to analyse stability of parameters for different features set. In case large variability of optimal hyperparameters values is seen, consider reducing the search space. 'performance' param_names Optional, str, list of str Name or names of parameters that will be plotted in case of plot_type=\"parameter\" None show Optional, bool If True, the plots are showed to the user, otherwise they are not shown. True **figure_kwargs Keyword arguments that are passed to the plt.figure, at its initialization. {} Returns: Type Description Plot (plt.axis or list of plt.axis) Axis containing the target plot, or list of such axes. Source code in probatus/feature_elimination/feature_elimination.py def plot ( self , plot_type = 'performance' , param_names = None , show = True , ** figure_kwargs ): \"\"\" Generates plots that allow to analyse the results. Args: plot_type (Optional, str): String indicating the plot type: - `performance`: Performance of the optimized model at each iteration. This plot allows to select the optimal features set. - `parameter`: Plots the optimized hyperparameter's values at each iteration. This plot allows to analyse stability of parameters for different features set. In case large variability of optimal hyperparameters values is seen, consider reducing the search space. param_names (Optional, str, list of str): Name or names of parameters that will be plotted in case of `plot_type=\"parameter\"` show (Optional, bool): If True, the plots are showed to the user, otherwise they are not shown. **figure_kwargs: Keyword arguments that are passed to the plt.figure, at its initialization. Returns: Plot (plt.axis or list of plt.axis): Axis containing the target plot, or list of such axes. \"\"\" x_ticks = list ( reversed ( self . report_df [ 'num_features' ] . tolist ())) if plot_type == 'performance' : plt . figure ( ** figure_kwargs ) plt . plot ( self . report_df [ 'num_features' ], self . report_df [ 'train_metric_mean' ], label = 'Train Score' ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = 'coerce' ), self . report_df [ 'train_metric_mean' ] - self . report_df [ 'train_metric_std' ], self . report_df [ 'train_metric_mean' ] + self . report_df [ 'train_metric_std' ], alpha =. 3 ) plt . plot ( self . report_df [ 'num_features' ], self . report_df [ 'val_metric_mean' ], label = 'Validation Score' ) plt . fill_between ( pd . to_numeric ( self . report_df . num_features , errors = 'coerce' ), self . report_df [ 'val_metric_mean' ] - self . report_df [ 'val_metric_std' ], self . report_df [ 'val_metric_mean' ] + self . report_df [ 'val_metric_std' ], alpha =. 3 ) plt . xlabel ( 'Number of features' ) plt . ylabel ( 'Performance' ) plt . title ( 'Backwards Feature Elimination using SHAP & CV' ) plt . legend ( loc = \"lower left\" ) ax = plt . gca () ax . invert_xaxis () ax . set_xticks ( x_ticks ) if show : plt . show () else : plt . close () elif plot_type == 'parameter' : param_names = assure_list_of_strings ( param_names , 'target_columns' ) ax = [] for param_name in param_names : plt . figure ( ** figure_kwargs ) plt . plot ( self . report_df [ 'num_features' ], self . report_df [ f 'param_ { param_name } ' ], label = f ' { param_name } optimized value' ) plt . xlabel ( 'Number of features' ) plt . ylabel ( f 'Optimizal { param_name } value' ) plt . title ( f 'Optimization of { param_name } for different numbers of features' ) plt . legend ( loc = \"lower left\" ) current_ax = plt . gca () current_ax . invert_xaxis () current_ax . set_xticks ( x_ticks ) ax . append ( current_ax ) if show : plt . show () else : plt . close () else : raise ( ValueError ( 'Wrong value of plot_type. Select from \"performance\" or \"parameter\"' )) if isinstance ( ax , list ) and len ( ax ) == 1 : ax = ax [ 0 ] return ax","title":"plot()"},{"location":"api/metric_volatility.html","text":"Metric Volatility \u00b6 The aim of this module is analysis of how well a model performs on a given dataset, and how stable the performance is. The following features are implemented: BaseVolatilityEstimator - Base class, provides main functionality with fit method that can be overwritten by subclasses TrainTestVolatility - Estimation of volatility of metrics. The estimation is done by splitting the data into train and test multiple times and training and scoring a model based on these metrics. SplitSeedVolatility - Estimates volatility of metrics based on splitting the data into train and test sets multiple times randomly, each time with different seed. BootstrappedVolatility - stimates volatility of metrics based on splitting the data into train and test with static seed, and bootstrapping train and test set. \u00b6 BaseVolatilityEstimator \u00b6 Base object for estimating volatility estimation. This class is a base class, therefore cannot be used on its own. Implements common API that can be used by all subclasses. __init__ ( self , model , metrics = 'roc_auc' , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , random_state = 42 ) special \u00b6 Initializes the class Parameters: Name Type Description Default model model object Binary classification model or pipeline. required metrics string, list of strings, Scorer or list of Scorers Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the sklearn documentation . In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. 'roc_auc' test_prc float Percentage of input data used as test. By default 0.25. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 stats_tests_to_apply None, string or list of strings List of tests to apply. Available options: 'ES': Epps-Singleton, 'KS': Kolmogorov-Smirnov statistic, 'PSI': Population Stability Index, 'SW': Shapiro-Wilk based difference statistic, 'AD': Anderson-Darling TS. None random_state int The seed used by the random number generator. 42 Source code in probatus/metric_volatility/volatility.py def __init__ ( self , model , metrics = 'roc_auc' , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , random_state = 42 ): \"\"\" Initializes the class Args: model (model object): Binary classification model or pipeline. metrics (string, list of strings, Scorer or list of Scorers): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html). In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. test_prc (float, optional): Percentage of input data used as test. By default 0.25. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply (None, string or list of strings, optional): List of tests to apply. Available options: - 'ES': Epps-Singleton, - 'KS': Kolmogorov-Smirnov statistic, - 'PSI': Population Stability Index, - 'SW': Shapiro-Wilk based difference statistic, - 'AD': Anderson-Darling TS. random_state (int, optional): The seed used by the random number generator. \"\"\" self . model = model self . n_jobs = n_jobs self . random_state = random_state self . test_prc = test_prc self . iterations_results = None self . report = None self . fitted = False self . allowed_stats_tests = list ( DistributionStatistics . statistical_test_dict . keys ()) # TODO set reasonable default value for the parameter, to choose the statistical test for the user for different # ways to compute volatility if stats_tests_to_apply is not None : self . stats_tests_to_apply = assure_list_of_strings ( stats_tests_to_apply , 'stats_tests_to_apply' ) assure_list_values_allowed ( variable = self . stats_tests_to_apply , variable_name = 'stats_tests_to_apply' , allowed_values = self . allowed_stats_tests ) else : self . stats_tests_to_apply = [] self . stats_tests_objects = [] if len ( self . stats_tests_to_apply ) > 0 : warnings . warn ( \"Computing statistics for distributions is an experimental feature. While using it, keep in \" \"mind that the samples of metrics might be correlated.\" ) for test_name in self . stats_tests_to_apply : self . stats_tests_objects . append ( DistributionStatistics ( statistical_test = test_name )) self . scorers = get_scorers ( metrics ) compute ( self , metrics = None ) \u00b6 Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" if self . fitted is False : raise ( NotFittedError ( 'The object has not been fitted. Please run fit() method first' )) if self . report is None : raise ( ValueError ( 'Report is None, thus it has not been computed by fit method. Please extend the ' 'BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()' )) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ] fit ( self , * args , ** kwargs ) \u00b6 Base fit functionality that should be executed before each fit. Source code in probatus/metric_volatility/volatility.py def fit ( self , * args , ** kwargs ): \"\"\" Base fit functionality that should be executed before each fit. \"\"\" # Set seed for results reproducibility np . random . seed ( self . random_state ) # Initialize the report and results self . iterations_results = None self . report = None self . fitted = True fit_compute ( self , * args , ** kwargs ) \u00b6 Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute () plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ) \u00b6 Plots distribution of the metric Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ): \"\"\" Plots distribution of the metric Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ])) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = 'Train {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = 'Test {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( 'Distributions {} ' . format ( metric )) axs [ axis_index ] . legend ( loc = 'upper right' ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = 'Delta {} ' . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( 'Distributions delta {} ' . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = 'upper right' ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = ' {} score' . format ( metric ), ylabel = 'Results count' ) BootstrappedVolatility \u00b6 Estimation of volatility of metrics by bootstrapping both train and test set. By default at every iteration the train test split is the same. The test shows volatility of metric with regards to sampling different rows from static train and test sets. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import BootstrappedVolatility X , y = make_classification ( n_features = 4 ) RandomForestClassifier () volatility = BootstrappedVolatility ( clf , iterations = 500 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot () __init__ ( self , model , ** kwargs ) special \u00b6 Initializes the class. Parameters: Name Type Description Default model model object Binary classification model or pipeline. required **kwargs Keyword arguments that can be overwrite the default parameters from TrainTestVolatility: iterations : Number of iterations in seed bootstrapping. By default 1000. train_sampling_fraction : Fraction of train data sampled, if sample_train_type is not None. Default value is 1. test_sampling_fraction : Fraction of test data sampled, if sample_test_type is not None. Default value is 1. metrics : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the sklearn documentation . In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. test_prc : Percentage of input data used as test. By default 0.25. n_jobs : Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply : List of tests to apply. Available options described in BaseVolatilityEstimator. random_state : The seed used by the random number generator. {} Source code in probatus/metric_volatility/volatility.py def __init__ ( self , model , ** kwargs ): \"\"\" Initializes the class. Args: model (model object): Binary classification model or pipeline. **kwargs: Keyword arguments that can be overwrite the default parameters from TrainTestVolatility: - `iterations`: Number of iterations in seed bootstrapping. By default 1000. - `train_sampling_fraction`: Fraction of train data sampled, if sample_train_type is not None. Default value is 1. - `test_sampling_fraction`: Fraction of test data sampled, if sample_test_type is not None. Default value is 1. - `metrics` : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html). In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. - `test_prc`: Percentage of input data used as test. By default 0.25. - `n_jobs`: Number of parallel executions. If -1 use all available cores. By default 1. - `stats_tests_to_apply`: List of tests to apply. Available options described in BaseVolatilityEstimator. - `random_state`: The seed used by the random number generator. \"\"\" super () . __init__ ( model = model , sample_train_test_split_seed = False , train_sampling_type = 'bootstrap' , test_sampling_type = 'bootstrap' , ** kwargs ) compute ( self , metrics = None ) inherited \u00b6 Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" if self . fitted is False : raise ( NotFittedError ( 'The object has not been fitted. Please run fit() method first' )) if self . report is None : raise ( ValueError ( 'Report is None, thus it has not been computed by fit method. Please extend the ' 'BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()' )) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ] fit ( self , X , y ) inherited \u00b6 Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.DataFrame or numpy.ndarray Array with targets. required Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y ): \"\"\" Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.DataFrame or numpy.ndarray): Array with targets. \"\"\" super () . fit () X = assure_numpy_array ( X ) y = assure_numpy_array ( y ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations ) * self . random_state ) . astype ( int ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = X , y = y , model = self . model , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction ) for split_seed in tqdm ( random_seeds )) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report () fit_compute ( self , * args , ** kwargs ) inherited \u00b6 Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute () plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ) inherited \u00b6 Plots distribution of the metric Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ): \"\"\" Plots distribution of the metric Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ])) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = 'Train {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = 'Test {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( 'Distributions {} ' . format ( metric )) axs [ axis_index ] . legend ( loc = 'upper right' ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = 'Delta {} ' . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( 'Distributions delta {} ' . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = 'upper right' ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = ' {} score' . format ( metric ), ylabel = 'Results count' ) SplitSeedVolatility \u00b6 Estimation of volatility of metrics depending on the seed used to split the data. At every iteration it splits the data into train and test set using a different stratified split and volatility of the metrics is calculated. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import SplitSeedVolatility X , y = make_classification ( n_features = 4 ) clf = RandomForestClassifier () volatility = SplitSeedVolatility ( clf , iterations = 500 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot () __init__ ( self , model , ** kwargs ) special \u00b6 Initializes the class Parameters: Name Type Description Default model model object Binary classification model or pipeline. required **kwargs Keyword arguments that can be overwrite the default parameters from TrainTestVolatility: iterations : Number of iterations in seed bootstrapping. By default 1000. metrics : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the sklearn documentation . In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. test_prc : Percentage of input data used as test. By default 0.25. n_jobs : Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply : List of tests to apply. Available options described in BaseVolatilityEstimator. random_state : The seed used by the random number generator. {} Source code in probatus/metric_volatility/volatility.py def __init__ ( self , model , ** kwargs ): \"\"\" Initializes the class Args: model (model object): Binary classification model or pipeline. **kwargs: Keyword arguments that can be overwrite the default parameters from TrainTestVolatility: - `iterations`: Number of iterations in seed bootstrapping. By default 1000. - `metrics` : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html). In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. - `test_prc`: Percentage of input data used as test. By default 0.25. - `n_jobs`: Number of parallel executions. If -1 use all available cores. By default 1. - `stats_tests_to_apply`: List of tests to apply. Available options described in BaseVolatilityEstimator. - `random_state`: The seed used by the random number generator. \"\"\" super () . __init__ ( model = model , sample_train_test_split_seed = True , train_sampling_type = None , test_sampling_type = None , train_sampling_fraction = 1 , test_sampling_fraction = 1 , ** kwargs ) compute ( self , metrics = None ) inherited \u00b6 Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" if self . fitted is False : raise ( NotFittedError ( 'The object has not been fitted. Please run fit() method first' )) if self . report is None : raise ( ValueError ( 'Report is None, thus it has not been computed by fit method. Please extend the ' 'BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()' )) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ] fit ( self , X , y ) inherited \u00b6 Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.DataFrame or numpy.ndarray Array with targets. required Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y ): \"\"\" Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.DataFrame or numpy.ndarray): Array with targets. \"\"\" super () . fit () X = assure_numpy_array ( X ) y = assure_numpy_array ( y ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations ) * self . random_state ) . astype ( int ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = X , y = y , model = self . model , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction ) for split_seed in tqdm ( random_seeds )) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report () fit_compute ( self , * args , ** kwargs ) inherited \u00b6 Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute () plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ) inherited \u00b6 Plots distribution of the metric Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ): \"\"\" Plots distribution of the metric Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ])) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = 'Train {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = 'Test {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( 'Distributions {} ' . format ( metric )) axs [ axis_index ] . legend ( loc = 'upper right' ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = 'Delta {} ' . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( 'Distributions delta {} ' . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = 'upper right' ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = ' {} score' . format ( metric ), ylabel = 'Results count' ) TrainTestVolatility \u00b6 Estimation of volatility of metrics. The estimation is done by splitting the data into train and test multiple times and training and scoring a model based on these metrics. The class allows for choosing whether at each iteration the train test split should be the same or different, whether and how the train and test sets should be sampled. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import TrainTestVolatility X , y = make_classification ( n_features = 4 ) clf = RandomForestClassifier () volatility = TrainTestVolatility ( clf , iterations = 500 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot () __init__ ( self , model , iterations = 1000 , sample_train_test_split_seed = True , train_sampling_type = None , test_sampling_type = None , train_sampling_fraction = 1 , test_sampling_fraction = 1 , ** kwargs ) special \u00b6 Initializes the class. Parameters: Name Type Description Default model model object Binary classification model or pipeline. required iterations int Number of iterations in seed bootstrapping. By default 1000. 1000 sample_train_test_split_seed bool Flag indicating whether each train test split should be done randomly or measurement should be done for single split. Default is True, which indicates that each. iteration is performed on a random train test split. If the value is False, the random_seed for the split is set to train_test_split_seed. True train_sampling_type str String indicating what type of sampling should be applied on train set: None indicates that no additional sampling is done after splitting data, 'bootstrap' indicates that sampling with replacement will be performed on train data, 'subsample': indicates that sampling without repetition will be performed on train data. None test_sampling_type str String indicating what type of sampling should be applied on test set: None indicates that no additional sampling is done after splitting data, 'bootstrap' indicates that sampling with replacement will be performed on test data, 'subsample': indicates that sampling without repetition will be performed on test data. None train_sampling_fraction float Fraction of train data sampled, if sample_train_type is not None. Default value is 1. 1 test_sampling_fraction Optional float Fraction of test data sampled, if sample_test_type is not None. Default value is 1. 1 **kwargs keyword arguments used to overwrite default parameters from init of BaseVolatilityEstimator: metrics : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the sklearn documentation . In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. test_prc : Percentage of input data used as test. By default 0.25. n_jobs : Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply : List of tests to apply. Available options described in BaseVolatilityEstimator. random_state : The seed used by the random number generator. {} Source code in probatus/metric_volatility/volatility.py def __init__ ( self , model , iterations = 1000 , sample_train_test_split_seed = True , train_sampling_type = None , test_sampling_type = None , train_sampling_fraction = 1 , test_sampling_fraction = 1 , ** kwargs ): \"\"\" Initializes the class. Args: model (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations in seed bootstrapping. By default 1000. sample_train_test_split_seed (bool, optional): Flag indicating whether each train test split should be done randomly or measurement should be done for single split. Default is True, which indicates that each. iteration is performed on a random train test split. If the value is False, the random_seed for the split is set to train_test_split_seed. train_sampling_type (str, optional): String indicating what type of sampling should be applied on train set: - None indicates that no additional sampling is done after splitting data, - 'bootstrap' indicates that sampling with replacement will be performed on train data, - 'subsample': indicates that sampling without repetition will be performed on train data. test_sampling_type (str, optional): String indicating what type of sampling should be applied on test set: - None indicates that no additional sampling is done after splitting data, - 'bootstrap' indicates that sampling with replacement will be performed on test data, - 'subsample': indicates that sampling without repetition will be performed on test data. train_sampling_fraction (float, optional): Fraction of train data sampled, if sample_train_type is not None. Default value is 1. test_sampling_fraction (Optional float): Fraction of test data sampled, if sample_test_type is not None. Default value is 1. **kwargs: keyword arguments used to overwrite default parameters from init of BaseVolatilityEstimator: - `metrics` : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html). In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. - `test_prc`: Percentage of input data used as test. By default 0.25. - `n_jobs`: Number of parallel executions. If -1 use all available cores. By default 1. - `stats_tests_to_apply`: List of tests to apply. Available options described in BaseVolatilityEstimator. - `random_state`: The seed used by the random number generator. \"\"\" super () . __init__ ( model = model , ** kwargs ) self . iterations = iterations self . train_sampling_type = train_sampling_type self . test_sampling_type = test_sampling_type self . sample_train_test_split_seed = sample_train_test_split_seed self . train_sampling_fraction = train_sampling_fraction self . test_sampling_fraction = test_sampling_fraction check_sampling_input ( train_sampling_type , train_sampling_fraction , 'train' ) check_sampling_input ( test_sampling_type , test_sampling_fraction , 'test' ) compute ( self , metrics = None ) inherited \u00b6 Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" if self . fitted is False : raise ( NotFittedError ( 'The object has not been fitted. Please run fit() method first' )) if self . report is None : raise ( ValueError ( 'Report is None, thus it has not been computed by fit method. Please extend the ' 'BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()' )) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ] fit ( self , X , y ) \u00b6 Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.DataFrame or numpy.ndarray Array with targets. required Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y ): \"\"\" Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.DataFrame or numpy.ndarray): Array with targets. \"\"\" super () . fit () X = assure_numpy_array ( X ) y = assure_numpy_array ( y ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations ) * self . random_state ) . astype ( int ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = X , y = y , model = self . model , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction ) for split_seed in tqdm ( random_seeds )) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report () fit_compute ( self , * args , ** kwargs ) inherited \u00b6 Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute () plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ) inherited \u00b6 Plots distribution of the metric Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ): \"\"\" Plots distribution of the metric Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ])) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = 'Train {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = 'Test {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( 'Distributions {} ' . format ( metric )) axs [ axis_index ] . legend ( loc = 'upper right' ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = 'Delta {} ' . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( 'Distributions delta {} ' . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = 'upper right' ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = ' {} score' . format ( metric ), ylabel = 'Results count' )","title":"probatus.metric_volatility"},{"location":"api/metric_volatility.html#metric-volatility","text":"The aim of this module is analysis of how well a model performs on a given dataset, and how stable the performance is. The following features are implemented: BaseVolatilityEstimator - Base class, provides main functionality with fit method that can be overwritten by subclasses TrainTestVolatility - Estimation of volatility of metrics. The estimation is done by splitting the data into train and test multiple times and training and scoring a model based on these metrics. SplitSeedVolatility - Estimates volatility of metrics based on splitting the data into train and test sets multiple times randomly, each time with different seed. BootstrappedVolatility - stimates volatility of metrics based on splitting the data into train and test with static seed, and bootstrapping train and test set.","title":"Metric Volatility"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility","text":"","title":"probatus.metric_volatility.volatility"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BaseVolatilityEstimator","text":"Base object for estimating volatility estimation. This class is a base class, therefore cannot be used on its own. Implements common API that can be used by all subclasses.","title":"BaseVolatilityEstimator"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BaseVolatilityEstimator.__init__","text":"Initializes the class Parameters: Name Type Description Default model model object Binary classification model or pipeline. required metrics string, list of strings, Scorer or list of Scorers Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the sklearn documentation . In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. 'roc_auc' test_prc float Percentage of input data used as test. By default 0.25. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 stats_tests_to_apply None, string or list of strings List of tests to apply. Available options: 'ES': Epps-Singleton, 'KS': Kolmogorov-Smirnov statistic, 'PSI': Population Stability Index, 'SW': Shapiro-Wilk based difference statistic, 'AD': Anderson-Darling TS. None random_state int The seed used by the random number generator. 42 Source code in probatus/metric_volatility/volatility.py def __init__ ( self , model , metrics = 'roc_auc' , test_prc = 0.25 , n_jobs = 1 , stats_tests_to_apply = None , random_state = 42 ): \"\"\" Initializes the class Args: model (model object): Binary classification model or pipeline. metrics (string, list of strings, Scorer or list of Scorers): Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html). In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. test_prc (float, optional): Percentage of input data used as test. By default 0.25. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply (None, string or list of strings, optional): List of tests to apply. Available options: - 'ES': Epps-Singleton, - 'KS': Kolmogorov-Smirnov statistic, - 'PSI': Population Stability Index, - 'SW': Shapiro-Wilk based difference statistic, - 'AD': Anderson-Darling TS. random_state (int, optional): The seed used by the random number generator. \"\"\" self . model = model self . n_jobs = n_jobs self . random_state = random_state self . test_prc = test_prc self . iterations_results = None self . report = None self . fitted = False self . allowed_stats_tests = list ( DistributionStatistics . statistical_test_dict . keys ()) # TODO set reasonable default value for the parameter, to choose the statistical test for the user for different # ways to compute volatility if stats_tests_to_apply is not None : self . stats_tests_to_apply = assure_list_of_strings ( stats_tests_to_apply , 'stats_tests_to_apply' ) assure_list_values_allowed ( variable = self . stats_tests_to_apply , variable_name = 'stats_tests_to_apply' , allowed_values = self . allowed_stats_tests ) else : self . stats_tests_to_apply = [] self . stats_tests_objects = [] if len ( self . stats_tests_to_apply ) > 0 : warnings . warn ( \"Computing statistics for distributions is an experimental feature. While using it, keep in \" \"mind that the samples of metrics might be correlated.\" ) for test_name in self . stats_tests_to_apply : self . stats_tests_objects . append ( DistributionStatistics ( statistical_test = test_name )) self . scorers = get_scorers ( metrics )","title":"__init__()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BaseVolatilityEstimator.compute","text":"Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" if self . fitted is False : raise ( NotFittedError ( 'The object has not been fitted. Please run fit() method first' )) if self . report is None : raise ( ValueError ( 'Report is None, thus it has not been computed by fit method. Please extend the ' 'BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()' )) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ]","title":"compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BaseVolatilityEstimator.fit","text":"Base fit functionality that should be executed before each fit. Source code in probatus/metric_volatility/volatility.py def fit ( self , * args , ** kwargs ): \"\"\" Base fit functionality that should be executed before each fit. \"\"\" # Set seed for results reproducibility np . random . seed ( self . random_state ) # Initialize the report and results self . iterations_results = None self . report = None self . fitted = True","title":"fit()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BaseVolatilityEstimator.fit_compute","text":"Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BaseVolatilityEstimator.plot","text":"Plots distribution of the metric Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ): \"\"\" Plots distribution of the metric Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ])) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = 'Train {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = 'Test {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( 'Distributions {} ' . format ( metric )) axs [ axis_index ] . legend ( loc = 'upper right' ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = 'Delta {} ' . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( 'Distributions delta {} ' . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = 'upper right' ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = ' {} score' . format ( metric ), ylabel = 'Results count' )","title":"plot()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility","text":"Estimation of volatility of metrics by bootstrapping both train and test set. By default at every iteration the train test split is the same. The test shows volatility of metric with regards to sampling different rows from static train and test sets. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import BootstrappedVolatility X , y = make_classification ( n_features = 4 ) RandomForestClassifier () volatility = BootstrappedVolatility ( clf , iterations = 500 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot ()","title":"BootstrappedVolatility"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility.__init__","text":"Initializes the class. Parameters: Name Type Description Default model model object Binary classification model or pipeline. required **kwargs Keyword arguments that can be overwrite the default parameters from TrainTestVolatility: iterations : Number of iterations in seed bootstrapping. By default 1000. train_sampling_fraction : Fraction of train data sampled, if sample_train_type is not None. Default value is 1. test_sampling_fraction : Fraction of test data sampled, if sample_test_type is not None. Default value is 1. metrics : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the sklearn documentation . In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. test_prc : Percentage of input data used as test. By default 0.25. n_jobs : Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply : List of tests to apply. Available options described in BaseVolatilityEstimator. random_state : The seed used by the random number generator. {} Source code in probatus/metric_volatility/volatility.py def __init__ ( self , model , ** kwargs ): \"\"\" Initializes the class. Args: model (model object): Binary classification model or pipeline. **kwargs: Keyword arguments that can be overwrite the default parameters from TrainTestVolatility: - `iterations`: Number of iterations in seed bootstrapping. By default 1000. - `train_sampling_fraction`: Fraction of train data sampled, if sample_train_type is not None. Default value is 1. - `test_sampling_fraction`: Fraction of test data sampled, if sample_test_type is not None. Default value is 1. - `metrics` : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html). In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. - `test_prc`: Percentage of input data used as test. By default 0.25. - `n_jobs`: Number of parallel executions. If -1 use all available cores. By default 1. - `stats_tests_to_apply`: List of tests to apply. Available options described in BaseVolatilityEstimator. - `random_state`: The seed used by the random number generator. \"\"\" super () . __init__ ( model = model , sample_train_test_split_seed = False , train_sampling_type = 'bootstrap' , test_sampling_type = 'bootstrap' , ** kwargs )","title":"__init__()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility.compute","text":"Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" if self . fitted is False : raise ( NotFittedError ( 'The object has not been fitted. Please run fit() method first' )) if self . report is None : raise ( ValueError ( 'Report is None, thus it has not been computed by fit method. Please extend the ' 'BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()' )) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ]","title":"compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility.fit","text":"Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.DataFrame or numpy.ndarray Array with targets. required Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y ): \"\"\" Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.DataFrame or numpy.ndarray): Array with targets. \"\"\" super () . fit () X = assure_numpy_array ( X ) y = assure_numpy_array ( y ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations ) * self . random_state ) . astype ( int ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = X , y = y , model = self . model , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction ) for split_seed in tqdm ( random_seeds )) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report ()","title":"fit()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility.fit_compute","text":"Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.BootstrappedVolatility.plot","text":"Plots distribution of the metric Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ): \"\"\" Plots distribution of the metric Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ])) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = 'Train {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = 'Test {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( 'Distributions {} ' . format ( metric )) axs [ axis_index ] . legend ( loc = 'upper right' ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = 'Delta {} ' . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( 'Distributions delta {} ' . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = 'upper right' ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = ' {} score' . format ( metric ), ylabel = 'Results count' )","title":"plot()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility","text":"Estimation of volatility of metrics depending on the seed used to split the data. At every iteration it splits the data into train and test set using a different stratified split and volatility of the metrics is calculated. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import SplitSeedVolatility X , y = make_classification ( n_features = 4 ) clf = RandomForestClassifier () volatility = SplitSeedVolatility ( clf , iterations = 500 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot ()","title":"SplitSeedVolatility"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility.__init__","text":"Initializes the class Parameters: Name Type Description Default model model object Binary classification model or pipeline. required **kwargs Keyword arguments that can be overwrite the default parameters from TrainTestVolatility: iterations : Number of iterations in seed bootstrapping. By default 1000. metrics : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the sklearn documentation . In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. test_prc : Percentage of input data used as test. By default 0.25. n_jobs : Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply : List of tests to apply. Available options described in BaseVolatilityEstimator. random_state : The seed used by the random number generator. {} Source code in probatus/metric_volatility/volatility.py def __init__ ( self , model , ** kwargs ): \"\"\" Initializes the class Args: model (model object): Binary classification model or pipeline. **kwargs: Keyword arguments that can be overwrite the default parameters from TrainTestVolatility: - `iterations`: Number of iterations in seed bootstrapping. By default 1000. - `metrics` : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html). In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. - `test_prc`: Percentage of input data used as test. By default 0.25. - `n_jobs`: Number of parallel executions. If -1 use all available cores. By default 1. - `stats_tests_to_apply`: List of tests to apply. Available options described in BaseVolatilityEstimator. - `random_state`: The seed used by the random number generator. \"\"\" super () . __init__ ( model = model , sample_train_test_split_seed = True , train_sampling_type = None , test_sampling_type = None , train_sampling_fraction = 1 , test_sampling_fraction = 1 , ** kwargs )","title":"__init__()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility.compute","text":"Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" if self . fitted is False : raise ( NotFittedError ( 'The object has not been fitted. Please run fit() method first' )) if self . report is None : raise ( ValueError ( 'Report is None, thus it has not been computed by fit method. Please extend the ' 'BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()' )) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ]","title":"compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility.fit","text":"Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.DataFrame or numpy.ndarray Array with targets. required Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y ): \"\"\" Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.DataFrame or numpy.ndarray): Array with targets. \"\"\" super () . fit () X = assure_numpy_array ( X ) y = assure_numpy_array ( y ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations ) * self . random_state ) . astype ( int ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = X , y = y , model = self . model , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction ) for split_seed in tqdm ( random_seeds )) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report ()","title":"fit()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility.fit_compute","text":"Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.SplitSeedVolatility.plot","text":"Plots distribution of the metric Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ): \"\"\" Plots distribution of the metric Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ])) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = 'Train {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = 'Test {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( 'Distributions {} ' . format ( metric )) axs [ axis_index ] . legend ( loc = 'upper right' ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = 'Delta {} ' . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( 'Distributions delta {} ' . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = 'upper right' ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = ' {} score' . format ( metric ), ylabel = 'Results count' )","title":"plot()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility","text":"Estimation of volatility of metrics. The estimation is done by splitting the data into train and test multiple times and training and scoring a model based on these metrics. The class allows for choosing whether at each iteration the train test split should be the same or different, whether and how the train and test sets should be sampled. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.metric_volatility import TrainTestVolatility X , y = make_classification ( n_features = 4 ) clf = RandomForestClassifier () volatility = TrainTestVolatility ( clf , iterations = 500 , test_prc = 0.5 ) volatility_report = volatility . fit_compute ( X , y ) volatility . plot ()","title":"TrainTestVolatility"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility.__init__","text":"Initializes the class. Parameters: Name Type Description Default model model object Binary classification model or pipeline. required iterations int Number of iterations in seed bootstrapping. By default 1000. 1000 sample_train_test_split_seed bool Flag indicating whether each train test split should be done randomly or measurement should be done for single split. Default is True, which indicates that each. iteration is performed on a random train test split. If the value is False, the random_seed for the split is set to train_test_split_seed. True train_sampling_type str String indicating what type of sampling should be applied on train set: None indicates that no additional sampling is done after splitting data, 'bootstrap' indicates that sampling with replacement will be performed on train data, 'subsample': indicates that sampling without repetition will be performed on train data. None test_sampling_type str String indicating what type of sampling should be applied on test set: None indicates that no additional sampling is done after splitting data, 'bootstrap' indicates that sampling with replacement will be performed on test data, 'subsample': indicates that sampling without repetition will be performed on test data. None train_sampling_fraction float Fraction of train data sampled, if sample_train_type is not None. Default value is 1. 1 test_sampling_fraction Optional float Fraction of test data sampled, if sample_test_type is not None. Default value is 1. 1 **kwargs keyword arguments used to overwrite default parameters from init of BaseVolatilityEstimator: metrics : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the sklearn documentation . In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. test_prc : Percentage of input data used as test. By default 0.25. n_jobs : Number of parallel executions. If -1 use all available cores. By default 1. stats_tests_to_apply : List of tests to apply. Available options described in BaseVolatilityEstimator. random_state : The seed used by the random number generator. {} Source code in probatus/metric_volatility/volatility.py def __init__ ( self , model , iterations = 1000 , sample_train_test_split_seed = True , train_sampling_type = None , test_sampling_type = None , train_sampling_fraction = 1 , test_sampling_fraction = 1 , ** kwargs ): \"\"\" Initializes the class. Args: model (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations in seed bootstrapping. By default 1000. sample_train_test_split_seed (bool, optional): Flag indicating whether each train test split should be done randomly or measurement should be done for single split. Default is True, which indicates that each. iteration is performed on a random train test split. If the value is False, the random_seed for the split is set to train_test_split_seed. train_sampling_type (str, optional): String indicating what type of sampling should be applied on train set: - None indicates that no additional sampling is done after splitting data, - 'bootstrap' indicates that sampling with replacement will be performed on train data, - 'subsample': indicates that sampling without repetition will be performed on train data. test_sampling_type (str, optional): String indicating what type of sampling should be applied on test set: - None indicates that no additional sampling is done after splitting data, - 'bootstrap' indicates that sampling with replacement will be performed on test data, - 'subsample': indicates that sampling without repetition will be performed on test data. train_sampling_fraction (float, optional): Fraction of train data sampled, if sample_train_type is not None. Default value is 1. test_sampling_fraction (Optional float): Fraction of test data sampled, if sample_test_type is not None. Default value is 1. **kwargs: keyword arguments used to overwrite default parameters from init of BaseVolatilityEstimator: - `metrics` : Metrics for which the score is calculated. It can be either a name or list of names metric names and needs to be aligned with predefined classification scorers names in sklearn, see the [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html). In case a custom metric is used, one can create own Scorer (probatus.utils) and provide as a metric. By default 'roc_auc' is measured. - `test_prc`: Percentage of input data used as test. By default 0.25. - `n_jobs`: Number of parallel executions. If -1 use all available cores. By default 1. - `stats_tests_to_apply`: List of tests to apply. Available options described in BaseVolatilityEstimator. - `random_state`: The seed used by the random number generator. \"\"\" super () . __init__ ( model = model , ** kwargs ) self . iterations = iterations self . train_sampling_type = train_sampling_type self . test_sampling_type = test_sampling_type self . sample_train_test_split_seed = sample_train_test_split_seed self . train_sampling_fraction = train_sampling_fraction self . test_sampling_fraction = test_sampling_fraction check_sampling_input ( train_sampling_type , train_sampling_fraction , 'train' ) check_sampling_input ( test_sampling_type , test_sampling_fraction , 'test' )","title":"__init__()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility.compute","text":"Reports the statistics. Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def compute ( self , metrics = None ): \"\"\" Reports the statistics. Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" if self . fitted is False : raise ( NotFittedError ( 'The object has not been fitted. Please run fit() method first' )) if self . report is None : raise ( ValueError ( 'Report is None, thus it has not been computed by fit method. Please extend the ' 'BaseVolatilityEstimator class, overwrite fit method, and within fit run compute_report()' )) if metrics is None : return self . report else : if not isinstance ( metrics , list ): metrics = [ metrics ] return self . report . loc [ metrics ]","title":"compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility.fit","text":"Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Parameters: Name Type Description Default X pandas.DataFrame or numpy.ndarray Array with samples and features. required y pandas.DataFrame or numpy.ndarray Array with targets. required Source code in probatus/metric_volatility/volatility.py def fit ( self , X , y ): \"\"\" Bootstraps a number of random seeds, then splits the data based on the sampled seeds and estimates performance of the model based on the split data. Args: X (pandas.DataFrame or numpy.ndarray): Array with samples and features. y (pandas.DataFrame or numpy.ndarray): Array with targets. \"\"\" super () . fit () X = assure_numpy_array ( X ) y = assure_numpy_array ( y ) if self . sample_train_test_split_seed : random_seeds = np . random . random_integers ( 0 , 999999 , self . iterations ) else : random_seeds = ( np . ones ( self . iterations ) * self . random_state ) . astype ( int ) results_per_iteration = Parallel ( n_jobs = self . n_jobs )( delayed ( get_metric )( X = X , y = y , model = self . model , test_size = self . test_prc , split_seed = split_seed , scorers = self . scorers , train_sampling_type = self . train_sampling_type , test_sampling_type = self . test_sampling_type , train_sampling_fraction = self . train_sampling_fraction , test_sampling_fraction = self . test_sampling_fraction ) for split_seed in tqdm ( random_seeds )) self . iterations_results = pd . concat ( results_per_iteration , ignore_index = True ) self . _create_report ()","title":"fit()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility.fit_compute","text":"Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: Type Description pandas.Dataframe Report that contains the evaluation mean and std on train and test sets for each metric. Source code in probatus/metric_volatility/volatility.py def fit_compute ( self , * args , ** kwargs ): \"\"\" Runs trains and evaluates a number of models on train and test sets extracted using different random seeds. Reports the statistics of the selected metric. Returns: pandas.Dataframe: Report that contains the evaluation mean and std on train and test sets for each metric. \"\"\" self . fit ( * args , ** kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/metric_volatility.html#probatus.metric_volatility.volatility.TrainTestVolatility.plot","text":"Plots distribution of the metric Parameters: Name Type Description Default metrics str or list of strings Name or list of names of metrics to be plotted. If not all metrics are presented. None bins int Number of bins into which histogram is built. 10 height_per_subplot int Height of each subplot. Default is 5. 5 width_per_subplot int Width of each subplot. Default is 5. 5 Source code in probatus/metric_volatility/volatility.py def plot ( self , metrics = None , bins = 10 , height_per_subplot = 5 , width_per_subplot = 5 ): \"\"\" Plots distribution of the metric Args: metrics (str or list of strings, optional): Name or list of names of metrics to be plotted. If not all metrics are presented. bins (int, optional): Number of bins into which histogram is built. height_per_subplot (int, optional): Height of each subplot. Default is 5. width_per_subplot (int, optional): Width of each subplot. Default is 5. \"\"\" target_report = self . compute ( metrics = metrics ) if target_report . shape [ 0 ] >= 1 : fig , axs = plt . subplots ( target_report . shape [ 0 ], 2 , figsize = ( width_per_subplot * 2 , height_per_subplot * target_report . shape [ 0 ])) # Enable traversing the axs axs = axs . flatten () axis_index = 0 for metric , row in target_report . iterrows (): train , test , delta = self . _get_samples_to_plot ( metric_name = metric ) axs [ axis_index ] . hist ( train , alpha = 0.5 , label = 'Train {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . hist ( test , alpha = 0.5 , label = 'Test {} ' . format ( metric ), bins = bins ) axs [ axis_index ] . set_title ( 'Distributions {} ' . format ( metric )) axs [ axis_index ] . legend ( loc = 'upper right' ) axs [ axis_index + 1 ] . hist ( delta , alpha = 0.5 , label = 'Delta {} ' . format ( metric ), bins = bins ) axs [ axis_index + 1 ] . set_title ( 'Distributions delta {} ' . format ( metric )) axs [ axis_index + 1 ] . legend ( loc = 'upper right' ) axis_index += 2 for ax in axs . flat : ax . set ( xlabel = ' {} score' . format ( metric ), ylabel = 'Results count' )","title":"plot()"},{"location":"api/model_interpret.html","text":"Model Interpretation using SHAP \u00b6 The aim of this module is providing tools for model interpretation using SHAP library. The class below is a convenience wrapper, that implements multiple plots for tree-based models. \u00b6 ShapModelInterpreter \u00b6 This class is a wrapper that allows to easily analyse model's features. It allows to plot SHAP feature importance, SHAP summary plot and SHAP dependence plots. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split import numpy as np import pandas as pd feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 ) X = pd . DataFrame ( X , columns = feature_names ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Prepare and fit model. Remember about class_weight=\"balanced\" or an equivalent. clf = RandomForestClassifier ( class_weight = 'balanced' , n_estimators = 100 , max_depth = 2 , random_state = 0 ) clf . fit ( X_train , y_train ) # Train ShapModelAnalyser shap_interpreter = ShapModelInterpreter ( clf ) feature_importance = shap_interpreter . fit_compute ( X_train , X_test , y_train , y_test ) # Make plots shap_interpreter . plot ( 'importance' ) shap_interpreter . plot ( 'summary' ) shap_interpreter . plot ( 'dependence' , target_columns = [ 'f1' , 'f2' ]) shap_interpreter . plot ( 'sample' , samples_index = [ 521 , 78 ]) __init__ ( self , clf ) special \u00b6 Initializes the class. Parameters: Name Type Description Default clf binary classifier Model fitted on X_train. required Source code in probatus/interpret/model_interpret.py def __init__ ( self , clf ): \"\"\" Initializes the class. Args: clf (binary classifier): Model fitted on X_train. \"\"\" self . clf = clf self . fitted = False compute ( self ) \u00b6 Computes the DataFrame, that presents the importance of each feature. Returns: Type Description (pd.DataFrame) Dataframe with SHAP feature importance. Source code in probatus/interpret/model_interpret.py def compute ( self ): \"\"\" Computes the DataFrame, that presents the importance of each feature. Returns: (pd.DataFrame): Dataframe with SHAP feature importance. \"\"\" self . _check_if_fitted () # Compute SHAP importance self . importance_df = calculate_shap_importance ( self . shap_values , self . column_names ) return self . importance_df fit ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , approximate = False , ** shap_kwargs ) \u00b6 Fits the object and calculates the shap values for the provided datasets. Parameters: Name Type Description Default X_train pd.DataFrame Dataframe containing training data. required X_test pd.DataFrame Dataframe containing test data. required y_train pd.Series Series of binary labels for train data. required y_test pd.Series Series of binary labels for test data. required column_names Optional, None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names Optional, None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None approximate boolean , if True uses shap approximations - less accurate, but very fast. False **shap_kwargs keyword arguments passed to shap.TreeExplainer. {} Source code in probatus/interpret/model_interpret.py def fit ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , approximate = False , ** shap_kwargs ): \"\"\" Fits the object and calculates the shap values for the provided datasets. Args: X_train (pd.DataFrame): Dataframe containing training data. X_test (pd.DataFrame): Dataframe containing test data. y_train (pd.Series): Series of binary labels for train data. y_test (pd.Series): Series of binary labels for test data. column_names (Optional, None, or list of str): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (Optional, None, or list of str): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. approximate (boolean):, if True uses shap approximations - less accurate, but very fast. **shap_kwargs: keyword arguments passed to shap.TreeExplainer. \"\"\" self . X_train = assure_pandas_df ( X_train ) self . X_test = assure_pandas_df ( X_test ) self . y_train = y_train self . y_test = y_test # Set class names self . class_names = class_names if self . class_names is None : self . class_names = [ 'Negative Class' , 'Positive Class' ] # Set column names self . column_names = assure_column_names_consistency ( column_names , self . X_train ) # Calculate Metrics self . auc_train = roc_auc_score ( self . y_train , self . clf . predict_proba ( self . X_train )[:, 1 ]) self . auc_test = roc_auc_score ( self . y_test , self . clf . predict_proba ( self . X_test )[:, 1 ]) self . results_text = \"Train AUC: {} , \\n Test AUC: {} .\" . format ( np . round ( self . auc_train , 3 ), np . round ( self . auc_test , 3 ) ) self . shap_values , self . explainer = shap_calc ( self . clf , self . X_test , approximate = approximate , return_explainer = True , data = self . X_train , ** shap_kwargs ) # Get expected_value from the explainer self . expected_value = self . explainer . expected_value # For sklearn models the expected values consists of two elements (negative_class and positive_class) if isinstance ( self . expected_value , list ) or isinstance ( self . expected_value , np . ndarray ): self . expected_value = self . expected_value [ 1 ] # Initialize tree dependence plotter self . tdp = TreeDependencePlotter ( self . clf ) . fit ( self . X_test , self . y_test , precalc_shap = self . shap_values ) self . fitted = True fit_compute ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , approximate = False , ** shap_kwargs ) \u00b6 Fits the object and calculates the shap values for the provided datasets. Parameters: Name Type Description Default X_train pd.DataFrame Dataframe containing training data. required X_test pd.DataFrame Dataframe containing test data. required y_train pd.Series Series of binary labels for train data. required y_test pd.Series Series of binary labels for test data. required column_names Optional, None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names Optional, None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None approximate boolean , if True uses shap approximations - less accurate, but very fast. False **shap_kwargs keyword arguments passed to shap.TreeExplainer. {} Returns: Type Description (pd.DataFrame) Dataframe with SHAP feature importance. Source code in probatus/interpret/model_interpret.py def fit_compute ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , approximate = False , ** shap_kwargs ): \"\"\" Fits the object and calculates the shap values for the provided datasets. Args: X_train (pd.DataFrame): Dataframe containing training data. X_test (pd.DataFrame): Dataframe containing test data. y_train (pd.Series): Series of binary labels for train data. y_test (pd.Series): Series of binary labels for test data. column_names (Optional, None, or list of str): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (Optional, None, or list of str): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. approximate (boolean):, if True uses shap approximations - less accurate, but very fast. **shap_kwargs: keyword arguments passed to shap.TreeExplainer. Returns: (pd.DataFrame): Dataframe with SHAP feature importance. \"\"\" self . fit ( X_train = X_train , X_test = X_test , y_train = y_train , y_test = y_test , column_names = column_names , class_names = class_names , approximate = approximate , ** shap_kwargs ) return self . compute () plot ( self , plot_type , target_columns = None , samples_index = None , ** plot_kwargs ) \u00b6 Plots the appropriate SHAP plot Parameters: Name Type Description Default plot_type str One of the following: 'importance': Feature importance plot, SHAP bar summary plot 'summary': SHAP Summary plot 'dependence': Dependence plot for each feature 'sample': Explanation of a given sample in the test data required target_columns Optional, None, str or list of str List of features names, for which the plots should be generated. If None, all features will be plotted. None samples_index None, int, list or pd.Index Index of samples to be explained if the plot_type=sample . None **plot_kwargs Keyword arguments passed to the plot method. For 'importance' and 'summary' plot_type, the kwargs are passed to shap.summary_plot, for 'dependence' plot_type, they are passed to probatus.interpret.TreeDependencePlotter.feature_plot method. {} Source code in probatus/interpret/model_interpret.py def plot ( self , plot_type , target_columns = None , samples_index = None , ** plot_kwargs ): \"\"\" Plots the appropriate SHAP plot Args: plot_type (str): One of the following: - 'importance': Feature importance plot, SHAP bar summary plot - 'summary': SHAP Summary plot - 'dependence': Dependence plot for each feature - 'sample': Explanation of a given sample in the test data target_columns (Optional, None, str or list of str): List of features names, for which the plots should be generated. If None, all features will be plotted. samples_index (None, int, list or pd.Index): Index of samples to be explained if the `plot_type=sample`. **plot_kwargs: Keyword arguments passed to the plot method. For 'importance' and 'summary' plot_type, the kwargs are passed to shap.summary_plot, for 'dependence' plot_type, they are passed to probatus.interpret.TreeDependencePlotter.feature_plot method. \"\"\" if target_columns is None : target_columns = self . column_names target_columns = assure_list_of_strings ( target_columns , 'target_columns' ) target_columns_indices = [ self . column_names . index ( target_column ) for target_column in target_columns ] if plot_type in [ 'importance' , 'summary' ]: # Get the target features target_X_test = self . X_test [ target_columns ] target_shap_values = self . shap_values [:, target_columns_indices ] # Set summary plot settings if plot_type == 'importance' : plot_type = 'bar' plot_title = 'SHAP Feature Importance' else : plot_type = 'dot' plot_title = 'SHAP Summary plot' shap . summary_plot ( target_shap_values , target_X_test , plot_type = plot_type , class_names = self . class_names , show = False , ** plot_kwargs ) ax = plt . gca () ax . set_title ( plot_title ) ax . annotate ( self . results_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = 'axes fraction' , textcoords = 'offset points' , va = 'top' ) plt . show () elif plot_type == 'dependence' : ax = [] for feature_name in target_columns : print () ax . append ( self . tdp . plot ( feature = feature_name , figsize = ( 10 , 7 ), target_names = self . class_names )) plt . show () elif plot_type == 'sample' : # Ensure the correct samples_index type if samples_index is None : raise ( ValueError ( 'For sample plot, you need to specify the samples_index be plotted plot' )) elif isinstance ( samples_index , int ) or isinstance ( samples_index , str ): samples_index = [ samples_index ] elif not ( isinstance ( samples_index , list ) or isinstance ( samples_index , pd . Index )): raise ( TypeError ( 'sample_index must be one of the following: int, str, list or pd.Index' )) ax = [] for sample_index in samples_index : sample_loc = self . X_test . index . get_loc ( sample_index ) shap . plots . _waterfall . waterfall_legacy ( self . expected_value , self . shap_values [ sample_loc , :], self . X_test . loc [ sample_index ], show = False , ** plot_kwargs ) plot_title = f 'SHAP Sample Explanation for index= { sample_index } ' current_ax = plt . gca () current_ax . set_title ( plot_title ) ax . append ( current_ax ) plt . show () else : raise ValueError ( \"Wrong plot type, select from 'importance', 'summary', or 'dependence'\" ) if isinstance ( ax , list ) and len ( ax ) == 1 : ax = ax [ 0 ] return ax","title":"probatus.interpret"},{"location":"api/model_interpret.html#model-interpretation-using-shap","text":"The aim of this module is providing tools for model interpretation using SHAP library. The class below is a convenience wrapper, that implements multiple plots for tree-based models.","title":"Model Interpretation using SHAP"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret","text":"","title":"probatus.interpret.model_interpret"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter","text":"This class is a wrapper that allows to easily analyse model's features. It allows to plot SHAP feature importance, SHAP summary plot and SHAP dependence plots. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split import numpy as np import pandas as pd feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 ) X = pd . DataFrame ( X , columns = feature_names ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Prepare and fit model. Remember about class_weight=\"balanced\" or an equivalent. clf = RandomForestClassifier ( class_weight = 'balanced' , n_estimators = 100 , max_depth = 2 , random_state = 0 ) clf . fit ( X_train , y_train ) # Train ShapModelAnalyser shap_interpreter = ShapModelInterpreter ( clf ) feature_importance = shap_interpreter . fit_compute ( X_train , X_test , y_train , y_test ) # Make plots shap_interpreter . plot ( 'importance' ) shap_interpreter . plot ( 'summary' ) shap_interpreter . plot ( 'dependence' , target_columns = [ 'f1' , 'f2' ]) shap_interpreter . plot ( 'sample' , samples_index = [ 521 , 78 ])","title":"ShapModelInterpreter"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter.__init__","text":"Initializes the class. Parameters: Name Type Description Default clf binary classifier Model fitted on X_train. required Source code in probatus/interpret/model_interpret.py def __init__ ( self , clf ): \"\"\" Initializes the class. Args: clf (binary classifier): Model fitted on X_train. \"\"\" self . clf = clf self . fitted = False","title":"__init__()"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter.compute","text":"Computes the DataFrame, that presents the importance of each feature. Returns: Type Description (pd.DataFrame) Dataframe with SHAP feature importance. Source code in probatus/interpret/model_interpret.py def compute ( self ): \"\"\" Computes the DataFrame, that presents the importance of each feature. Returns: (pd.DataFrame): Dataframe with SHAP feature importance. \"\"\" self . _check_if_fitted () # Compute SHAP importance self . importance_df = calculate_shap_importance ( self . shap_values , self . column_names ) return self . importance_df","title":"compute()"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter.fit","text":"Fits the object and calculates the shap values for the provided datasets. Parameters: Name Type Description Default X_train pd.DataFrame Dataframe containing training data. required X_test pd.DataFrame Dataframe containing test data. required y_train pd.Series Series of binary labels for train data. required y_test pd.Series Series of binary labels for test data. required column_names Optional, None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names Optional, None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None approximate boolean , if True uses shap approximations - less accurate, but very fast. False **shap_kwargs keyword arguments passed to shap.TreeExplainer. {} Source code in probatus/interpret/model_interpret.py def fit ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , approximate = False , ** shap_kwargs ): \"\"\" Fits the object and calculates the shap values for the provided datasets. Args: X_train (pd.DataFrame): Dataframe containing training data. X_test (pd.DataFrame): Dataframe containing test data. y_train (pd.Series): Series of binary labels for train data. y_test (pd.Series): Series of binary labels for test data. column_names (Optional, None, or list of str): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (Optional, None, or list of str): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. approximate (boolean):, if True uses shap approximations - less accurate, but very fast. **shap_kwargs: keyword arguments passed to shap.TreeExplainer. \"\"\" self . X_train = assure_pandas_df ( X_train ) self . X_test = assure_pandas_df ( X_test ) self . y_train = y_train self . y_test = y_test # Set class names self . class_names = class_names if self . class_names is None : self . class_names = [ 'Negative Class' , 'Positive Class' ] # Set column names self . column_names = assure_column_names_consistency ( column_names , self . X_train ) # Calculate Metrics self . auc_train = roc_auc_score ( self . y_train , self . clf . predict_proba ( self . X_train )[:, 1 ]) self . auc_test = roc_auc_score ( self . y_test , self . clf . predict_proba ( self . X_test )[:, 1 ]) self . results_text = \"Train AUC: {} , \\n Test AUC: {} .\" . format ( np . round ( self . auc_train , 3 ), np . round ( self . auc_test , 3 ) ) self . shap_values , self . explainer = shap_calc ( self . clf , self . X_test , approximate = approximate , return_explainer = True , data = self . X_train , ** shap_kwargs ) # Get expected_value from the explainer self . expected_value = self . explainer . expected_value # For sklearn models the expected values consists of two elements (negative_class and positive_class) if isinstance ( self . expected_value , list ) or isinstance ( self . expected_value , np . ndarray ): self . expected_value = self . expected_value [ 1 ] # Initialize tree dependence plotter self . tdp = TreeDependencePlotter ( self . clf ) . fit ( self . X_test , self . y_test , precalc_shap = self . shap_values ) self . fitted = True","title":"fit()"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter.fit_compute","text":"Fits the object and calculates the shap values for the provided datasets. Parameters: Name Type Description Default X_train pd.DataFrame Dataframe containing training data. required X_test pd.DataFrame Dataframe containing test data. required y_train pd.Series Series of binary labels for train data. required y_test pd.Series Series of binary labels for test data. required column_names Optional, None, or list of str List of feature names for the dataset. If None, then column names from the X_train dataframe are used. None class_names Optional, None, or list of str List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. None approximate boolean , if True uses shap approximations - less accurate, but very fast. False **shap_kwargs keyword arguments passed to shap.TreeExplainer. {} Returns: Type Description (pd.DataFrame) Dataframe with SHAP feature importance. Source code in probatus/interpret/model_interpret.py def fit_compute ( self , X_train , X_test , y_train , y_test , column_names = None , class_names = None , approximate = False , ** shap_kwargs ): \"\"\" Fits the object and calculates the shap values for the provided datasets. Args: X_train (pd.DataFrame): Dataframe containing training data. X_test (pd.DataFrame): Dataframe containing test data. y_train (pd.Series): Series of binary labels for train data. y_test (pd.Series): Series of binary labels for test data. column_names (Optional, None, or list of str): List of feature names for the dataset. If None, then column names from the X_train dataframe are used. class_names (Optional, None, or list of str): List of class names e.g. ['neg', 'pos']. If none, the default ['Negative Class', 'Positive Class'] are used. approximate (boolean):, if True uses shap approximations - less accurate, but very fast. **shap_kwargs: keyword arguments passed to shap.TreeExplainer. Returns: (pd.DataFrame): Dataframe with SHAP feature importance. \"\"\" self . fit ( X_train = X_train , X_test = X_test , y_train = y_train , y_test = y_test , column_names = column_names , class_names = class_names , approximate = approximate , ** shap_kwargs ) return self . compute ()","title":"fit_compute()"},{"location":"api/model_interpret.html#probatus.interpret.model_interpret.ShapModelInterpreter.plot","text":"Plots the appropriate SHAP plot Parameters: Name Type Description Default plot_type str One of the following: 'importance': Feature importance plot, SHAP bar summary plot 'summary': SHAP Summary plot 'dependence': Dependence plot for each feature 'sample': Explanation of a given sample in the test data required target_columns Optional, None, str or list of str List of features names, for which the plots should be generated. If None, all features will be plotted. None samples_index None, int, list or pd.Index Index of samples to be explained if the plot_type=sample . None **plot_kwargs Keyword arguments passed to the plot method. For 'importance' and 'summary' plot_type, the kwargs are passed to shap.summary_plot, for 'dependence' plot_type, they are passed to probatus.interpret.TreeDependencePlotter.feature_plot method. {} Source code in probatus/interpret/model_interpret.py def plot ( self , plot_type , target_columns = None , samples_index = None , ** plot_kwargs ): \"\"\" Plots the appropriate SHAP plot Args: plot_type (str): One of the following: - 'importance': Feature importance plot, SHAP bar summary plot - 'summary': SHAP Summary plot - 'dependence': Dependence plot for each feature - 'sample': Explanation of a given sample in the test data target_columns (Optional, None, str or list of str): List of features names, for which the plots should be generated. If None, all features will be plotted. samples_index (None, int, list or pd.Index): Index of samples to be explained if the `plot_type=sample`. **plot_kwargs: Keyword arguments passed to the plot method. For 'importance' and 'summary' plot_type, the kwargs are passed to shap.summary_plot, for 'dependence' plot_type, they are passed to probatus.interpret.TreeDependencePlotter.feature_plot method. \"\"\" if target_columns is None : target_columns = self . column_names target_columns = assure_list_of_strings ( target_columns , 'target_columns' ) target_columns_indices = [ self . column_names . index ( target_column ) for target_column in target_columns ] if plot_type in [ 'importance' , 'summary' ]: # Get the target features target_X_test = self . X_test [ target_columns ] target_shap_values = self . shap_values [:, target_columns_indices ] # Set summary plot settings if plot_type == 'importance' : plot_type = 'bar' plot_title = 'SHAP Feature Importance' else : plot_type = 'dot' plot_title = 'SHAP Summary plot' shap . summary_plot ( target_shap_values , target_X_test , plot_type = plot_type , class_names = self . class_names , show = False , ** plot_kwargs ) ax = plt . gca () ax . set_title ( plot_title ) ax . annotate ( self . results_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = 'axes fraction' , textcoords = 'offset points' , va = 'top' ) plt . show () elif plot_type == 'dependence' : ax = [] for feature_name in target_columns : print () ax . append ( self . tdp . plot ( feature = feature_name , figsize = ( 10 , 7 ), target_names = self . class_names )) plt . show () elif plot_type == 'sample' : # Ensure the correct samples_index type if samples_index is None : raise ( ValueError ( 'For sample plot, you need to specify the samples_index be plotted plot' )) elif isinstance ( samples_index , int ) or isinstance ( samples_index , str ): samples_index = [ samples_index ] elif not ( isinstance ( samples_index , list ) or isinstance ( samples_index , pd . Index )): raise ( TypeError ( 'sample_index must be one of the following: int, str, list or pd.Index' )) ax = [] for sample_index in samples_index : sample_loc = self . X_test . index . get_loc ( sample_index ) shap . plots . _waterfall . waterfall_legacy ( self . expected_value , self . shap_values [ sample_loc , :], self . X_test . loc [ sample_index ], show = False , ** plot_kwargs ) plot_title = f 'SHAP Sample Explanation for index= { sample_index } ' current_ax = plt . gca () current_ax . set_title ( plot_title ) ax . append ( current_ax ) plt . show () else : raise ValueError ( \"Wrong plot type, select from 'importance', 'summary', or 'dependence'\" ) if isinstance ( ax , list ) and len ( ax ) == 1 : ax = ax [ 0 ] return ax","title":"plot()"},{"location":"api/sample_similarity.html","text":"Sample Similarity \u00b6 The goal of sample similarity module is understanding how different two samples are from a multivariate perspective. One of the ways to indicate that is Resemblance Model. Having two datasets say X1 and X2, one can analyse how easy is it to recognize which dataset a randomly selected row comes from. The Resemblance model assigns label 0 to X1 dataset, and label 1 to X2 and trains a binary classification model that to predict, which sample a given row comes from. By looking at the test AUC, one can conclude that the samples have different distribution the AUC is significantly higher than 0.5. Further, by analysing feature importance one can understand, which of the features have predictive power. The following features are implemented: BaseResemblance - Base class, provides main functionality with fit method that can be overwritten by subclasses. SHAPImportanceResemblance (Recommended) - The class applies SHAP library, in order to interpret the tree based resemblance model model. PermutationImportanceResemblance - The class applies permutation feature importance, in order to understand, which features does the current model rely the most on. The higher the importance of the feature, the more a given feature possibly differs in X2 compared to X1. The importance indicates how much the test AUC drops if a given feature is permuted. \u00b6 BaseResemblanceModel \u00b6 This model checks for similarity of two samples. A possible use case is analysis whether train sample differs from test sample, due to e.g. non-stationarity. This is a base class and needs to be extended by a fit() method, which implements how data is split, how model is trained and evaluated. Further, inheriting classes need to implement how feature importance should be indicated. __init__ ( self , model , test_prc = 0.25 , n_jobs = 1 , random_state = 42 ) special \u00b6 Initializes the class. Parameters: Name Type Description Default model model object Binary classification model or pipeline. required test_prc float Percentage of data used to test the model. By default 0.25 is set. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 random_state int The seed used by the random number generator. 42 Source code in probatus/sample_similarity/resemblance_model.py def __init__ ( self , model , test_prc = 0.25 , n_jobs = 1 , random_state = 42 ): \"\"\" Initializes the class. Args: model (model object): Binary classification model or pipeline. test_prc (float, optional): Percentage of data used to test the model. By default 0.25 is set. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. random_state (int, optional): The seed used by the random number generator. \"\"\" self . model = model self . test_prc = test_prc self . n_jobs = n_jobs self . random_state = random_state self . fitted = False self . metric_name = 'roc_auc' self . scorer = get_scorers ( self . metric_name )[ 0 ] compute ( self , return_auc = False ) \u00b6 Checks if fit() method has been run and computes the output variables. Parameters: Name Type Description Default return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. False Returns: Type Description tuple(pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def compute ( self , return_auc = False ): \"\"\" Checks if fit() method has been run and computes the output variables. Args: return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. Returns: tuple(pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . _check_if_fitted () if return_auc : return self . report , self . auc_train , self . auc_test else : return self . report fit ( self , X1 , X2 , column_names = None ) \u00b6 Base fit functionality that should be executed before each fit. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Source code in probatus/sample_similarity/resemblance_model.py def fit ( self , X1 , X2 , column_names = None ): \"\"\" Base fit functionality that should be executed before each fit. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. \"\"\" # Set seed for results reproducibility np . random . seed ( self . random_state ) # Ensure inputs are correct self . X1 = assure_numpy_array ( X1 ) self . X2 = assure_numpy_array ( X2 ) # Check if any missing values warn_if_missing ( self . X1 , 'X1' ) warn_if_missing ( self . X2 , 'X2' ) # Ensure the same shapes if self . X1 . shape [ 1 ] != self . X2 . shape [ 1 ]: raise ( ValueError ( \"Passed variables do not have the same shape. The passed dimensions are {} and {} \" . format ( self . X1 . shape [ 1 ], self . X2 . shape [ 1 ]))) # Check if column_names are passed correctly self . column_names = assure_column_names_consistency ( column_names , X1 ) # Prepare dataset for modelling self . X = pd . DataFrame ( np . concatenate ([ self . X1 , self . X2 ]), columns = self . column_names ) . reset_index ( drop = True ) self . y = pd . Series ( np . concatenate ([ np . zeros ( self . X1 . shape [ 0 ]), np . ones ( self . X2 . shape [ 0 ]), ])) . reset_index ( drop = True ) # Reinitialize variables in case of multiple times being fit self . _init_output_variables () self . X_train , self . X_test , self . y_train , self . y_test = train_test_split ( self . X , self . y , test_size = self . test_prc , random_state = self . random_state , stratify = self . y ) self . model . fit ( self . X_train , self . y_train ) self . auc_train = np . round ( self . scorer . score ( self . model , self . X_train , self . y_train ), 3 ) self . auc_test = np . round ( self . scorer . score ( self . model , self . X_test , self . y_test ), 3 ) print ( f 'Finished model training: Train AUC { self . auc_train } ,' f ' Test AUC { self . auc_test } ' ) if self . auc_train > self . auc_test : warnings . warn ( 'Train AUC > Test AUC, which might indicate an overfit. \\n ' 'Strong overfit might lead to misleading conclusions when analysing feature importance. ' 'Consider retraining with more regularization applied to the model.' ) self . fitted = True fit_compute ( self , X1 , X2 , column_names = None , return_auc = False , ** fit_kwargs ) \u00b6 Fits the resemblance model and computes the report regarding feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. False **fit_kwargs keyword arguments passed to the fit() method: X1 : First sample to be compared. It needs to have the same number of columns as X2. X2 : Second sample to be compared. It needs to have the same number of columns as X1. column_names : List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. {} Returns: Type Description tuple of (pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def fit_compute ( self , X1 , X2 , column_names = None , return_auc = False , ** fit_kwargs ): \"\"\" Fits the resemblance model and computes the report regarding feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. **fit_kwargs: keyword arguments passed to the fit() method: - `X1`: First sample to be compared. It needs to have the same number of columns as X2. - `X2`: Second sample to be compared. It needs to have the same number of columns as X1. - `column_names`: List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: tuple of (pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . fit ( X1 , X2 , column_names = column_names , ** fit_kwargs ) return self . compute ( return_auc = return_auc ) get_data_splits ( self ) \u00b6 Returns the data splits used to train the Resemblance model. Returns: Type Description (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series) X_train, X_test, y_train, y_test. Source code in probatus/sample_similarity/resemblance_model.py def get_data_splits ( self ): \"\"\" Returns the data splits used to train the Resemblance model. Returns: (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series): X_train, X_test, y_train, y_test. \"\"\" self . _check_if_fitted () return self . X_train , self . X_test , self . y_train , self . y_test PermutationImportanceResemblance \u00b6 This model checks for similarity of two samples. A possible use case is analysis whether train sample differs from test sample, due to e.g. non-stationarity. It assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. If the model achieves test AUC significantly different than 0.5, it indicates that it is possible to distinguish the samples, and therefore, the samples differ. Features with high permutation importance contribute to that effect the most. Thus, their distribution might differ between two samples. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.sample_similarity import PermutationImportanceResemblance X1 , _ = make_classification ( n_samples = 1000 , n_features = 5 ) X2 , _ = make_classification ( n_samples = 1000 , n_features = 5 , shift = 0.5 ) clf = RandomForestClassifier ( max_depth = 2 ) perm = PermutationResemblanceModel ( clf ) feature_importance = perm . fit_compute ( X1 , X2 ) perm . plot () __init__ ( self , model , iterations = 100 , ** kwargs ) special \u00b6 Initializes the class. Parameters: Name Type Description Default model model object Binary classification model or pipeline. required iterations int Number of iterations performed to calculate permutation importance. By default 100 iterations per feature are done. 100 **kwargs Keyword arguments that can overwrite inherited default values in BaseResemblanceModel: test_prc : Percentage of data used to test the model. By default 0.25 is set. n_jobs : Number of parallel executions. If -1 use all available cores. By default 1. random_state : The seed used by the random number generator. {} Source code in probatus/sample_similarity/resemblance_model.py def __init__ ( self , model , iterations = 100 , ** kwargs ): \"\"\" Initializes the class. Args: model (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations performed to calculate permutation importance. By default 100 iterations per feature are done. **kwargs: Keyword arguments that can overwrite inherited default values in BaseResemblanceModel: - `test_prc`: Percentage of data used to test the model. By default 0.25 is set. - `n_jobs`: Number of parallel executions. If -1 use all available cores. By default 1. - `random_state`: The seed used by the random number generator. \"\"\" super () . __init__ ( model = model , ** kwargs ) self . iterations = iterations self . iterations_columns = [ 'feature' , 'importance' ] self . iterations_results = pd . DataFrame ( columns = self . iterations_columns ) self . plot_x_label = 'Permutation Feature Importance' self . plot_y_label = 'Feature Name' self . plot_title = 'Permutation Feature Importance of Resemblance Model' compute ( self , return_auc = False ) inherited \u00b6 Checks if fit() method has been run and computes the output variables. Parameters: Name Type Description Default return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. False Returns: Type Description tuple(pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def compute ( self , return_auc = False ): \"\"\" Checks if fit() method has been run and computes the output variables. Args: return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. Returns: tuple(pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . _check_if_fitted () if return_auc : return self . report , self . auc_train , self . auc_test else : return self . report fit ( self , X1 , X2 , column_names = None ) \u00b6 This function assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Source code in probatus/sample_similarity/resemblance_model.py def fit ( self , X1 , X2 , column_names = None ): \"\"\" This function assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. \"\"\" super () . fit ( X1 = X1 , X2 = X2 , column_names = column_names ) permutation_result = permutation_importance ( self . model , self . X_test , self . y_test , scoring = self . scorer . scorer , n_repeats = self . iterations , n_jobs = self . n_jobs ) # Prepare report self . report_columns = [ 'mean_importance' , 'std_importance' ] self . report = pd . DataFrame ( index = self . column_names , columns = self . report_columns , dtype = float ) for feature_index , feature_name in enumerate ( self . column_names ): # Fill in the report self . report . loc [ feature_name , 'mean_importance' ] = \\ permutation_result [ 'importances_mean' ][ feature_index ] self . report . loc [ feature_name , 'std_importance' ] = \\ permutation_result [ 'importances_std' ][ feature_index ] # Fill in the iterations current_iterations = pd . DataFrame ( np . stack ([ np . repeat ( feature_name , self . iterations ), permutation_result [ 'importances' ][ feature_index , :] . reshape (( self . iterations ,)) ], axis = 1 ), columns = self . iterations_columns ) self . iterations_results = pd . concat ([ self . iterations_results , current_iterations ]) self . iterations_results [ 'importance' ] = self . iterations_results [ 'importance' ] . astype ( float ) # Sort by mean test score of first metric self . report . sort_values ( by = 'mean_importance' , ascending = False , inplace = True ) fit_compute ( self , X1 , X2 , column_names = None , return_auc = False , ** fit_kwargs ) inherited \u00b6 Fits the resemblance model and computes the report regarding feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. False **fit_kwargs keyword arguments passed to the fit() method: X1 : First sample to be compared. It needs to have the same number of columns as X2. X2 : Second sample to be compared. It needs to have the same number of columns as X1. column_names : List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. {} Returns: Type Description tuple of (pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def fit_compute ( self , X1 , X2 , column_names = None , return_auc = False , ** fit_kwargs ): \"\"\" Fits the resemblance model and computes the report regarding feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. **fit_kwargs: keyword arguments passed to the fit() method: - `X1`: First sample to be compared. It needs to have the same number of columns as X2. - `X2`: Second sample to be compared. It needs to have the same number of columns as X1. - `column_names`: List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: tuple of (pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . fit ( X1 , X2 , column_names = column_names , ** fit_kwargs ) return self . compute ( return_auc = return_auc ) get_data_splits ( self ) inherited \u00b6 Returns the data splits used to train the Resemblance model. Returns: Type Description (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series) X_train, X_test, y_train, y_test. Source code in probatus/sample_similarity/resemblance_model.py def get_data_splits ( self ): \"\"\" Returns the data splits used to train the Resemblance model. Returns: (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series): X_train, X_test, y_train, y_test. \"\"\" self . _check_if_fitted () return self . X_train , self . X_test , self . y_train , self . y_test plot ( self , ax = None , top_n = None ) \u00b6 Plots the resulting AUC of the model as well as the feature importances. Parameters: Name Type Description Default ax matplotlib.axes Axes to which the output should be plotted. If not provided a new axes are created. None top_n int Number of the most important features to be plotted. By default are features are included into the plot. None Returns: Type Description matplotlib.axes, optional Axes that include the plot. Source code in probatus/sample_similarity/resemblance_model.py def plot ( self , ax = None , top_n = None ): \"\"\" Plots the resulting AUC of the model as well as the feature importances. Args: ax (matplotlib.axes, optional): Axes to which the output should be plotted. If not provided a new axes are created. top_n (int, optional): Number of the most important features to be plotted. By default are features are included into the plot. Returns: matplotlib.axes, optional: Axes that include the plot. \"\"\" feature_report = self . compute () self . iterations_results [ 'importance' ] = self . iterations_results [ 'importance' ] . astype ( float ) sorted_features = feature_report [ 'mean_importance' ] . \\ sort_values ( ascending = True ) . index . values if top_n is not None and top_n > 0 : sorted_features = sorted_features [ - top_n :] if ax is None : height_per_subplot = len ( sorted_features ) / 2. + 1 width_per_subplot = 10 fig , ax = plt . subplots ( figsize = ( width_per_subplot , height_per_subplot )) for position , feature in enumerate ( sorted_features ): ax . boxplot ( self . iterations_results [ self . iterations_results [ 'feature' ] == feature ][ 'importance' ], positions = [ position ], vert = False ) ax . set_yticks ( range ( position + 1 )) ax . set_yticklabels ( sorted_features ) ax . set_xlabel ( self . plot_x_label ) ax . set_ylabel ( self . plot_y_label ) ax . set_title ( self . plot_title ) fig_text = \"Train AUC: {} , \\n \" \\ \"Test AUC: {} .\" . \\ format ( self . auc_train , self . auc_test ) ax . annotate ( fig_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = 'axes fraction' , textcoords = 'offset points' , va = 'top' ) return ax SHAPImportanceResemblance \u00b6 This model checks for similarity of two samples. A possible use case is analysis whether train sample differs from test sample, due to e.g. non-stationarity. It assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. If the model achieves test AUC significantly different than 0.5, it indicates that it is possible to distinguish the samples, and therefore, the samples differ. Features with high permutation importance contribute to that effect the most. Thus, their distribution might differ between two samples. This class currently works only with the Tree based models. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.sample_similarity import SHAPImportanceResemblance X1 , _ = make_classification ( n_samples = 1000 , n_features = 5 ) X2 , _ = make_classification ( n_samples = 1000 , n_features = 5 , shift = 0.5 ) clf = RandomForestClassifier ( max_depth = 2 ) rm = SHAPImportanceResemblance ( clf ) feature_importance = rm . fit_compute ( X1 , X2 ) rm . plot () __init__ ( self , model , ** kwargs ) special \u00b6 Initializes the class. Parameters: Name Type Description Default model model object Binary classification model or pipeline. required **kwargs Keyword arguments that can overwrite inherited default values in BaseResemblanceModel: test_prc : Percentage of data used to test the model. By default 0.25 is set. n_jobs : Number of parallel executions. If -1 use all available cores. By default 1. random_state : The seed used by the random number generator. {} Source code in probatus/sample_similarity/resemblance_model.py def __init__ ( self , model , ** kwargs ): \"\"\" Initializes the class. Args: model (model object): Binary classification model or pipeline. **kwargs: Keyword arguments that can overwrite inherited default values in BaseResemblanceModel: - `test_prc`: Percentage of data used to test the model. By default 0.25 is set. - `n_jobs`: Number of parallel executions. If -1 use all available cores. By default 1. - `random_state`: The seed used by the random number generator. \"\"\" super () . __init__ ( model = model , ** kwargs ) self . plot_title = 'SHAP summary plot' compute ( self , return_auc = False ) inherited \u00b6 Checks if fit() method has been run and computes the output variables. Parameters: Name Type Description Default return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. False Returns: Type Description tuple(pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def compute ( self , return_auc = False ): \"\"\" Checks if fit() method has been run and computes the output variables. Args: return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. Returns: tuple(pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . _check_if_fitted () if return_auc : return self . report , self . auc_train , self . auc_test else : return self . report fit ( self , X1 , X2 , column_names = None ) \u00b6 This function assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Source code in probatus/sample_similarity/resemblance_model.py def fit ( self , X1 , X2 , column_names = None ): \"\"\" This function assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. \"\"\" super () . fit ( X1 = X1 , X2 = X2 , column_names = column_names ) self . shap_values_test = shap_calc ( self . model , self . X_test , data = self . X_train ) self . report = calculate_shap_importance ( self . shap_values_test , self . column_names ) fit_compute ( self , X1 , X2 , column_names = None , return_auc = False , ** fit_kwargs ) inherited \u00b6 Fits the resemblance model and computes the report regarding feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. False **fit_kwargs keyword arguments passed to the fit() method: X1 : First sample to be compared. It needs to have the same number of columns as X2. X2 : Second sample to be compared. It needs to have the same number of columns as X1. column_names : List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. {} Returns: Type Description tuple of (pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def fit_compute ( self , X1 , X2 , column_names = None , return_auc = False , ** fit_kwargs ): \"\"\" Fits the resemblance model and computes the report regarding feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. **fit_kwargs: keyword arguments passed to the fit() method: - `X1`: First sample to be compared. It needs to have the same number of columns as X2. - `X2`: Second sample to be compared. It needs to have the same number of columns as X1. - `column_names`: List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: tuple of (pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . fit ( X1 , X2 , column_names = column_names , ** fit_kwargs ) return self . compute ( return_auc = return_auc ) get_data_splits ( self ) inherited \u00b6 Returns the data splits used to train the Resemblance model. Returns: Type Description (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series) X_train, X_test, y_train, y_test. Source code in probatus/sample_similarity/resemblance_model.py def get_data_splits ( self ): \"\"\" Returns the data splits used to train the Resemblance model. Returns: (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series): X_train, X_test, y_train, y_test. \"\"\" self . _check_if_fitted () return self . X_train , self . X_test , self . y_train , self . y_test get_shap_values ( self ) \u00b6 Gets the SHAP values generated on the test set. Returns: Type Description (np.array) SHAP values generated on the test set. Source code in probatus/sample_similarity/resemblance_model.py def get_shap_values ( self ): ''' Gets the SHAP values generated on the test set. Returns: (np.array) SHAP values generated on the test set. ''' self . _check_if_fitted () return self . shap_values_test plot ( self , plot_type = 'bar' , ** summary_plot_kwargs ) \u00b6 Plots the resulting AUC of the model as well as the feature importances. Parameters: Name Type Description Default plot_type Optional, str Type of plot, used to compute shap.summary_plot. By default 'bar', available ones are \"dot\", \"bar\", \"violin\", 'bar' **summary_plot_kwargs kwargs passed to the shap.summary_plot. {} Returns: Type Description matplotlib.axes, optional Axes that include the plot. Source code in probatus/sample_similarity/resemblance_model.py def plot ( self , plot_type = 'bar' , ** summary_plot_kwargs ): \"\"\" Plots the resulting AUC of the model as well as the feature importances. Args: plot_type (Optional, str): Type of plot, used to compute shap.summary_plot. By default 'bar', available ones are \"dot\", \"bar\", \"violin\", **summary_plot_kwargs: kwargs passed to the shap.summary_plot. Returns: matplotlib.axes, optional: Axes that include the plot. \"\"\" # This line serves as a double check if the object has been fitted self . _check_if_fitted () shap . summary_plot ( self . shap_values_test , self . X_test , plot_type = plot_type , class_names = [ 'First Sample' , 'Second Sample' ], show = False , ** summary_plot_kwargs ) ax = plt . gca () ax . set_title ( self . plot_title ) fig_text = \"Train AUC: {} , \\n \" \\ \"Test AUC: {} .\" . \\ format ( self . auc_train , self . auc_test ) ax . annotate ( fig_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = 'axes fraction' , textcoords = 'offset points' , va = 'top' ) return ax","title":"probatus.sample_similarity"},{"location":"api/sample_similarity.html#sample-similarity","text":"The goal of sample similarity module is understanding how different two samples are from a multivariate perspective. One of the ways to indicate that is Resemblance Model. Having two datasets say X1 and X2, one can analyse how easy is it to recognize which dataset a randomly selected row comes from. The Resemblance model assigns label 0 to X1 dataset, and label 1 to X2 and trains a binary classification model that to predict, which sample a given row comes from. By looking at the test AUC, one can conclude that the samples have different distribution the AUC is significantly higher than 0.5. Further, by analysing feature importance one can understand, which of the features have predictive power. The following features are implemented: BaseResemblance - Base class, provides main functionality with fit method that can be overwritten by subclasses. SHAPImportanceResemblance (Recommended) - The class applies SHAP library, in order to interpret the tree based resemblance model model. PermutationImportanceResemblance - The class applies permutation feature importance, in order to understand, which features does the current model rely the most on. The higher the importance of the feature, the more a given feature possibly differs in X2 compared to X1. The importance indicates how much the test AUC drops if a given feature is permuted.","title":"Sample Similarity"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model","text":"","title":"probatus.sample_similarity.resemblance_model"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.BaseResemblanceModel","text":"This model checks for similarity of two samples. A possible use case is analysis whether train sample differs from test sample, due to e.g. non-stationarity. This is a base class and needs to be extended by a fit() method, which implements how data is split, how model is trained and evaluated. Further, inheriting classes need to implement how feature importance should be indicated.","title":"BaseResemblanceModel"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.BaseResemblanceModel.__init__","text":"Initializes the class. Parameters: Name Type Description Default model model object Binary classification model or pipeline. required test_prc float Percentage of data used to test the model. By default 0.25 is set. 0.25 n_jobs int Number of parallel executions. If -1 use all available cores. By default 1. 1 random_state int The seed used by the random number generator. 42 Source code in probatus/sample_similarity/resemblance_model.py def __init__ ( self , model , test_prc = 0.25 , n_jobs = 1 , random_state = 42 ): \"\"\" Initializes the class. Args: model (model object): Binary classification model or pipeline. test_prc (float, optional): Percentage of data used to test the model. By default 0.25 is set. n_jobs (int, optional): Number of parallel executions. If -1 use all available cores. By default 1. random_state (int, optional): The seed used by the random number generator. \"\"\" self . model = model self . test_prc = test_prc self . n_jobs = n_jobs self . random_state = random_state self . fitted = False self . metric_name = 'roc_auc' self . scorer = get_scorers ( self . metric_name )[ 0 ]","title":"__init__()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.BaseResemblanceModel.compute","text":"Checks if fit() method has been run and computes the output variables. Parameters: Name Type Description Default return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. False Returns: Type Description tuple(pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def compute ( self , return_auc = False ): \"\"\" Checks if fit() method has been run and computes the output variables. Args: return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. Returns: tuple(pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . _check_if_fitted () if return_auc : return self . report , self . auc_train , self . auc_test else : return self . report","title":"compute()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.BaseResemblanceModel.fit","text":"Base fit functionality that should be executed before each fit. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Source code in probatus/sample_similarity/resemblance_model.py def fit ( self , X1 , X2 , column_names = None ): \"\"\" Base fit functionality that should be executed before each fit. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. \"\"\" # Set seed for results reproducibility np . random . seed ( self . random_state ) # Ensure inputs are correct self . X1 = assure_numpy_array ( X1 ) self . X2 = assure_numpy_array ( X2 ) # Check if any missing values warn_if_missing ( self . X1 , 'X1' ) warn_if_missing ( self . X2 , 'X2' ) # Ensure the same shapes if self . X1 . shape [ 1 ] != self . X2 . shape [ 1 ]: raise ( ValueError ( \"Passed variables do not have the same shape. The passed dimensions are {} and {} \" . format ( self . X1 . shape [ 1 ], self . X2 . shape [ 1 ]))) # Check if column_names are passed correctly self . column_names = assure_column_names_consistency ( column_names , X1 ) # Prepare dataset for modelling self . X = pd . DataFrame ( np . concatenate ([ self . X1 , self . X2 ]), columns = self . column_names ) . reset_index ( drop = True ) self . y = pd . Series ( np . concatenate ([ np . zeros ( self . X1 . shape [ 0 ]), np . ones ( self . X2 . shape [ 0 ]), ])) . reset_index ( drop = True ) # Reinitialize variables in case of multiple times being fit self . _init_output_variables () self . X_train , self . X_test , self . y_train , self . y_test = train_test_split ( self . X , self . y , test_size = self . test_prc , random_state = self . random_state , stratify = self . y ) self . model . fit ( self . X_train , self . y_train ) self . auc_train = np . round ( self . scorer . score ( self . model , self . X_train , self . y_train ), 3 ) self . auc_test = np . round ( self . scorer . score ( self . model , self . X_test , self . y_test ), 3 ) print ( f 'Finished model training: Train AUC { self . auc_train } ,' f ' Test AUC { self . auc_test } ' ) if self . auc_train > self . auc_test : warnings . warn ( 'Train AUC > Test AUC, which might indicate an overfit. \\n ' 'Strong overfit might lead to misleading conclusions when analysing feature importance. ' 'Consider retraining with more regularization applied to the model.' ) self . fitted = True","title":"fit()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.BaseResemblanceModel.fit_compute","text":"Fits the resemblance model and computes the report regarding feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. False **fit_kwargs keyword arguments passed to the fit() method: X1 : First sample to be compared. It needs to have the same number of columns as X2. X2 : Second sample to be compared. It needs to have the same number of columns as X1. column_names : List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. {} Returns: Type Description tuple of (pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def fit_compute ( self , X1 , X2 , column_names = None , return_auc = False , ** fit_kwargs ): \"\"\" Fits the resemblance model and computes the report regarding feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. **fit_kwargs: keyword arguments passed to the fit() method: - `X1`: First sample to be compared. It needs to have the same number of columns as X2. - `X2`: Second sample to be compared. It needs to have the same number of columns as X1. - `column_names`: List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: tuple of (pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . fit ( X1 , X2 , column_names = column_names , ** fit_kwargs ) return self . compute ( return_auc = return_auc )","title":"fit_compute()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.BaseResemblanceModel.get_data_splits","text":"Returns the data splits used to train the Resemblance model. Returns: Type Description (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series) X_train, X_test, y_train, y_test. Source code in probatus/sample_similarity/resemblance_model.py def get_data_splits ( self ): \"\"\" Returns the data splits used to train the Resemblance model. Returns: (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series): X_train, X_test, y_train, y_test. \"\"\" self . _check_if_fitted () return self . X_train , self . X_test , self . y_train , self . y_test","title":"get_data_splits()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance","text":"This model checks for similarity of two samples. A possible use case is analysis whether train sample differs from test sample, due to e.g. non-stationarity. It assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. If the model achieves test AUC significantly different than 0.5, it indicates that it is possible to distinguish the samples, and therefore, the samples differ. Features with high permutation importance contribute to that effect the most. Thus, their distribution might differ between two samples. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.sample_similarity import PermutationImportanceResemblance X1 , _ = make_classification ( n_samples = 1000 , n_features = 5 ) X2 , _ = make_classification ( n_samples = 1000 , n_features = 5 , shift = 0.5 ) clf = RandomForestClassifier ( max_depth = 2 ) perm = PermutationResemblanceModel ( clf ) feature_importance = perm . fit_compute ( X1 , X2 ) perm . plot ()","title":"PermutationImportanceResemblance"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.__init__","text":"Initializes the class. Parameters: Name Type Description Default model model object Binary classification model or pipeline. required iterations int Number of iterations performed to calculate permutation importance. By default 100 iterations per feature are done. 100 **kwargs Keyword arguments that can overwrite inherited default values in BaseResemblanceModel: test_prc : Percentage of data used to test the model. By default 0.25 is set. n_jobs : Number of parallel executions. If -1 use all available cores. By default 1. random_state : The seed used by the random number generator. {} Source code in probatus/sample_similarity/resemblance_model.py def __init__ ( self , model , iterations = 100 , ** kwargs ): \"\"\" Initializes the class. Args: model (model object): Binary classification model or pipeline. iterations (int, optional): Number of iterations performed to calculate permutation importance. By default 100 iterations per feature are done. **kwargs: Keyword arguments that can overwrite inherited default values in BaseResemblanceModel: - `test_prc`: Percentage of data used to test the model. By default 0.25 is set. - `n_jobs`: Number of parallel executions. If -1 use all available cores. By default 1. - `random_state`: The seed used by the random number generator. \"\"\" super () . __init__ ( model = model , ** kwargs ) self . iterations = iterations self . iterations_columns = [ 'feature' , 'importance' ] self . iterations_results = pd . DataFrame ( columns = self . iterations_columns ) self . plot_x_label = 'Permutation Feature Importance' self . plot_y_label = 'Feature Name' self . plot_title = 'Permutation Feature Importance of Resemblance Model'","title":"__init__()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.compute","text":"Checks if fit() method has been run and computes the output variables. Parameters: Name Type Description Default return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. False Returns: Type Description tuple(pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def compute ( self , return_auc = False ): \"\"\" Checks if fit() method has been run and computes the output variables. Args: return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. Returns: tuple(pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . _check_if_fitted () if return_auc : return self . report , self . auc_train , self . auc_test else : return self . report","title":"compute()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.fit","text":"This function assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Source code in probatus/sample_similarity/resemblance_model.py def fit ( self , X1 , X2 , column_names = None ): \"\"\" This function assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using permutation importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. \"\"\" super () . fit ( X1 = X1 , X2 = X2 , column_names = column_names ) permutation_result = permutation_importance ( self . model , self . X_test , self . y_test , scoring = self . scorer . scorer , n_repeats = self . iterations , n_jobs = self . n_jobs ) # Prepare report self . report_columns = [ 'mean_importance' , 'std_importance' ] self . report = pd . DataFrame ( index = self . column_names , columns = self . report_columns , dtype = float ) for feature_index , feature_name in enumerate ( self . column_names ): # Fill in the report self . report . loc [ feature_name , 'mean_importance' ] = \\ permutation_result [ 'importances_mean' ][ feature_index ] self . report . loc [ feature_name , 'std_importance' ] = \\ permutation_result [ 'importances_std' ][ feature_index ] # Fill in the iterations current_iterations = pd . DataFrame ( np . stack ([ np . repeat ( feature_name , self . iterations ), permutation_result [ 'importances' ][ feature_index , :] . reshape (( self . iterations ,)) ], axis = 1 ), columns = self . iterations_columns ) self . iterations_results = pd . concat ([ self . iterations_results , current_iterations ]) self . iterations_results [ 'importance' ] = self . iterations_results [ 'importance' ] . astype ( float ) # Sort by mean test score of first metric self . report . sort_values ( by = 'mean_importance' , ascending = False , inplace = True )","title":"fit()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.fit_compute","text":"Fits the resemblance model and computes the report regarding feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. False **fit_kwargs keyword arguments passed to the fit() method: X1 : First sample to be compared. It needs to have the same number of columns as X2. X2 : Second sample to be compared. It needs to have the same number of columns as X1. column_names : List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. {} Returns: Type Description tuple of (pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def fit_compute ( self , X1 , X2 , column_names = None , return_auc = False , ** fit_kwargs ): \"\"\" Fits the resemblance model and computes the report regarding feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. **fit_kwargs: keyword arguments passed to the fit() method: - `X1`: First sample to be compared. It needs to have the same number of columns as X2. - `X2`: Second sample to be compared. It needs to have the same number of columns as X1. - `column_names`: List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: tuple of (pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . fit ( X1 , X2 , column_names = column_names , ** fit_kwargs ) return self . compute ( return_auc = return_auc )","title":"fit_compute()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.get_data_splits","text":"Returns the data splits used to train the Resemblance model. Returns: Type Description (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series) X_train, X_test, y_train, y_test. Source code in probatus/sample_similarity/resemblance_model.py def get_data_splits ( self ): \"\"\" Returns the data splits used to train the Resemblance model. Returns: (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series): X_train, X_test, y_train, y_test. \"\"\" self . _check_if_fitted () return self . X_train , self . X_test , self . y_train , self . y_test","title":"get_data_splits()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.PermutationImportanceResemblance.plot","text":"Plots the resulting AUC of the model as well as the feature importances. Parameters: Name Type Description Default ax matplotlib.axes Axes to which the output should be plotted. If not provided a new axes are created. None top_n int Number of the most important features to be plotted. By default are features are included into the plot. None Returns: Type Description matplotlib.axes, optional Axes that include the plot. Source code in probatus/sample_similarity/resemblance_model.py def plot ( self , ax = None , top_n = None ): \"\"\" Plots the resulting AUC of the model as well as the feature importances. Args: ax (matplotlib.axes, optional): Axes to which the output should be plotted. If not provided a new axes are created. top_n (int, optional): Number of the most important features to be plotted. By default are features are included into the plot. Returns: matplotlib.axes, optional: Axes that include the plot. \"\"\" feature_report = self . compute () self . iterations_results [ 'importance' ] = self . iterations_results [ 'importance' ] . astype ( float ) sorted_features = feature_report [ 'mean_importance' ] . \\ sort_values ( ascending = True ) . index . values if top_n is not None and top_n > 0 : sorted_features = sorted_features [ - top_n :] if ax is None : height_per_subplot = len ( sorted_features ) / 2. + 1 width_per_subplot = 10 fig , ax = plt . subplots ( figsize = ( width_per_subplot , height_per_subplot )) for position , feature in enumerate ( sorted_features ): ax . boxplot ( self . iterations_results [ self . iterations_results [ 'feature' ] == feature ][ 'importance' ], positions = [ position ], vert = False ) ax . set_yticks ( range ( position + 1 )) ax . set_yticklabels ( sorted_features ) ax . set_xlabel ( self . plot_x_label ) ax . set_ylabel ( self . plot_y_label ) ax . set_title ( self . plot_title ) fig_text = \"Train AUC: {} , \\n \" \\ \"Test AUC: {} .\" . \\ format ( self . auc_train , self . auc_test ) ax . annotate ( fig_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = 'axes fraction' , textcoords = 'offset points' , va = 'top' ) return ax","title":"plot()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance","text":"This model checks for similarity of two samples. A possible use case is analysis whether train sample differs from test sample, due to e.g. non-stationarity. It assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. If the model achieves test AUC significantly different than 0.5, it indicates that it is possible to distinguish the samples, and therefore, the samples differ. Features with high permutation importance contribute to that effect the most. Thus, their distribution might differ between two samples. This class currently works only with the Tree based models. Examples: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from probatus.sample_similarity import SHAPImportanceResemblance X1 , _ = make_classification ( n_samples = 1000 , n_features = 5 ) X2 , _ = make_classification ( n_samples = 1000 , n_features = 5 , shift = 0.5 ) clf = RandomForestClassifier ( max_depth = 2 ) rm = SHAPImportanceResemblance ( clf ) feature_importance = rm . fit_compute ( X1 , X2 ) rm . plot ()","title":"SHAPImportanceResemblance"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.__init__","text":"Initializes the class. Parameters: Name Type Description Default model model object Binary classification model or pipeline. required **kwargs Keyword arguments that can overwrite inherited default values in BaseResemblanceModel: test_prc : Percentage of data used to test the model. By default 0.25 is set. n_jobs : Number of parallel executions. If -1 use all available cores. By default 1. random_state : The seed used by the random number generator. {} Source code in probatus/sample_similarity/resemblance_model.py def __init__ ( self , model , ** kwargs ): \"\"\" Initializes the class. Args: model (model object): Binary classification model or pipeline. **kwargs: Keyword arguments that can overwrite inherited default values in BaseResemblanceModel: - `test_prc`: Percentage of data used to test the model. By default 0.25 is set. - `n_jobs`: Number of parallel executions. If -1 use all available cores. By default 1. - `random_state`: The seed used by the random number generator. \"\"\" super () . __init__ ( model = model , ** kwargs ) self . plot_title = 'SHAP summary plot'","title":"__init__()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.compute","text":"Checks if fit() method has been run and computes the output variables. Parameters: Name Type Description Default return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. False Returns: Type Description tuple(pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def compute ( self , return_auc = False ): \"\"\" Checks if fit() method has been run and computes the output variables. Args: return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or feature importances. By default the second option is selected. Returns: tuple(pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . _check_if_fitted () if return_auc : return self . report , self . auc_train , self . auc_test else : return self . report","title":"compute()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.fit","text":"This function assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None Source code in probatus/sample_similarity/resemblance_model.py def fit ( self , X1 , X2 , column_names = None ): \"\"\" This function assigns to labels to each sample, 0 to first sample, 1 to the second. Then, It randomly selects a portion of data to train on. The resulting model tries to distinguish which sample does a given test row comes from. This provides insights on how distinguishable these samples are and which features contribute to that. The feature importance is calculated using SHAP feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. \"\"\" super () . fit ( X1 = X1 , X2 = X2 , column_names = column_names ) self . shap_values_test = shap_calc ( self . model , self . X_test , data = self . X_train ) self . report = calculate_shap_importance ( self . shap_values_test , self . column_names )","title":"fit()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.fit_compute","text":"Fits the resemblance model and computes the report regarding feature importance. Parameters: Name Type Description Default X1 np.ndarray or pd.DataFrame First sample to be compared. It needs to have the same number of columns as X2. required X2 np.ndarray or pd.DataFrame Second sample to be compared. It needs to have the same number of columns as X1. required column_names list of str List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. None return_auc bool Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. False **fit_kwargs keyword arguments passed to the fit() method: X1 : First sample to be compared. It needs to have the same number of columns as X2. X2 : Second sample to be compared. It needs to have the same number of columns as X1. column_names : List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. {} Returns: Type Description tuple of (pd.DataFrame, float, float) or pd.DataFrame Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. Source code in probatus/sample_similarity/resemblance_model.py def fit_compute ( self , X1 , X2 , column_names = None , return_auc = False , ** fit_kwargs ): \"\"\" Fits the resemblance model and computes the report regarding feature importance. Args: X1 (np.ndarray or pd.DataFrame): First sample to be compared. It needs to have the same number of columns as X2. X2 (np.ndarray or pd.DataFrame): Second sample to be compared. It needs to have the same number of columns as X1. column_names (list of str, optional): List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. return_auc (bool, optional): Flag indicating whether the method should return a tuple (feature importances, train AUC, test AUC), or only feature importances. By default the second option is selected. **fit_kwargs: keyword arguments passed to the fit() method: - `X1`: First sample to be compared. It needs to have the same number of columns as X2. - `X2`: Second sample to be compared. It needs to have the same number of columns as X1. - `column_names`: List of feature names of the provided samples. If provided it will be used to overwrite the existing feature names. If not provided the existing feature names are used or default feature names are generated. Returns: tuple of (pd.DataFrame, float, float) or pd.DataFrame: Depending on value of return_tuple either returns a tuple (feature importances, train AUC, test AUC), or feature importances. \"\"\" self . fit ( X1 , X2 , column_names = column_names , ** fit_kwargs ) return self . compute ( return_auc = return_auc )","title":"fit_compute()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.get_data_splits","text":"Returns the data splits used to train the Resemblance model. Returns: Type Description (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series) X_train, X_test, y_train, y_test. Source code in probatus/sample_similarity/resemblance_model.py def get_data_splits ( self ): \"\"\" Returns the data splits used to train the Resemblance model. Returns: (pd.DataFrame, pd.DataFrame, pd.Series, pd.Series): X_train, X_test, y_train, y_test. \"\"\" self . _check_if_fitted () return self . X_train , self . X_test , self . y_train , self . y_test","title":"get_data_splits()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.get_shap_values","text":"Gets the SHAP values generated on the test set. Returns: Type Description (np.array) SHAP values generated on the test set. Source code in probatus/sample_similarity/resemblance_model.py def get_shap_values ( self ): ''' Gets the SHAP values generated on the test set. Returns: (np.array) SHAP values generated on the test set. ''' self . _check_if_fitted () return self . shap_values_test","title":"get_shap_values()"},{"location":"api/sample_similarity.html#probatus.sample_similarity.resemblance_model.SHAPImportanceResemblance.plot","text":"Plots the resulting AUC of the model as well as the feature importances. Parameters: Name Type Description Default plot_type Optional, str Type of plot, used to compute shap.summary_plot. By default 'bar', available ones are \"dot\", \"bar\", \"violin\", 'bar' **summary_plot_kwargs kwargs passed to the shap.summary_plot. {} Returns: Type Description matplotlib.axes, optional Axes that include the plot. Source code in probatus/sample_similarity/resemblance_model.py def plot ( self , plot_type = 'bar' , ** summary_plot_kwargs ): \"\"\" Plots the resulting AUC of the model as well as the feature importances. Args: plot_type (Optional, str): Type of plot, used to compute shap.summary_plot. By default 'bar', available ones are \"dot\", \"bar\", \"violin\", **summary_plot_kwargs: kwargs passed to the shap.summary_plot. Returns: matplotlib.axes, optional: Axes that include the plot. \"\"\" # This line serves as a double check if the object has been fitted self . _check_if_fitted () shap . summary_plot ( self . shap_values_test , self . X_test , plot_type = plot_type , class_names = [ 'First Sample' , 'Second Sample' ], show = False , ** summary_plot_kwargs ) ax = plt . gca () ax . set_title ( self . plot_title ) fig_text = \"Train AUC: {} , \\n \" \\ \"Test AUC: {} .\" . \\ format ( self . auc_train , self . auc_test ) ax . annotate ( fig_text , ( 0 , 0 ), ( 0 , - 50 ), fontsize = 12 , xycoords = 'axes fraction' , textcoords = 'offset points' , va = 'top' ) return ax","title":"plot()"},{"location":"api/stat_tests.html","text":"Statistical Tests \u00b6 This module allows to apply different statistical tests. \u00b6 AutoDist \u00b6 Class to automatically apply all implemented statistical distribution tests and binning strategies to (a selection of) features in two dataframes. Examples: df1 = pd . DataFrame ( np . random . normal ( size = ( 1000 , 2 )), columns = [ 'feat_0' , 'feat_1' ]) df2 = pd . DataFrame ( np . random . normal ( size = ( 1000 , 2 )), columns = [ 'feat_0' , 'feat_1' ]) myAutoDist = AutoDist ( statistical_tests = 'all' , binning_strategies = 'all' , bin_count = [ 10 , 20 ]) res = myAutoDist . fit ( df1 , df2 , columns = df1 . columns ) __init__ ( self , statistical_tests = 'all' , binning_strategies = 'default' , bin_count = 10 ) special \u00b6 Initializes the class. Parameters: Name Type Description Default statistical_tests Optional, str Statistical tests to apply, either list of tests names, or 'all'. Statistical methods implemented: 'ES': Epps-Singleton, 'KS': Kolmogorov-Smirnov statistic, 'PSI': Population Stability Index, 'AD': Anderson-Darling TS. 'all' binning_strategies Optional, str Binning strategies to apply for each test, either list of tests names, 'all' or 'default'. Binning strategies that can be chosen: 'SimpleBucketer': equally spaced bins, 'AgglomerativeBucketer': binning by applying the Scikit-learn implementation of Agglomerative Clustering, 'QuantileBucketer': bins with equal number of elements, None: no binning is applied. Note that not all statistical tests will be performed since some () require binning strategies. 'default': applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. 'all': each binning strategy is used for each statistical test 'default' bin_count integer, None or list of integers bin_count value(s) to be used, note that None can only be used when no bucketing strategy is applied. 10 Source code in probatus/stat_tests/distribution_statistics.py def __init__ ( self , statistical_tests = \"all\" , binning_strategies = \"default\" , bin_count = 10 ): \"\"\" Initializes the class. Args: statistical_tests (Optional, str): Statistical tests to apply, either list of tests names, or 'all'. Statistical methods implemented: - 'ES': Epps-Singleton, - 'KS': Kolmogorov-Smirnov statistic, - 'PSI': Population Stability Index, - 'AD': Anderson-Darling TS. binning_strategies (Optional, str): Binning strategies to apply for each test, either list of tests names, 'all' or 'default'. Binning strategies that can be chosen: - 'SimpleBucketer': equally spaced bins, - 'AgglomerativeBucketer': binning by applying the Scikit-learn implementation of Agglomerative Clustering, - 'QuantileBucketer': bins with equal number of elements, - None: no binning is applied. Note that not all statistical tests will be performed since some () require binning strategies. - 'default': applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. - 'all': each binning strategy is used for each statistical test bin_count (integer, None or list of integers): bin_count value(s) to be used, note that None can only be used when no bucketing strategy is applied. \"\"\" self . fitted = False # Initialize statistical tests to be performed if statistical_tests == \"all\" : self . statistical_tests = list ( DistributionStatistics . statistical_test_dict . keys () ) elif isinstance ( statistical_tests , str ): self . statistical_tests = [ statistical_tests ] else : self . statistical_tests = statistical_tests # Initialize binning strategies to be used if binning_strategies == \"all\" : self . binning_strategies = list ( DistributionStatistics . binning_strategy_dict . keys () ) elif isinstance ( binning_strategies , str ): self . binning_strategies = [ binning_strategies ] elif binning_strategies is None : self . binning_strategies = [ None ] else : self . binning_strategies = binning_strategies if not isinstance ( bin_count , list ): self . bin_count = [ bin_count ] else : self . bin_count = bin_count compute ( self , df1 , df2 , column_names = None , return_failed_tests = True , suppress_warnings = False ) \u00b6 Fit the AutoDist object to data; i.e. apply the statistical tests and binning strategies Parameters: Name Type Description Default df1 pd.DataFrame dataframe 1 for distribution comparison with dataframe 2. required df2 pd.DataFrame dataframe 2 for distribution comparison with dataframe 1. required column_names list of str list of columns in df1 and df2 that should be compared. If None, all column names will be compared None return_failed_tests bool remove tests in result that did not succeed. True suppress_warnings bool whether to suppress warnings during the fit process. False Returns: Type Description (pd.DataFrame) dataframe with results of the performed statistical tests and binning strategies. Source code in probatus/stat_tests/distribution_statistics.py def compute ( self , df1 , df2 , column_names = None , return_failed_tests = True , suppress_warnings = False , ): \"\"\" Fit the AutoDist object to data; i.e. apply the statistical tests and binning strategies Args: df1 (pd.DataFrame): dataframe 1 for distribution comparison with dataframe 2. df2 (pd.DataFrame): dataframe 2 for distribution comparison with dataframe 1. column_names (list of str): list of columns in df1 and df2 that should be compared. If None, all column names will be compared return_failed_tests (bool): remove tests in result that did not succeed. suppress_warnings (bool): whether to suppress warnings during the fit process. Returns: (pd.DataFrame): dataframe with results of the performed statistical tests and binning strategies. \"\"\" if column_names is None : column_names = df1 . columns . to_list () if len ( set ( column_names ) - set ( df2 . columns )): raise Exception ( \"column_names was set to None but columns in provided dataframes are different\" ) # Check if all columns in column_names are in df1 and df2 elif len ( set ( column_names ) - set ( df1 . columns )) or len ( set ( column_names ) - set ( df2 . columns ) ): raise Exception ( \"Not all columns in `column_names` are in the provided dataframes\" ) # Calculate statistics and p-values for all combinations result_all = pd . DataFrame () for col , stat_test , bin_strat , bins in tqdm ( list ( itertools . product ( column_names , self . statistical_tests , self . binning_strategies , self . bin_count , ) ) ): if self . binning_strategies == [ \"default\" ]: bin_strat = DistributionStatistics . statistical_test_dict [ stat_test ][ \"default_binning\" ] dist = DistributionStatistics ( statistical_test = stat_test , binning_strategy = bin_strat , bin_count = bins ) try : if suppress_warnings : warnings . filterwarnings ( \"ignore\" ) _ = dist . compute ( df1 [ col ], df2 [ col ]) if suppress_warnings : warnings . filterwarnings ( \"default\" ) statistic = dist . statistic p_value = dist . p_value except : statistic , p_value = \"an error occurred\" , None pass # Append result to results list result_ = { \"column\" : col , \"statistical_test\" : stat_test , \"binning_strategy\" : bin_strat , \"bin_count\" : bins , \"statistic\" : statistic , \"p_value\" : p_value , } result_all = result_all . append ( result_ , ignore_index = True ) if not return_failed_tests : result_all = result_all [ result_all [ \"statistic\" ] != \"an error occurred\" ] self . fitted = True self . _result = result_all [ [ \"column\" , \"statistical_test\" , \"binning_strategy\" , \"bin_count\" , \"statistic\" , \"p_value\" , ] ] self . _result [ \"bin_count\" ] = self . _result [ \"bin_count\" ] . astype ( int ) self . _result . loc [ self . _result [ \"binning_strategy\" ] . isnull (), \"bin_count\" ] = 0 self . _result . loc [ self . _result [ \"binning_strategy\" ] . isnull (), \"binning_strategy\" ] = \"no_bucketing\" # Remove duplicates that appear if multiple bin numbers are passed, and binning strategy None self . _result = self . _result . \\ drop_duplicates ( subset = [ 'column' , 'statistical_test' , 'binning_strategy' , 'bin_count' ], keep = 'first' ) # create pivot table as final output self . result = pd . pivot_table ( self . _result , values = [ \"statistic\" , \"p_value\" ], index = \"column\" , columns = [ \"statistical_test\" , \"binning_strategy\" , \"bin_count\" ], aggfunc = \"sum\" , ) # flatten multi-index self . result . columns = [ \"_\" . join ([ str ( x ) for x in line ]) for line in self . result . columns . values ] self . result . reset_index ( inplace = True ) return self . result DistributionStatistics \u00b6 Wrapper that applies a statistical method to compare two distributions. Depending on a test, one can also apply binning of the data. Examples: d1 = np . histogram ( np . random . normal ( size = 1000 ), 10 )[ 0 ] d2 = np . histogram ( np . random . normal ( size = 1000 ), 10 )[ 0 ] myTest = DistributionStatistics ( 'KS' , bin_count = 10 ) myTest . compute ( d1 , d2 , verbose = True ) __init__ ( self , statistical_test , binning_strategy = 'default' , bin_count = 10 ) special \u00b6 Initializes the class. Parameters: Name Type Description Default statistical_test string Statistical method to apply, statistical methods implemented: 'ES': Epps-Singleton 'KS': Kolmogorov-Smirnov statistic 'PSI': Population Stability Index 'SW': Shapiro-Wilk based difference statistic 'AD': Anderson-Darling TS required binning_strategy Optional, string Binning strategy to apply, binning strategies implemented: 'simplebucketer': equally spaced bins 'agglomerativebucketer': binning by applying the Scikit-learn implementation of Agglomerative Clustering 'quantilebucketer': bins with equal number of elements 'default': applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. None: no binning is applied. The test is computed based on original distribution. 'default' bin_count Optional, int In case binning_strategy is not None, specify the number of bins to be used by the binning strategy. By default 10 bins are used. 10 Source code in probatus/stat_tests/distribution_statistics.py def __init__ ( self , statistical_test , binning_strategy = \"default\" , bin_count = 10 ): \"\"\" Initializes the class. Args: statistical_test (string): Statistical method to apply, statistical methods implemented: - 'ES': Epps-Singleton - 'KS': Kolmogorov-Smirnov statistic - 'PSI': Population Stability Index - 'SW': Shapiro-Wilk based difference statistic - 'AD': Anderson-Darling TS binning_strategy (Optional, string): Binning strategy to apply, binning strategies implemented: - 'simplebucketer': equally spaced bins - 'agglomerativebucketer': binning by applying the Scikit-learn implementation of Agglomerative Clustering - 'quantilebucketer': bins with equal number of elements - 'default': applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. - None: no binning is applied. The test is computed based on original distribution. bin_count (Optional, int): In case binning_strategy is not None, specify the number of bins to be used by the binning strategy. By default 10 bins are used. \"\"\" self . statistical_test = statistical_test . upper () self . binning_strategy = binning_strategy self . bin_count = bin_count self . fitted = False # Initialize the statistical test if self . statistical_test not in self . statistical_test_dict : raise NotImplementedError ( \"The statistical test should be one of {} \" . format ( self . statistical_test_dict . keys () ) ) else : self . statistical_test_name = self . statistical_test_dict [ self . statistical_test ][ \"name\" ] self . _statistical_test_function = self . statistical_test_dict [ self . statistical_test ][ \"func\" ] # Initialize the binning strategy if self . binning_strategy : self . binning_strategy = self . binning_strategy . lower () if self . binning_strategy == \"default\" : self . binning_strategy = self . statistical_test_dict [ self . statistical_test ][ \"default_binning\" ] if self . binning_strategy not in self . binning_strategy_dict : raise NotImplementedError ( \"The binning strategy should be one of {} \" . format ( list ( self . binning_strategy_dict . keys ()) ) ) else : binner = self . binning_strategy_dict [ self . binning_strategy ] if binner is not None : self . binner = binner ( bin_count = self . bin_count ) compute ( self , d1 , d2 , verbose = False , ** kwargs ) \u00b6 Apply the statistical test and compute statistic value and p-value. Parameters: Name Type Description Default d1 (np.array or pd.DataFrame): distribution 1. required d2 (np.array or pd.DataFrame): distribution 2. required verbose (bool): Flag indicating whether prints should be shown. False **kwargs Keyword arguments passed to the statistical test function: verbose : helpful interpretation msgs printed to stdout (default False). {} Returns: Type Description (Tuple of floats) statistic value and p_value. For PSI test the return is only statistic Source code in probatus/stat_tests/distribution_statistics.py def compute ( self , d1 , d2 , verbose = False , ** kwargs ): \"\"\" Apply the statistical test and compute statistic value and p-value. Args: d1: (np.array or pd.DataFrame): distribution 1. d2: (np.array or pd.DataFrame): distribution 2. verbose: (bool): Flag indicating whether prints should be shown. **kwargs: Keyword arguments passed to the statistical test function: - `verbose`: helpful interpretation msgs printed to stdout (default False). Returns: (Tuple of floats): statistic value and p_value. For PSI test the return is only statistic \"\"\" check_numeric_dtypes ( d1 ) check_numeric_dtypes ( d2 ) # Bin the data if self . binning_strategy : self . binner . fit ( d1 ) d1_preprocessed = self . binner . counts_ d2_preprocessed = self . binner . compute ( d2 ) else : d1_preprocessed , d2_preprocessed = d1 , d2 # Perform the statistical test res = self . _statistical_test_function ( d1_preprocessed , d2_preprocessed , verbose = verbose , ** kwargs ) self . fitted = True # Check form of results and return if type ( res ) == tuple : self . statistic , self . p_value = res return self . statistic , self . p_value else : self . statistic = res return self . statistic","title":"probatus.stat_tests"},{"location":"api/stat_tests.html#statistical-tests","text":"This module allows to apply different statistical tests.","title":"Statistical Tests"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics","text":"","title":"probatus.stat_tests.distribution_statistics"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.AutoDist","text":"Class to automatically apply all implemented statistical distribution tests and binning strategies to (a selection of) features in two dataframes. Examples: df1 = pd . DataFrame ( np . random . normal ( size = ( 1000 , 2 )), columns = [ 'feat_0' , 'feat_1' ]) df2 = pd . DataFrame ( np . random . normal ( size = ( 1000 , 2 )), columns = [ 'feat_0' , 'feat_1' ]) myAutoDist = AutoDist ( statistical_tests = 'all' , binning_strategies = 'all' , bin_count = [ 10 , 20 ]) res = myAutoDist . fit ( df1 , df2 , columns = df1 . columns )","title":"AutoDist"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.AutoDist.__init__","text":"Initializes the class. Parameters: Name Type Description Default statistical_tests Optional, str Statistical tests to apply, either list of tests names, or 'all'. Statistical methods implemented: 'ES': Epps-Singleton, 'KS': Kolmogorov-Smirnov statistic, 'PSI': Population Stability Index, 'AD': Anderson-Darling TS. 'all' binning_strategies Optional, str Binning strategies to apply for each test, either list of tests names, 'all' or 'default'. Binning strategies that can be chosen: 'SimpleBucketer': equally spaced bins, 'AgglomerativeBucketer': binning by applying the Scikit-learn implementation of Agglomerative Clustering, 'QuantileBucketer': bins with equal number of elements, None: no binning is applied. Note that not all statistical tests will be performed since some () require binning strategies. 'default': applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. 'all': each binning strategy is used for each statistical test 'default' bin_count integer, None or list of integers bin_count value(s) to be used, note that None can only be used when no bucketing strategy is applied. 10 Source code in probatus/stat_tests/distribution_statistics.py def __init__ ( self , statistical_tests = \"all\" , binning_strategies = \"default\" , bin_count = 10 ): \"\"\" Initializes the class. Args: statistical_tests (Optional, str): Statistical tests to apply, either list of tests names, or 'all'. Statistical methods implemented: - 'ES': Epps-Singleton, - 'KS': Kolmogorov-Smirnov statistic, - 'PSI': Population Stability Index, - 'AD': Anderson-Darling TS. binning_strategies (Optional, str): Binning strategies to apply for each test, either list of tests names, 'all' or 'default'. Binning strategies that can be chosen: - 'SimpleBucketer': equally spaced bins, - 'AgglomerativeBucketer': binning by applying the Scikit-learn implementation of Agglomerative Clustering, - 'QuantileBucketer': bins with equal number of elements, - None: no binning is applied. Note that not all statistical tests will be performed since some () require binning strategies. - 'default': applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. - 'all': each binning strategy is used for each statistical test bin_count (integer, None or list of integers): bin_count value(s) to be used, note that None can only be used when no bucketing strategy is applied. \"\"\" self . fitted = False # Initialize statistical tests to be performed if statistical_tests == \"all\" : self . statistical_tests = list ( DistributionStatistics . statistical_test_dict . keys () ) elif isinstance ( statistical_tests , str ): self . statistical_tests = [ statistical_tests ] else : self . statistical_tests = statistical_tests # Initialize binning strategies to be used if binning_strategies == \"all\" : self . binning_strategies = list ( DistributionStatistics . binning_strategy_dict . keys () ) elif isinstance ( binning_strategies , str ): self . binning_strategies = [ binning_strategies ] elif binning_strategies is None : self . binning_strategies = [ None ] else : self . binning_strategies = binning_strategies if not isinstance ( bin_count , list ): self . bin_count = [ bin_count ] else : self . bin_count = bin_count","title":"__init__()"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.AutoDist.compute","text":"Fit the AutoDist object to data; i.e. apply the statistical tests and binning strategies Parameters: Name Type Description Default df1 pd.DataFrame dataframe 1 for distribution comparison with dataframe 2. required df2 pd.DataFrame dataframe 2 for distribution comparison with dataframe 1. required column_names list of str list of columns in df1 and df2 that should be compared. If None, all column names will be compared None return_failed_tests bool remove tests in result that did not succeed. True suppress_warnings bool whether to suppress warnings during the fit process. False Returns: Type Description (pd.DataFrame) dataframe with results of the performed statistical tests and binning strategies. Source code in probatus/stat_tests/distribution_statistics.py def compute ( self , df1 , df2 , column_names = None , return_failed_tests = True , suppress_warnings = False , ): \"\"\" Fit the AutoDist object to data; i.e. apply the statistical tests and binning strategies Args: df1 (pd.DataFrame): dataframe 1 for distribution comparison with dataframe 2. df2 (pd.DataFrame): dataframe 2 for distribution comparison with dataframe 1. column_names (list of str): list of columns in df1 and df2 that should be compared. If None, all column names will be compared return_failed_tests (bool): remove tests in result that did not succeed. suppress_warnings (bool): whether to suppress warnings during the fit process. Returns: (pd.DataFrame): dataframe with results of the performed statistical tests and binning strategies. \"\"\" if column_names is None : column_names = df1 . columns . to_list () if len ( set ( column_names ) - set ( df2 . columns )): raise Exception ( \"column_names was set to None but columns in provided dataframes are different\" ) # Check if all columns in column_names are in df1 and df2 elif len ( set ( column_names ) - set ( df1 . columns )) or len ( set ( column_names ) - set ( df2 . columns ) ): raise Exception ( \"Not all columns in `column_names` are in the provided dataframes\" ) # Calculate statistics and p-values for all combinations result_all = pd . DataFrame () for col , stat_test , bin_strat , bins in tqdm ( list ( itertools . product ( column_names , self . statistical_tests , self . binning_strategies , self . bin_count , ) ) ): if self . binning_strategies == [ \"default\" ]: bin_strat = DistributionStatistics . statistical_test_dict [ stat_test ][ \"default_binning\" ] dist = DistributionStatistics ( statistical_test = stat_test , binning_strategy = bin_strat , bin_count = bins ) try : if suppress_warnings : warnings . filterwarnings ( \"ignore\" ) _ = dist . compute ( df1 [ col ], df2 [ col ]) if suppress_warnings : warnings . filterwarnings ( \"default\" ) statistic = dist . statistic p_value = dist . p_value except : statistic , p_value = \"an error occurred\" , None pass # Append result to results list result_ = { \"column\" : col , \"statistical_test\" : stat_test , \"binning_strategy\" : bin_strat , \"bin_count\" : bins , \"statistic\" : statistic , \"p_value\" : p_value , } result_all = result_all . append ( result_ , ignore_index = True ) if not return_failed_tests : result_all = result_all [ result_all [ \"statistic\" ] != \"an error occurred\" ] self . fitted = True self . _result = result_all [ [ \"column\" , \"statistical_test\" , \"binning_strategy\" , \"bin_count\" , \"statistic\" , \"p_value\" , ] ] self . _result [ \"bin_count\" ] = self . _result [ \"bin_count\" ] . astype ( int ) self . _result . loc [ self . _result [ \"binning_strategy\" ] . isnull (), \"bin_count\" ] = 0 self . _result . loc [ self . _result [ \"binning_strategy\" ] . isnull (), \"binning_strategy\" ] = \"no_bucketing\" # Remove duplicates that appear if multiple bin numbers are passed, and binning strategy None self . _result = self . _result . \\ drop_duplicates ( subset = [ 'column' , 'statistical_test' , 'binning_strategy' , 'bin_count' ], keep = 'first' ) # create pivot table as final output self . result = pd . pivot_table ( self . _result , values = [ \"statistic\" , \"p_value\" ], index = \"column\" , columns = [ \"statistical_test\" , \"binning_strategy\" , \"bin_count\" ], aggfunc = \"sum\" , ) # flatten multi-index self . result . columns = [ \"_\" . join ([ str ( x ) for x in line ]) for line in self . result . columns . values ] self . result . reset_index ( inplace = True ) return self . result","title":"compute()"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.DistributionStatistics","text":"Wrapper that applies a statistical method to compare two distributions. Depending on a test, one can also apply binning of the data. Examples: d1 = np . histogram ( np . random . normal ( size = 1000 ), 10 )[ 0 ] d2 = np . histogram ( np . random . normal ( size = 1000 ), 10 )[ 0 ] myTest = DistributionStatistics ( 'KS' , bin_count = 10 ) myTest . compute ( d1 , d2 , verbose = True )","title":"DistributionStatistics"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.DistributionStatistics.__init__","text":"Initializes the class. Parameters: Name Type Description Default statistical_test string Statistical method to apply, statistical methods implemented: 'ES': Epps-Singleton 'KS': Kolmogorov-Smirnov statistic 'PSI': Population Stability Index 'SW': Shapiro-Wilk based difference statistic 'AD': Anderson-Darling TS required binning_strategy Optional, string Binning strategy to apply, binning strategies implemented: 'simplebucketer': equally spaced bins 'agglomerativebucketer': binning by applying the Scikit-learn implementation of Agglomerative Clustering 'quantilebucketer': bins with equal number of elements 'default': applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. None: no binning is applied. The test is computed based on original distribution. 'default' bin_count Optional, int In case binning_strategy is not None, specify the number of bins to be used by the binning strategy. By default 10 bins are used. 10 Source code in probatus/stat_tests/distribution_statistics.py def __init__ ( self , statistical_test , binning_strategy = \"default\" , bin_count = 10 ): \"\"\" Initializes the class. Args: statistical_test (string): Statistical method to apply, statistical methods implemented: - 'ES': Epps-Singleton - 'KS': Kolmogorov-Smirnov statistic - 'PSI': Population Stability Index - 'SW': Shapiro-Wilk based difference statistic - 'AD': Anderson-Darling TS binning_strategy (Optional, string): Binning strategy to apply, binning strategies implemented: - 'simplebucketer': equally spaced bins - 'agglomerativebucketer': binning by applying the Scikit-learn implementation of Agglomerative Clustering - 'quantilebucketer': bins with equal number of elements - 'default': applies a default binning for a given stats_test. For all tests appart from PSI, no binning (None) is used. For PSI by default quantilebucketer is used. - None: no binning is applied. The test is computed based on original distribution. bin_count (Optional, int): In case binning_strategy is not None, specify the number of bins to be used by the binning strategy. By default 10 bins are used. \"\"\" self . statistical_test = statistical_test . upper () self . binning_strategy = binning_strategy self . bin_count = bin_count self . fitted = False # Initialize the statistical test if self . statistical_test not in self . statistical_test_dict : raise NotImplementedError ( \"The statistical test should be one of {} \" . format ( self . statistical_test_dict . keys () ) ) else : self . statistical_test_name = self . statistical_test_dict [ self . statistical_test ][ \"name\" ] self . _statistical_test_function = self . statistical_test_dict [ self . statistical_test ][ \"func\" ] # Initialize the binning strategy if self . binning_strategy : self . binning_strategy = self . binning_strategy . lower () if self . binning_strategy == \"default\" : self . binning_strategy = self . statistical_test_dict [ self . statistical_test ][ \"default_binning\" ] if self . binning_strategy not in self . binning_strategy_dict : raise NotImplementedError ( \"The binning strategy should be one of {} \" . format ( list ( self . binning_strategy_dict . keys ()) ) ) else : binner = self . binning_strategy_dict [ self . binning_strategy ] if binner is not None : self . binner = binner ( bin_count = self . bin_count )","title":"__init__()"},{"location":"api/stat_tests.html#probatus.stat_tests.distribution_statistics.DistributionStatistics.compute","text":"Apply the statistical test and compute statistic value and p-value. Parameters: Name Type Description Default d1 (np.array or pd.DataFrame): distribution 1. required d2 (np.array or pd.DataFrame): distribution 2. required verbose (bool): Flag indicating whether prints should be shown. False **kwargs Keyword arguments passed to the statistical test function: verbose : helpful interpretation msgs printed to stdout (default False). {} Returns: Type Description (Tuple of floats) statistic value and p_value. For PSI test the return is only statistic Source code in probatus/stat_tests/distribution_statistics.py def compute ( self , d1 , d2 , verbose = False , ** kwargs ): \"\"\" Apply the statistical test and compute statistic value and p-value. Args: d1: (np.array or pd.DataFrame): distribution 1. d2: (np.array or pd.DataFrame): distribution 2. verbose: (bool): Flag indicating whether prints should be shown. **kwargs: Keyword arguments passed to the statistical test function: - `verbose`: helpful interpretation msgs printed to stdout (default False). Returns: (Tuple of floats): statistic value and p_value. For PSI test the return is only statistic \"\"\" check_numeric_dtypes ( d1 ) check_numeric_dtypes ( d2 ) # Bin the data if self . binning_strategy : self . binner . fit ( d1 ) d1_preprocessed = self . binner . counts_ d2_preprocessed = self . binner . compute ( d2 ) else : d1_preprocessed , d2_preprocessed = d1 , d2 # Perform the statistical test res = self . _statistical_test_function ( d1_preprocessed , d2_preprocessed , verbose = verbose , ** kwargs ) self . fitted = True # Check form of results and return if type ( res ) == tuple : self . statistic , self . p_value = res return self . statistic , self . p_value else : self . statistic = res return self . statistic","title":"compute()"},{"location":"tutorials/nb_binning.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Binning \u00b6 % matplotlib inline % config Completer . use_jedi = False % load_ext autoreload % autoreload 2 import pandas as pd import numpy as np import matplotlib.pyplot as plt pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.max_row' , 500 ) pd . set_option ( 'display.max_colwidth' , 200 ) This notebook explains how the various implemented binning strategies of probatus work. First, we import all binning strategies: from probatus.binning import AgglomerativeBucketer , SimpleBucketer , QuantileBucketer , TreeBucketer Let's create some data on which we want to apply the binning strategies. We choose a logistic function because it clearly supports the explanation on how binning strategies work. Moreover, the typical reliability curve for a trained random forest model has this shape and binning strategies could be used for probability calibration (see also the website of Scikit-learn on probability calibration ). def log_function ( x ): return 1 / ( 1 + np . exp ( - x )) x = [ log_function ( x ) for x in np . arange ( - 10 , 10 , 0.01 )] plt . plot ( x ); Simple binning \u00b6 The SimpleBucketer object creates binning of the values in x into equally sized bins. The attributes counts , the number of elements per bin, and boundaries , the actual boundaries that resulted from the binning strategy are assigned to the object instance. In this example we choose to get 4 bins: mySimpleBucketer = SimpleBucketer ( bin_count = 4 ) mySimpleBucketer . fit ( x ) print ( 'counts' , mySimpleBucketer . counts_ ) print ( 'boundaries' , mySimpleBucketer . boundaries_ ) counts [891 109 110 890] boundaries [4.53978687e-05 2.50022585e-01 4.99999772e-01 7.49976959e-01 9.99954146e-01] df = pd . DataFrame ({ 'x' : x }) df [ 'label' ] = pd . cut ( x , bins = mySimpleBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . plot ( ax = ax , y = 'x' , legend = False ) As can be seen, the number of elements in the tails of the data is larger than in the middle: df . groupby ( 'label' )[ 'x' ] . count () . plot ( kind = 'bar' ); plt . title ( 'Histogram' ); Quantile binning \u00b6 The QuantileBucketer object creates bins that all contain an equal amount of samples myQuantileBucketer = QuantileBucketer ( bin_count = 4 ) myQuantileBucketer . fit ( x ) print ( 'counts' , myQuantileBucketer . counts_ ) print ( 'boundaries' , myQuantileBucketer . boundaries_ ) counts [500 500 500 500] boundaries [4.53978687e-05 6.67631251e-03 4.98750010e-01 9.93257042e-01 9.99954146e-01] df = pd . DataFrame ({ 'x' : x }) df [ 'label' ] = pd . cut ( x , bins = myQuantileBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . plot ( ax = ax , y = 'x' , legend = False ) As can be seen, the number of elements is the same in all bins: df . groupby ( 'label' )[ 'x' ] . count () . plot ( kind = 'bar' ); plt . title ( 'Histogram' ); Binning by agglomerative clustering \u00b6 The AgglomerativeBucketer class applies the Scikit-Learn AgglomerativeClustering algorithm to the data and uses the clusters to determine the bins. We use different data to show the value of this algoritm; we create the following distribution: x_agglomerative = np . append ( np . random . normal ( 0 , 1 , size = 1000 ), np . random . normal ( 6 , 0.2 , size = 50 )) plt . hist ( x_agglomerative , bins = 20 ); plt . title ( 'Sample data' ); When we apply the AgglomerativeBucketer algorithm with 2 bins, we see that the algorithm nicely creates a split in between the two centers myAgglomerativeBucketer = AgglomerativeBucketer ( bin_count = 2 ) myAgglomerativeBucketer . fit ( x_agglomerative ) print ( 'counts' , myAgglomerativeBucketer . counts_ ) print ( 'boundaries' , myAgglomerativeBucketer . boundaries_ ) counts [1000 50] boundaries [-2.71525699097944, 4.582406874429196, 6.454492599188006] df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = myAgglomerativeBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax ) Note that the SimpleBucketer strategy would just have created a split in the middle of the maximum and the minimum (at about 1.75). The QuantileBucketer strategy had created two bins with equal amount of elements in it, resulting in a split at around 0. counts_agglomerative_simple , boundaries_agglomerative_simple = SimpleBucketer . simple_bins ( x_agglomerative , 2 ) df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = boundaries_agglomerative_simple , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax ) counts_agglomerative_quantile , boundaries_agglomerative_quantile = QuantileBucketer . quantile_bins ( x_agglomerative , 2 ) df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = boundaries_agglomerative_quantile , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax ) Binning with Decision Trees \u00b6 Binning with decision trees leverages the information of a binary feature or the binary target in order to create buckets that have a significantly different proportion of the binary feature/target. It works by fitting a tree on 1 feature only. It leverages the properties of the split finder algorithm in the decision tree. The splits are done to maximize the gini/entropy. The leaves approximate the optimal bins. The example below shows a distribution defined by a step function def make_step_function ( x ): if x < 4 : return 0.001 elif x < 6 : return 0.3 elif x < 8 : return 0.5 elif x < 9 : return 0.95 else : return 0.9999 x = np . arange ( 0 , 10 , 0.001 ) probs = [ make_step_function ( x_ ) for x_ in x ] y = np . array ([ 1 if np . random . rand () < prob else 0 for prob in probs ]) fig , ax = plt . subplots () ax2 = ax . twinx () ax . hist ( x [ y == 0 ], alpha = 0.15 ) ax . hist ( x [ y == 1 ], alpha = 0.15 ) ax2 . plot ( x , probs , color = 'black' ) plt . show () The light blue histogram indicates the distribution of class 0 ( y=0 ), while the light orange histogram indicates the distribution of class 1 ( y=1 ). The black line indicates the probability function that isused to assign class 0 or 1. In this toy example, it's a step function. # Try a tree bucketer tb = TreeBucketer ( inf_edges = True , max_depth = 4 , criterion = 'entropy' , min_samples_leaf = 400 , #Minimum number of entries in the bins min_impurity_decrease = 0.001 ) . fit ( x , y ) counts_tree , boundaries_tree = tb . counts_ , tb . boundaries_ df_tree = pd . DataFrame ({ 'x' : x , 'y' : y , 'probs' : probs }) df_tree [ 'label' ] = pd . cut ( x , bins = boundaries_tree , include_lowest = True ) # Try a quantile bucketer myQuantileBucketer = QuantileBucketer ( bin_count = 16 ) myQuantileBucketer . fit ( x ) q_boundaries = myQuantileBucketer . boundaries_ q_counts = myQuantileBucketer . counts_ df_q = pd . DataFrame ({ 'x' : x , 'y' : y , 'probs' : probs }) df_q [ 'label' ] = pd . cut ( x , bins = q_boundaries , include_lowest = True ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) for label in df_tree . label . unique (): df_tree [ df_tree . label == label ] . plot ( ax = ax [ 0 ], x = 'x' , y = 'probs' , legend = False ) ax [ 0 ] . scatter ( df_tree [ df_tree . label == label ][ 'x' ] . mean (), df_tree [ df_tree . label == label ][ 'y' ] . mean ()) ax [ 0 ] . set_title ( \"Tree bucketer\" ) for label in df_q . label . unique (): df_q [ df_q . label == label ] . plot ( ax = ax [ 1 ], x = 'x' , y = 'probs' , legend = False ) ax [ 1 ] . scatter ( df_q [ df_q . label == label ][ 'x' ] . mean (), df_q [ df_q . label == label ][ 'y' ] . mean ()) ax [ 1 ] . set_title ( \"Quantile bucketer\" ) print ( f \"counts by TreeBucketer: { counts_tree } \" ) print ( f \"counts by QuantileBucketer: { q_counts } \" ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 17985.87it/s] counts by TreeBucketer: [4000 1998 2001 936 1065] counts by QuantileBucketer: [625 625 625 625 625 625 625 625 625 625 625 625 625 625 625 625] Comparing the TreeBucketer and the QuantileBucketer (the dots compare the average distribution of class 1 in the bin): Each buckets obtained by the TreeBucketer follow the probability distribution (ie the entries in the bucket have the same probability of being class 1). On the contrary, the QuantileBucketer splits the values below 4 in 6 buckets, which all have the same probability of being class 1. Note also that the tree is grown with the maximum depth of 4, which potentially let's it grow up to 16 buckets ($2^4$). The learned tree is visualized below, whreere the splitting according to the step function is visualized clearly. from sklearn.tree import plot_tree fig , ax = plt . subplots ( figsize = ( 12 , 5 )) tre_out = plot_tree ( tb . tree , ax = ax )","title":"Nb binning"},{"location":"tutorials/nb_binning.html#binning","text":"% matplotlib inline % config Completer . use_jedi = False % load_ext autoreload % autoreload 2 import pandas as pd import numpy as np import matplotlib.pyplot as plt pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.max_row' , 500 ) pd . set_option ( 'display.max_colwidth' , 200 ) This notebook explains how the various implemented binning strategies of probatus work. First, we import all binning strategies: from probatus.binning import AgglomerativeBucketer , SimpleBucketer , QuantileBucketer , TreeBucketer Let's create some data on which we want to apply the binning strategies. We choose a logistic function because it clearly supports the explanation on how binning strategies work. Moreover, the typical reliability curve for a trained random forest model has this shape and binning strategies could be used for probability calibration (see also the website of Scikit-learn on probability calibration ). def log_function ( x ): return 1 / ( 1 + np . exp ( - x )) x = [ log_function ( x ) for x in np . arange ( - 10 , 10 , 0.01 )] plt . plot ( x );","title":"Binning"},{"location":"tutorials/nb_binning.html#simple-binning","text":"The SimpleBucketer object creates binning of the values in x into equally sized bins. The attributes counts , the number of elements per bin, and boundaries , the actual boundaries that resulted from the binning strategy are assigned to the object instance. In this example we choose to get 4 bins: mySimpleBucketer = SimpleBucketer ( bin_count = 4 ) mySimpleBucketer . fit ( x ) print ( 'counts' , mySimpleBucketer . counts_ ) print ( 'boundaries' , mySimpleBucketer . boundaries_ ) counts [891 109 110 890] boundaries [4.53978687e-05 2.50022585e-01 4.99999772e-01 7.49976959e-01 9.99954146e-01] df = pd . DataFrame ({ 'x' : x }) df [ 'label' ] = pd . cut ( x , bins = mySimpleBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . plot ( ax = ax , y = 'x' , legend = False ) As can be seen, the number of elements in the tails of the data is larger than in the middle: df . groupby ( 'label' )[ 'x' ] . count () . plot ( kind = 'bar' ); plt . title ( 'Histogram' );","title":"Simple binning"},{"location":"tutorials/nb_binning.html#quantile-binning","text":"The QuantileBucketer object creates bins that all contain an equal amount of samples myQuantileBucketer = QuantileBucketer ( bin_count = 4 ) myQuantileBucketer . fit ( x ) print ( 'counts' , myQuantileBucketer . counts_ ) print ( 'boundaries' , myQuantileBucketer . boundaries_ ) counts [500 500 500 500] boundaries [4.53978687e-05 6.67631251e-03 4.98750010e-01 9.93257042e-01 9.99954146e-01] df = pd . DataFrame ({ 'x' : x }) df [ 'label' ] = pd . cut ( x , bins = myQuantileBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . plot ( ax = ax , y = 'x' , legend = False ) As can be seen, the number of elements is the same in all bins: df . groupby ( 'label' )[ 'x' ] . count () . plot ( kind = 'bar' ); plt . title ( 'Histogram' );","title":"Quantile binning"},{"location":"tutorials/nb_binning.html#binning-by-agglomerative-clustering","text":"The AgglomerativeBucketer class applies the Scikit-Learn AgglomerativeClustering algorithm to the data and uses the clusters to determine the bins. We use different data to show the value of this algoritm; we create the following distribution: x_agglomerative = np . append ( np . random . normal ( 0 , 1 , size = 1000 ), np . random . normal ( 6 , 0.2 , size = 50 )) plt . hist ( x_agglomerative , bins = 20 ); plt . title ( 'Sample data' ); When we apply the AgglomerativeBucketer algorithm with 2 bins, we see that the algorithm nicely creates a split in between the two centers myAgglomerativeBucketer = AgglomerativeBucketer ( bin_count = 2 ) myAgglomerativeBucketer . fit ( x_agglomerative ) print ( 'counts' , myAgglomerativeBucketer . counts_ ) print ( 'boundaries' , myAgglomerativeBucketer . boundaries_ ) counts [1000 50] boundaries [-2.71525699097944, 4.582406874429196, 6.454492599188006] df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = myAgglomerativeBucketer . boundaries_ , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax ) Note that the SimpleBucketer strategy would just have created a split in the middle of the maximum and the minimum (at about 1.75). The QuantileBucketer strategy had created two bins with equal amount of elements in it, resulting in a split at around 0. counts_agglomerative_simple , boundaries_agglomerative_simple = SimpleBucketer . simple_bins ( x_agglomerative , 2 ) df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = boundaries_agglomerative_simple , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax ) counts_agglomerative_quantile , boundaries_agglomerative_quantile = QuantileBucketer . quantile_bins ( x_agglomerative , 2 ) df = pd . DataFrame ({ 'x' : x_agglomerative }) df [ 'label' ] = pd . cut ( x_agglomerative , bins = boundaries_agglomerative_quantile , include_lowest = True ) fig , ax = plt . subplots () for label in df . label . unique (): df [ df . label == label ] . hist ( ax = ax )","title":"Binning by agglomerative clustering"},{"location":"tutorials/nb_binning.html#binning-with-decision-trees","text":"Binning with decision trees leverages the information of a binary feature or the binary target in order to create buckets that have a significantly different proportion of the binary feature/target. It works by fitting a tree on 1 feature only. It leverages the properties of the split finder algorithm in the decision tree. The splits are done to maximize the gini/entropy. The leaves approximate the optimal bins. The example below shows a distribution defined by a step function def make_step_function ( x ): if x < 4 : return 0.001 elif x < 6 : return 0.3 elif x < 8 : return 0.5 elif x < 9 : return 0.95 else : return 0.9999 x = np . arange ( 0 , 10 , 0.001 ) probs = [ make_step_function ( x_ ) for x_ in x ] y = np . array ([ 1 if np . random . rand () < prob else 0 for prob in probs ]) fig , ax = plt . subplots () ax2 = ax . twinx () ax . hist ( x [ y == 0 ], alpha = 0.15 ) ax . hist ( x [ y == 1 ], alpha = 0.15 ) ax2 . plot ( x , probs , color = 'black' ) plt . show () The light blue histogram indicates the distribution of class 0 ( y=0 ), while the light orange histogram indicates the distribution of class 1 ( y=1 ). The black line indicates the probability function that isused to assign class 0 or 1. In this toy example, it's a step function. # Try a tree bucketer tb = TreeBucketer ( inf_edges = True , max_depth = 4 , criterion = 'entropy' , min_samples_leaf = 400 , #Minimum number of entries in the bins min_impurity_decrease = 0.001 ) . fit ( x , y ) counts_tree , boundaries_tree = tb . counts_ , tb . boundaries_ df_tree = pd . DataFrame ({ 'x' : x , 'y' : y , 'probs' : probs }) df_tree [ 'label' ] = pd . cut ( x , bins = boundaries_tree , include_lowest = True ) # Try a quantile bucketer myQuantileBucketer = QuantileBucketer ( bin_count = 16 ) myQuantileBucketer . fit ( x ) q_boundaries = myQuantileBucketer . boundaries_ q_counts = myQuantileBucketer . counts_ df_q = pd . DataFrame ({ 'x' : x , 'y' : y , 'probs' : probs }) df_q [ 'label' ] = pd . cut ( x , bins = q_boundaries , include_lowest = True ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) for label in df_tree . label . unique (): df_tree [ df_tree . label == label ] . plot ( ax = ax [ 0 ], x = 'x' , y = 'probs' , legend = False ) ax [ 0 ] . scatter ( df_tree [ df_tree . label == label ][ 'x' ] . mean (), df_tree [ df_tree . label == label ][ 'y' ] . mean ()) ax [ 0 ] . set_title ( \"Tree bucketer\" ) for label in df_q . label . unique (): df_q [ df_q . label == label ] . plot ( ax = ax [ 1 ], x = 'x' , y = 'probs' , legend = False ) ax [ 1 ] . scatter ( df_q [ df_q . label == label ][ 'x' ] . mean (), df_q [ df_q . label == label ][ 'y' ] . mean ()) ax [ 1 ] . set_title ( \"Quantile bucketer\" ) print ( f \"counts by TreeBucketer: { counts_tree } \" ) print ( f \"counts by QuantileBucketer: { q_counts } \" ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 17985.87it/s] counts by TreeBucketer: [4000 1998 2001 936 1065] counts by QuantileBucketer: [625 625 625 625 625 625 625 625 625 625 625 625 625 625 625 625] Comparing the TreeBucketer and the QuantileBucketer (the dots compare the average distribution of class 1 in the bin): Each buckets obtained by the TreeBucketer follow the probability distribution (ie the entries in the bucket have the same probability of being class 1). On the contrary, the QuantileBucketer splits the values below 4 in 6 buckets, which all have the same probability of being class 1. Note also that the tree is grown with the maximum depth of 4, which potentially let's it grow up to 16 buckets ($2^4$). The learned tree is visualized below, whreere the splitting according to the step function is visualized clearly. from sklearn.tree import plot_tree fig , ax = plt . subplots ( figsize = ( 12 , 5 )) tre_out = plot_tree ( tb . tree , ax = ax )","title":"Binning with Decision Trees"},{"location":"tutorials/nb_distribution_statistics.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Univariate Distribution Similarity \u00b6 There are many situations when you want to perform univariate distribution comparison of a given feature, e.g. stability of the feature over different months. In order to do that, you can use statistical tests. In this tutorial we present how to easily do this using the DistributionStatistics class, and with the statistical tests directly. The available statistical tests in probatus.stat_tests are: - Epps-Singleton ('ES') - Kolmogorov-Smirnov statistic ('KS') - Population Stability Index ('PSI') - Shapiro-Wilk based difference statistic ('SW') - Anderson-Darling TS ('AD') You can perform all these tests using a convenient wrapper class called DistributionStatistics . In this tutorial we will focus on how to perform two useful tests: Population Stability Index (widely applied in banking industry) and Kolmogorov-Smirnov. Setup \u00b6 % load_ext autoreload % autoreload 2 import numpy as np import pandas as pd import matplotlib.pyplot as plt from probatus.binning import AgglomerativeBucketer , SimpleBucketer , QuantileBucketer from probatus.stat_tests import DistributionStatistics , psi , ks Let's define some test distributions and visualize them. For these examples, we will use a normal distribution and a shifted version of the same distribution. counts = 1000 np . random . seed ( 0 ) d1 = pd . Series ( np . random . normal ( size = counts ), name = 'feature_1' ) d2 = pd . Series ( np . random . normal ( loc = 0.5 , size = counts ), name = 'feature_1' ) from probatus.utils.plots import plot_distributions_of_feature feature_distributions = [ d1 , d2 ] sample_names = [ 'expected' , 'actual' ] plot_distributions_of_feature ( feature_distributions , sample_names = sample_names , plot_perc_outliers_removed = 0.01 ) Binning - QuantileBucketer \u00b6 To visualize the data, we will bin the data using a quantile bucketer, available in the probatus.binning module. Binning is used by all the stats_tests in order to group observations. bins = 10 myBucketer = QuantileBucketer ( bins ) d1_bincounts = myBucketer . fit_compute ( d1 ) d2_bincounts = myBucketer . compute ( d2 ) print ( \"Bincounts for d1 and d2:\" ) print ( d1_bincounts ) print ( d2_bincounts ) Bincounts for d1 and d2: [100 100 100 100 100 100 100 100 100 100] [ 25 62 50 68 76 90 84 169 149 217] Let's plot the distribution for which we will calculate the statistics. plt . figure ( figsize = ( 20 , 5 )) plt . bar ( range ( 0 , len ( d1_bincounts )), d1_bincounts , label = 'd1: expected' ) plt . bar ( range ( 0 , len ( d2_bincounts )), d2_bincounts , label = 'd2: actual' , alpha = 0.5 ) plt . title ( 'PSI (bucketed)' , fontsize = 16 , fontweight = 'bold' ) plt . legend ( fontsize = 15 ) plt . show () By visualizing the bins, we can already notice that the distributions are different. Let's use the statistical test to prove that. PSI - Population Stability Index \u00b6 The population stability index ( Karakoulas, 2004 ) has long been used to evaluate distribution similarity in banking industry, while developing credit decision models. In probatus we have implemented the PSI according to Yurdakul 2018 , which derives a p-value, based on the hard to interpret PSI statistic. Using the p-value is a more reliable choice, because the banking industry-standard PSI critical values of 0.1 and 0.25 are unreliable heuristics because there is a strong dependency on sample sizes and number of bins. Aside from these heuristics, the PSI value is not easily interpretable in the context of common statistical frameworks (like a p-value or confidence levels). psi_value , p_value = psi ( d1_bincounts , d2_bincounts , verbose = True ) PSI = 0.32743036141828374 PSI: Critical values defined according to de facto industry standard: PSI > 0.25: Significant distribution change; investigate. PSI: Critical values defined according to Yurdakul (2018): 99.9% confident distributions have changed. Bssed on the above test, the distribution between the two samples significantly differ. Not only the PSI statistic is above the commonly used critical value, but also the p-value shows a very high confidence. PSI with DistributionStatistics \u00b6 Using DistributionStatistics class one can apply the above test, without the need to manually perform the binning. We initialize a DistributionStatistics instance with the desired test, binning_strategy (or choose \"default\" to choose the test's most appropriate binning strategy) and the number of bins. Then we start the test with the unbinned values as input. distribution_test = DistributionStatistics ( \"psi\" , binning_strategy = \"default\" , bin_count = 10 ) psi_value , p_value = distribution_test . compute ( d1 , d2 , verbose = True ) PSI = 0.32743036141828374 PSI: Critical values defined according to de facto industry standard: PSI > 0.25: Significant distribution change; investigate. PSI: Critical values defined according to Yurdakul (2018): 99.9% confident distributions have changed. KS: Kolmogorov-Smirnov with DistributionStatistics \u00b6 The Kolmogorov-Smirnov test compares two distributions by calculating the maximum difference of the two samples' distribution functions, as illustrated by the black arrow in the following figure. The KS test is available in probatus.stat_tests.ks . The main advantage of this method is its sensitivity to differences in both location and shape of the empirical cumulative distribution functions of the two samples. The main disadwantages are that: it works for continuous distributions (unless modified, e.g. see ( Jeng 2006 ), in large samples, small and unimportant differences can be statistically significant ( Taplin & Hunt 2019 ), and finally in small samples, large and important differences can be statistically insignificant ( Taplin & Hunt 2019 ). As before, using the test requires you to perform the binning beforehand k_value , p_value = ks ( d1 , d2 , verbose = True ) KS: pvalue = 2.104700973377179e-27 KS: Null hypothesis rejected with 99% confidence. Distributions very different. Again, we can also choose to combine the binning and the statistical test using the DistributionStatistics class. distribution_test = DistributionStatistics ( \"ks\" , binning_strategy = None ) ks_value , p_value = distribution_test . compute ( d1 , d2 , verbose = True ) KS: pvalue = 2.104700973377179e-27 KS: Null hypothesis rejected with 99% confidence. Distributions very different. AutoDist \u00b6 from probatus.stat_tests import AutoDist Multiple statistics can automatically be calculated using AutoDist . To show this, let's create two new dataframes with two features each. size , n_features = 100 , 2 df1 = pd . DataFrame ( np . random . normal ( size = ( size , n_features )), columns = [ f 'feat_ { x } ' for x in range ( n_features )]) df2 = pd . DataFrame ( np . random . normal ( size = ( size , n_features )), columns = [ f 'feat_ { x } ' for x in range ( n_features )]) We can now specify the statistical tests we want to perform and the binning strategies to perform. We can also set both of these variables to 'all' or binning strategies to 'default' to use the default binning strategy for every chosen statistical test. statistical_tests = [ \"KS\" , \"PSI\" ] binning_strategies = \"default\" Let's compute the statistics and their p_values: myAutoDist = AutoDist ( statistical_tests = statistical_tests , binning_strategies = binning_strategies , bin_count = 10 ) myAutoDist . compute ( df1 , df2 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 140.75it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column p_value_KS_no_bucketing_0 p_value_PSI_quantilebucketer_10 statistic_KS_no_bucketing_0 statistic_PSI_quantilebucketer_10 0 feat_0 0.815415 0.556756 0.09 0.192113 1 feat_1 0.281942 0.989078 0.14 0.374575","title":"Univariate Sample Similarity"},{"location":"tutorials/nb_distribution_statistics.html#univariate-distribution-similarity","text":"There are many situations when you want to perform univariate distribution comparison of a given feature, e.g. stability of the feature over different months. In order to do that, you can use statistical tests. In this tutorial we present how to easily do this using the DistributionStatistics class, and with the statistical tests directly. The available statistical tests in probatus.stat_tests are: - Epps-Singleton ('ES') - Kolmogorov-Smirnov statistic ('KS') - Population Stability Index ('PSI') - Shapiro-Wilk based difference statistic ('SW') - Anderson-Darling TS ('AD') You can perform all these tests using a convenient wrapper class called DistributionStatistics . In this tutorial we will focus on how to perform two useful tests: Population Stability Index (widely applied in banking industry) and Kolmogorov-Smirnov.","title":"Univariate Distribution Similarity"},{"location":"tutorials/nb_distribution_statistics.html#setup","text":"% load_ext autoreload % autoreload 2 import numpy as np import pandas as pd import matplotlib.pyplot as plt from probatus.binning import AgglomerativeBucketer , SimpleBucketer , QuantileBucketer from probatus.stat_tests import DistributionStatistics , psi , ks Let's define some test distributions and visualize them. For these examples, we will use a normal distribution and a shifted version of the same distribution. counts = 1000 np . random . seed ( 0 ) d1 = pd . Series ( np . random . normal ( size = counts ), name = 'feature_1' ) d2 = pd . Series ( np . random . normal ( loc = 0.5 , size = counts ), name = 'feature_1' ) from probatus.utils.plots import plot_distributions_of_feature feature_distributions = [ d1 , d2 ] sample_names = [ 'expected' , 'actual' ] plot_distributions_of_feature ( feature_distributions , sample_names = sample_names , plot_perc_outliers_removed = 0.01 )","title":"Setup"},{"location":"tutorials/nb_distribution_statistics.html#binning-quantilebucketer","text":"To visualize the data, we will bin the data using a quantile bucketer, available in the probatus.binning module. Binning is used by all the stats_tests in order to group observations. bins = 10 myBucketer = QuantileBucketer ( bins ) d1_bincounts = myBucketer . fit_compute ( d1 ) d2_bincounts = myBucketer . compute ( d2 ) print ( \"Bincounts for d1 and d2:\" ) print ( d1_bincounts ) print ( d2_bincounts ) Bincounts for d1 and d2: [100 100 100 100 100 100 100 100 100 100] [ 25 62 50 68 76 90 84 169 149 217] Let's plot the distribution for which we will calculate the statistics. plt . figure ( figsize = ( 20 , 5 )) plt . bar ( range ( 0 , len ( d1_bincounts )), d1_bincounts , label = 'd1: expected' ) plt . bar ( range ( 0 , len ( d2_bincounts )), d2_bincounts , label = 'd2: actual' , alpha = 0.5 ) plt . title ( 'PSI (bucketed)' , fontsize = 16 , fontweight = 'bold' ) plt . legend ( fontsize = 15 ) plt . show () By visualizing the bins, we can already notice that the distributions are different. Let's use the statistical test to prove that.","title":"Binning - QuantileBucketer"},{"location":"tutorials/nb_distribution_statistics.html#psi-population-stability-index","text":"The population stability index ( Karakoulas, 2004 ) has long been used to evaluate distribution similarity in banking industry, while developing credit decision models. In probatus we have implemented the PSI according to Yurdakul 2018 , which derives a p-value, based on the hard to interpret PSI statistic. Using the p-value is a more reliable choice, because the banking industry-standard PSI critical values of 0.1 and 0.25 are unreliable heuristics because there is a strong dependency on sample sizes and number of bins. Aside from these heuristics, the PSI value is not easily interpretable in the context of common statistical frameworks (like a p-value or confidence levels). psi_value , p_value = psi ( d1_bincounts , d2_bincounts , verbose = True ) PSI = 0.32743036141828374 PSI: Critical values defined according to de facto industry standard: PSI > 0.25: Significant distribution change; investigate. PSI: Critical values defined according to Yurdakul (2018): 99.9% confident distributions have changed. Bssed on the above test, the distribution between the two samples significantly differ. Not only the PSI statistic is above the commonly used critical value, but also the p-value shows a very high confidence.","title":"PSI - Population Stability Index"},{"location":"tutorials/nb_distribution_statistics.html#psi-with-distributionstatistics","text":"Using DistributionStatistics class one can apply the above test, without the need to manually perform the binning. We initialize a DistributionStatistics instance with the desired test, binning_strategy (or choose \"default\" to choose the test's most appropriate binning strategy) and the number of bins. Then we start the test with the unbinned values as input. distribution_test = DistributionStatistics ( \"psi\" , binning_strategy = \"default\" , bin_count = 10 ) psi_value , p_value = distribution_test . compute ( d1 , d2 , verbose = True ) PSI = 0.32743036141828374 PSI: Critical values defined according to de facto industry standard: PSI > 0.25: Significant distribution change; investigate. PSI: Critical values defined according to Yurdakul (2018): 99.9% confident distributions have changed.","title":"PSI with DistributionStatistics"},{"location":"tutorials/nb_distribution_statistics.html#ks-kolmogorov-smirnov-with-distributionstatistics","text":"The Kolmogorov-Smirnov test compares two distributions by calculating the maximum difference of the two samples' distribution functions, as illustrated by the black arrow in the following figure. The KS test is available in probatus.stat_tests.ks . The main advantage of this method is its sensitivity to differences in both location and shape of the empirical cumulative distribution functions of the two samples. The main disadwantages are that: it works for continuous distributions (unless modified, e.g. see ( Jeng 2006 ), in large samples, small and unimportant differences can be statistically significant ( Taplin & Hunt 2019 ), and finally in small samples, large and important differences can be statistically insignificant ( Taplin & Hunt 2019 ). As before, using the test requires you to perform the binning beforehand k_value , p_value = ks ( d1 , d2 , verbose = True ) KS: pvalue = 2.104700973377179e-27 KS: Null hypothesis rejected with 99% confidence. Distributions very different. Again, we can also choose to combine the binning and the statistical test using the DistributionStatistics class. distribution_test = DistributionStatistics ( \"ks\" , binning_strategy = None ) ks_value , p_value = distribution_test . compute ( d1 , d2 , verbose = True ) KS: pvalue = 2.104700973377179e-27 KS: Null hypothesis rejected with 99% confidence. Distributions very different.","title":"KS: Kolmogorov-Smirnov with DistributionStatistics"},{"location":"tutorials/nb_distribution_statistics.html#autodist","text":"from probatus.stat_tests import AutoDist Multiple statistics can automatically be calculated using AutoDist . To show this, let's create two new dataframes with two features each. size , n_features = 100 , 2 df1 = pd . DataFrame ( np . random . normal ( size = ( size , n_features )), columns = [ f 'feat_ { x } ' for x in range ( n_features )]) df2 = pd . DataFrame ( np . random . normal ( size = ( size , n_features )), columns = [ f 'feat_ { x } ' for x in range ( n_features )]) We can now specify the statistical tests we want to perform and the binning strategies to perform. We can also set both of these variables to 'all' or binning strategies to 'default' to use the default binning strategy for every chosen statistical test. statistical_tests = [ \"KS\" , \"PSI\" ] binning_strategies = \"default\" Let's compute the statistics and their p_values: myAutoDist = AutoDist ( statistical_tests = statistical_tests , binning_strategies = binning_strategies , bin_count = 10 ) myAutoDist . compute ( df1 , df2 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 140.75it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column p_value_KS_no_bucketing_0 p_value_PSI_quantilebucketer_10 statistic_KS_no_bucketing_0 statistic_PSI_quantilebucketer_10 0 feat_0 0.815415 0.556756 0.09 0.192113 1 feat_1 0.281942 0.989078 0.14 0.374575","title":"AutoDist"},{"location":"tutorials/nb_metric_volatility.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Metric Volatility Estimation \u00b6 The estimation of AUC of your model could be influenced by, for instance, how you split your data. If other random seed was used, your AUC could be 3% lower. In order to understand how stable your model evaluation is, and what performance you can expect on average from your model, you can use the metric_volatility module. Setup \u00b6 from probatus.metric_volatility import TrainTestVolatility , SplitSeedVolatility , BootstrappedVolatility from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier X , y = make_classification ( n_samples = 1000 , n_features = 10 , random_state = 1 ) clf = RandomForestClassifier ( n_estimators = 2 , max_depth = 2 , random_state = 0 ) TrainTestVolatility \u00b6 The class that provides a wide functionality for experimentation with metric volatility is TrainTestVolatility. Please refer to the API reference for full description of available parameters. By default, the class performs a simple experiment, in which it computes the metrics on data split into train and test set with different random seed at each iteration. Having computed the mean and standard deviation of the metrics, you can analyse the impact of random seed setting on your results and get a better estimation of performance on this dataset. When you run the fit() and compute() or fit_compute() , the experiment described above is performed and the report is returned. The train_mean and and test_mean show an averaged performance of the model, and delta_mean indicates on average how much the model overfits on the data. By looking at train_std , test_std , delta_std , you can assess the stability of these scores overall. High volatility on some of the splits, may indicate the need to change the sizes of these splits or make changes to the model. # Basic functionality volatility = TrainTestVolatility ( clf ) volatility . fit_compute ( X , y ) var element = $('#3ba4c3a1-6ccf-4808-a137-a7eee2543e12'); {\"model_id\": \"97439d13c4bd456aa9f66faa21bcebb3\", \"version_major\": 2, \"version_minor\": 0} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std roc_auc 0.841891 0.035806 0.824239 0.044371 0.017652 0.030296 The results above show quite unstable results, due to high train_std and test_std . However, the delta_mean is relatively, which indicates that the model might underfit and increasing the complexity of the model could bring improvements to the results. One can also present the distributions of train, test and deltas for each metric. The plots allows for a sensitivity analysis of the volatility . plot () In order to simplify the use of this class for the user, two convenience classes have been created to perform the main types of analyses with less parameters to set by the user. SplitSeedVolatility \u00b6 The estimation of volatility is done in the same way as the default analysis described in TrainTestVolatility. The main advantage of using that class is a lower number of parameters to set. volatility = SplitSeedVolatility ( clf , iterations = 500 , test_prc = 0.5 ) volatility . fit_compute ( X , y ) var element = $('#d8f6322d-07dd-4c1b-8852-0468ced0c8a4'); {\"model_id\": \"c3ef1fa4a2e344ecab808bbbd675d648\", \"version_major\": 2, \"version_minor\": 0} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std roc_auc 0.841218 0.037721 0.816639 0.043795 0.024578 0.026436 BootstrappedVolatility \u00b6 This class allows to perform a different experiment. At each iteration, the train test split is the same, however, the samples in both splits are bootstrapped (sampled with replacement). Thus, some of the samples might be omitted, and some will be used multiple times in a given run. With this experiment, you can estimate an average performance for a specific train test split, as well as indicate how volatile the scores are towards certain samples within your splits. Moreover, you can experiment with the amount of data sampled in each split, to tweak the test split size. volatility = BootstrappedVolatility ( clf , metrics = [ 'accuracy' , 'roc_auc' ]) volatility . fit_compute ( X , y ) var element = $('#10819025-323f-4be9-97d1-a905288033f9'); {\"model_id\": \"e31bb7035a9e4e7f97232ba2308c258f\", \"version_major\": 2, \"version_minor\": 0} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std accuracy 0.804475 0.041196 0.800232 0.044409 0.004243 0.036769 roc_auc 0.834054 0.043542 0.830157 0.047244 0.003897 0.037314","title":"Model Metrics Volatility"},{"location":"tutorials/nb_metric_volatility.html#metric-volatility-estimation","text":"The estimation of AUC of your model could be influenced by, for instance, how you split your data. If other random seed was used, your AUC could be 3% lower. In order to understand how stable your model evaluation is, and what performance you can expect on average from your model, you can use the metric_volatility module.","title":"Metric Volatility Estimation"},{"location":"tutorials/nb_metric_volatility.html#setup","text":"from probatus.metric_volatility import TrainTestVolatility , SplitSeedVolatility , BootstrappedVolatility from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier X , y = make_classification ( n_samples = 1000 , n_features = 10 , random_state = 1 ) clf = RandomForestClassifier ( n_estimators = 2 , max_depth = 2 , random_state = 0 )","title":"Setup"},{"location":"tutorials/nb_metric_volatility.html#traintestvolatility","text":"The class that provides a wide functionality for experimentation with metric volatility is TrainTestVolatility. Please refer to the API reference for full description of available parameters. By default, the class performs a simple experiment, in which it computes the metrics on data split into train and test set with different random seed at each iteration. Having computed the mean and standard deviation of the metrics, you can analyse the impact of random seed setting on your results and get a better estimation of performance on this dataset. When you run the fit() and compute() or fit_compute() , the experiment described above is performed and the report is returned. The train_mean and and test_mean show an averaged performance of the model, and delta_mean indicates on average how much the model overfits on the data. By looking at train_std , test_std , delta_std , you can assess the stability of these scores overall. High volatility on some of the splits, may indicate the need to change the sizes of these splits or make changes to the model. # Basic functionality volatility = TrainTestVolatility ( clf ) volatility . fit_compute ( X , y ) var element = $('#3ba4c3a1-6ccf-4808-a137-a7eee2543e12'); {\"model_id\": \"97439d13c4bd456aa9f66faa21bcebb3\", \"version_major\": 2, \"version_minor\": 0} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std roc_auc 0.841891 0.035806 0.824239 0.044371 0.017652 0.030296 The results above show quite unstable results, due to high train_std and test_std . However, the delta_mean is relatively, which indicates that the model might underfit and increasing the complexity of the model could bring improvements to the results. One can also present the distributions of train, test and deltas for each metric. The plots allows for a sensitivity analysis of the volatility . plot () In order to simplify the use of this class for the user, two convenience classes have been created to perform the main types of analyses with less parameters to set by the user.","title":"TrainTestVolatility"},{"location":"tutorials/nb_metric_volatility.html#splitseedvolatility","text":"The estimation of volatility is done in the same way as the default analysis described in TrainTestVolatility. The main advantage of using that class is a lower number of parameters to set. volatility = SplitSeedVolatility ( clf , iterations = 500 , test_prc = 0.5 ) volatility . fit_compute ( X , y ) var element = $('#d8f6322d-07dd-4c1b-8852-0468ced0c8a4'); {\"model_id\": \"c3ef1fa4a2e344ecab808bbbd675d648\", \"version_major\": 2, \"version_minor\": 0} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std roc_auc 0.841218 0.037721 0.816639 0.043795 0.024578 0.026436","title":"SplitSeedVolatility"},{"location":"tutorials/nb_metric_volatility.html#bootstrappedvolatility","text":"This class allows to perform a different experiment. At each iteration, the train test split is the same, however, the samples in both splits are bootstrapped (sampled with replacement). Thus, some of the samples might be omitted, and some will be used multiple times in a given run. With this experiment, you can estimate an average performance for a specific train test split, as well as indicate how volatile the scores are towards certain samples within your splits. Moreover, you can experiment with the amount of data sampled in each split, to tweak the test split size. volatility = BootstrappedVolatility ( clf , metrics = [ 'accuracy' , 'roc_auc' ]) volatility . fit_compute ( X , y ) var element = $('#10819025-323f-4be9-97d1-a905288033f9'); {\"model_id\": \"e31bb7035a9e4e7f97232ba2308c258f\", \"version_major\": 2, \"version_minor\": 0} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train_mean train_std test_mean test_std delta_mean delta_std accuracy 0.804475 0.041196 0.800232 0.044409 0.004243 0.036769 roc_auc 0.834054 0.043542 0.830157 0.047244 0.003897 0.037314","title":"BootstrappedVolatility"},{"location":"tutorials/nb_sample_similarity.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Sample Similarity \u00b6 The goal of a Resemblance Model is understanding how different two samples are from a multivariate perspective. For instance, if you suspect that your Out-Of-Time Test set may have a different distribution than the In-Time Train set, you can detect that using the Resemblance Model. Having two samples X1 and X2 with the same set of features, one can analyse how well a model can recognize which dataset a randomly selected row comes from. The Resemblance model assigns label 0 to X1 dataset, and label 1 to X2 . Then, the data is shuffled and split into Train split ( X_train , y_train ) and Test split ( X_test , y_test ). The user provides a binary classifier that is then fitted on the Train split and evaluated on both Train and Test. Interpreting such model allows to understand, which features and interactions between them differ between these two samples. It is crucial that the model does not overfit or underfit, because interpretation of such model will lead to wrong conclusions. Therefore, you should try fitting the model with a couple different hyperparameter settings, and make sure that Train AUC is not significantly higher than Test AUC Once you have the final Resemblance Model, the Test AUC significantly above 0.5 indicates predictive power of the model, as well as the change in the distribution between X1 and X2 . The higher the Test AUC , the larger the difference between two datasets. Then you can use interpret the model, in order to understand the patterns that the model has learned. There are two classes in probatus that allow you to analyse, which features have changed between two samples: SHAPImportanceResemblance (Recommended) - Trains a Resemblance model based on a tree classifier , then it uses SHAP library to analyse the differences in features between the two samples. The main advantage of using this method is its high speed, better understanding of the relations in the data and handling of categorical features and missing values. PermutationImportanceResemblance - Trains a Resemblance model for any provided classifier , and uses Permutation Importance to analyse, which features the model relies on. It is significantly slower, and requires preprocessing of the data before training the resemblance model. Setup \u00b6 from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier import numpy as np from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import pandas as pd # Prepare two samples feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] X1 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 )[ 0 ], columns = feature_names ) X2 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , shift = 0.5 , random_state = 0 )[ 0 ], columns = feature_names ) # Prepare model clf = RandomForestClassifier ( n_estimators = 100 , max_depth = 2 , random_state = 0 ) SHAP Importance Resemblance Model for Tree models \u00b6 Below you can see an example of how to use the model: from probatus.sample_similarity import SHAPImportanceResemblance rm = SHAPImportanceResemblance ( clf ) feature_importance , train_auc , test_auc = rm . fit_compute ( X1 , X2 , column_names = feature_names , return_auc = True ) display ( feature_importance ) Finished model training: Train AUC 0.825, Test AUC 0.781 Train AUC > Test AUC, which might indicate an overfit. Strong overfit might lead to misleading conclusions when analysing feature importance. Consider retraining with more regularization applied to the model. Shap values are related to the output probabilities of class 1 for this model, instead of log odds. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_abs_shap_value mean_shap_value f1 0.072373 -0.006695 f3 0.065888 0.003465 f2 0.035037 0.002740 f4 0.019859 -0.003436 By looking into the above results, one can conclude that the two samples significantly differ, since the Test AUC of the model is very high. The table shows the mean absolute shap values and mean shap values for the model's features: Mean Absolute SHAP Values provide insights about overall SHAP feature importance . Mean SHAP Values show in which direction on average the feature influences the prediction. Negative value indicates 0 class, and positive indicates 1 class. Below, the SHAP feature importance is plotted ax = rm . plot () In order to get more insights of the change in underlying relations in the data, let's plot a dot summary plot. ax = rm . plot ( plot_type = 'dot' ) We can see that second sample have higher values in all the features. Permutation Importance Resemblance Model \u00b6 Below we show the example on how to use the PermutationImportanceResemblance from probatus.sample_similarity import PermutationImportanceResemblance perm = PermutationImportanceResemblance ( clf ) feature_importance , train_auc , test_auc = perm . fit_compute ( X1 , X2 , column_names = feature_names , return_auc = True ) display ( feature_importance ) Finished model training: Train AUC 0.825, Test AUC 0.781 Train AUC > Test AUC, which might indicate an overfit. Strong overfit might lead to misleading conclusions when analysing feature importance. Consider retraining with more regularization applied to the model. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_importance std_importance f3 0.109280 0.012875 f1 0.090757 0.018341 f2 0.034103 0.010083 f4 0.006469 0.006438 Same as before, we can get more insights into the importance of the features. However, now we can also analyse standard deviation of the permutation importance. High std might indicate that permutation of this feature has a higher or lower impact only in part of the available samples, while low std, indicates a consistent effect. ax = perm . plot () Visualize the difference in the most important feature \u00b6 We can also use the utils to provide more insights into the feature distribution difference in the two samples. from probatus.utils.plots import plot_distributions_of_feature feature_distributions = [ X1 [ 'f3' ], X2 [ 'f3' ]] plot_distributions_of_feature ( feature_distributions , plot_perc_outliers_removed = 0.01 )","title":"Multivariate Sample Similarity"},{"location":"tutorials/nb_sample_similarity.html#sample-similarity","text":"The goal of a Resemblance Model is understanding how different two samples are from a multivariate perspective. For instance, if you suspect that your Out-Of-Time Test set may have a different distribution than the In-Time Train set, you can detect that using the Resemblance Model. Having two samples X1 and X2 with the same set of features, one can analyse how well a model can recognize which dataset a randomly selected row comes from. The Resemblance model assigns label 0 to X1 dataset, and label 1 to X2 . Then, the data is shuffled and split into Train split ( X_train , y_train ) and Test split ( X_test , y_test ). The user provides a binary classifier that is then fitted on the Train split and evaluated on both Train and Test. Interpreting such model allows to understand, which features and interactions between them differ between these two samples. It is crucial that the model does not overfit or underfit, because interpretation of such model will lead to wrong conclusions. Therefore, you should try fitting the model with a couple different hyperparameter settings, and make sure that Train AUC is not significantly higher than Test AUC Once you have the final Resemblance Model, the Test AUC significantly above 0.5 indicates predictive power of the model, as well as the change in the distribution between X1 and X2 . The higher the Test AUC , the larger the difference between two datasets. Then you can use interpret the model, in order to understand the patterns that the model has learned. There are two classes in probatus that allow you to analyse, which features have changed between two samples: SHAPImportanceResemblance (Recommended) - Trains a Resemblance model based on a tree classifier , then it uses SHAP library to analyse the differences in features between the two samples. The main advantage of using this method is its high speed, better understanding of the relations in the data and handling of categorical features and missing values. PermutationImportanceResemblance - Trains a Resemblance model for any provided classifier , and uses Permutation Importance to analyse, which features the model relies on. It is significantly slower, and requires preprocessing of the data before training the resemblance model.","title":"Sample Similarity"},{"location":"tutorials/nb_sample_similarity.html#setup","text":"from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier import numpy as np from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import pandas as pd # Prepare two samples feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] X1 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 )[ 0 ], columns = feature_names ) X2 = pd . DataFrame ( make_classification ( n_samples = 1000 , n_features = 4 , shift = 0.5 , random_state = 0 )[ 0 ], columns = feature_names ) # Prepare model clf = RandomForestClassifier ( n_estimators = 100 , max_depth = 2 , random_state = 0 )","title":"Setup"},{"location":"tutorials/nb_sample_similarity.html#shap-importance-resemblance-model-for-tree-models","text":"Below you can see an example of how to use the model: from probatus.sample_similarity import SHAPImportanceResemblance rm = SHAPImportanceResemblance ( clf ) feature_importance , train_auc , test_auc = rm . fit_compute ( X1 , X2 , column_names = feature_names , return_auc = True ) display ( feature_importance ) Finished model training: Train AUC 0.825, Test AUC 0.781 Train AUC > Test AUC, which might indicate an overfit. Strong overfit might lead to misleading conclusions when analysing feature importance. Consider retraining with more regularization applied to the model. Shap values are related to the output probabilities of class 1 for this model, instead of log odds. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_abs_shap_value mean_shap_value f1 0.072373 -0.006695 f3 0.065888 0.003465 f2 0.035037 0.002740 f4 0.019859 -0.003436 By looking into the above results, one can conclude that the two samples significantly differ, since the Test AUC of the model is very high. The table shows the mean absolute shap values and mean shap values for the model's features: Mean Absolute SHAP Values provide insights about overall SHAP feature importance . Mean SHAP Values show in which direction on average the feature influences the prediction. Negative value indicates 0 class, and positive indicates 1 class. Below, the SHAP feature importance is plotted ax = rm . plot () In order to get more insights of the change in underlying relations in the data, let's plot a dot summary plot. ax = rm . plot ( plot_type = 'dot' ) We can see that second sample have higher values in all the features.","title":"SHAP Importance Resemblance Model for Tree models"},{"location":"tutorials/nb_sample_similarity.html#permutation-importance-resemblance-model","text":"Below we show the example on how to use the PermutationImportanceResemblance from probatus.sample_similarity import PermutationImportanceResemblance perm = PermutationImportanceResemblance ( clf ) feature_importance , train_auc , test_auc = perm . fit_compute ( X1 , X2 , column_names = feature_names , return_auc = True ) display ( feature_importance ) Finished model training: Train AUC 0.825, Test AUC 0.781 Train AUC > Test AUC, which might indicate an overfit. Strong overfit might lead to misleading conclusions when analysing feature importance. Consider retraining with more regularization applied to the model. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_importance std_importance f3 0.109280 0.012875 f1 0.090757 0.018341 f2 0.034103 0.010083 f4 0.006469 0.006438 Same as before, we can get more insights into the importance of the features. However, now we can also analyse standard deviation of the permutation importance. High std might indicate that permutation of this feature has a higher or lower impact only in part of the available samples, while low std, indicates a consistent effect. ax = perm . plot ()","title":"Permutation Importance Resemblance Model"},{"location":"tutorials/nb_sample_similarity.html#visualize-the-difference-in-the-most-important-feature","text":"We can also use the utils to provide more insights into the feature distribution difference in the two samples. from probatus.utils.plots import plot_distributions_of_feature feature_distributions = [ X1 [ 'f3' ], X2 [ 'f3' ]] plot_distributions_of_feature ( feature_distributions , plot_perc_outliers_removed = 0.01 )","title":"Visualize the difference in the most important feature"},{"location":"tutorials/nb_shap_dependence.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Shap dependence \u00b6 This notebook illustrates the use of the interpret.shap_dependence module. The shap_dependence module can be used to explain the module using shap values. Imports \u00b6 First let's import some dependencies and set some settings: import warnings import pandas as pd from sklearn.ensemble import RandomForestClassifier from probatus.interpret.shap_dependence import TreeDependencePlotter warnings . filterwarnings ( 'ignore' ) Data preparation \u00b6 Now let's load a sample dataset # Download and read the dataset iris = pd . read_csv ( 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv' ) # Change the problem in a binary classification problem: 'setosa' or not. iris [ 'species' ] = iris [ 'species' ] . apply ( lambda x : 1 if x == 'setosa' else 0 ) iris = iris . rename ( columns = { 'species' : 'setosa' }) # Extract the relevant features features = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ] X = iris [ features ] y = iris [ 'setosa' ] # Show the dataset iris . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width setosa 0 5.1 3.5 1.4 0.2 1 1 4.9 3.0 1.4 0.2 1 2 4.7 3.2 1.3 0.2 1 3 4.6 3.1 1.5 0.2 1 4 5.0 3.6 1.4 0.2 1 and let's train a random forest classifier: clf = RandomForestClassifier () clf = clf . fit ( X , y ) Shap dependence \u00b6 First, we fit the TreeDependencePlotter tdp = TreeDependencePlotter ( clf ) . fit ( X , y ) Now, we can plot the shap dependence for a specific feature. The top plot shows the shap values for target_names = [ 'Not setosa' , 'Setosa' ] fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 10 , 7 ), target_names = target_names ) Plotting with outliers \u00b6 Say we alter the dataset artificially to include an outlier. The plotting will be disturbed, but we can still plot by setting the plotter's parameters. # Add an outlier X . at [ 0 , 'sepal_length' ] = 25 # Retrain the classifier and fit the plotter clf = RandomForestClassifier () . fit ( X , y ) tdp = TreeDependencePlotter ( clf ) . fit ( X , y ) # Plot the dependence plot. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 ), target_names = target_names ) As we can see, the plot is not readable, and the histogram makes little sense. Both issues can be solved: Removing the outliers \u00b6 The outliers can be removed by specifying the min_q and max_q parameters. The min_q parameter is the minimum quantile after which data points are considered, and max_q is the maximum quantile before which they are considered. In this case (with one large outlier), we can set the max_q parameter to 0.99 to only remove the largest point. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 ), max_q = 0.99 , target_names = target_names ) Working with skewed distributions \u00b6 The binning functionality of probatus can be used to plot a sensible histogram under different distributions of feature data. For example, using the 'quantile' setting (without removing the outlier, produces the following histogram. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 ), type_binning = 'agglomerative' , target_names = target_names )","title":"Nb shap dependence"},{"location":"tutorials/nb_shap_dependence.html#shap-dependence","text":"This notebook illustrates the use of the interpret.shap_dependence module. The shap_dependence module can be used to explain the module using shap values.","title":"Shap dependence"},{"location":"tutorials/nb_shap_dependence.html#imports","text":"First let's import some dependencies and set some settings: import warnings import pandas as pd from sklearn.ensemble import RandomForestClassifier from probatus.interpret.shap_dependence import TreeDependencePlotter warnings . filterwarnings ( 'ignore' )","title":"Imports"},{"location":"tutorials/nb_shap_dependence.html#data-preparation","text":"Now let's load a sample dataset # Download and read the dataset iris = pd . read_csv ( 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv' ) # Change the problem in a binary classification problem: 'setosa' or not. iris [ 'species' ] = iris [ 'species' ] . apply ( lambda x : 1 if x == 'setosa' else 0 ) iris = iris . rename ( columns = { 'species' : 'setosa' }) # Extract the relevant features features = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ] X = iris [ features ] y = iris [ 'setosa' ] # Show the dataset iris . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width setosa 0 5.1 3.5 1.4 0.2 1 1 4.9 3.0 1.4 0.2 1 2 4.7 3.2 1.3 0.2 1 3 4.6 3.1 1.5 0.2 1 4 5.0 3.6 1.4 0.2 1 and let's train a random forest classifier: clf = RandomForestClassifier () clf = clf . fit ( X , y )","title":"Data preparation"},{"location":"tutorials/nb_shap_dependence.html#shap-dependence_1","text":"First, we fit the TreeDependencePlotter tdp = TreeDependencePlotter ( clf ) . fit ( X , y ) Now, we can plot the shap dependence for a specific feature. The top plot shows the shap values for target_names = [ 'Not setosa' , 'Setosa' ] fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 10 , 7 ), target_names = target_names )","title":"Shap dependence"},{"location":"tutorials/nb_shap_dependence.html#plotting-with-outliers","text":"Say we alter the dataset artificially to include an outlier. The plotting will be disturbed, but we can still plot by setting the plotter's parameters. # Add an outlier X . at [ 0 , 'sepal_length' ] = 25 # Retrain the classifier and fit the plotter clf = RandomForestClassifier () . fit ( X , y ) tdp = TreeDependencePlotter ( clf ) . fit ( X , y ) # Plot the dependence plot. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 ), target_names = target_names ) As we can see, the plot is not readable, and the histogram makes little sense. Both issues can be solved:","title":"Plotting with outliers"},{"location":"tutorials/nb_shap_dependence.html#removing-the-outliers","text":"The outliers can be removed by specifying the min_q and max_q parameters. The min_q parameter is the minimum quantile after which data points are considered, and max_q is the maximum quantile before which they are considered. In this case (with one large outlier), we can set the max_q parameter to 0.99 to only remove the largest point. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 ), max_q = 0.99 , target_names = target_names )","title":"Removing the outliers"},{"location":"tutorials/nb_shap_dependence.html#working-with-skewed-distributions","text":"The binning functionality of probatus can be used to plot a sensible histogram under different distributions of feature data. For example, using the 'quantile' setting (without removing the outlier, produces the following histogram. fig = tdp . plot ( feature = 'sepal_length' , figsize = ( 7 , 4 ), type_binning = 'agglomerative' , target_names = target_names )","title":"Working with skewed distributions"},{"location":"tutorials/nb_shap_feature_elimination.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Recursive Feature Elimination using SHAP importance and CV \u00b6 Backwards Recursive Feature Elimination allows to efficiently reduce the number of features in your dataset, without losing the predictive power of the model. Probatus implements the following feature elimination routine for tree-based models : While any features left, iterate: 1. Optimize hyperparameters and fit a model for a given features set. You can either use GridSearchCV of RandomSearchCV from sklearn. 2. Remove n lowest SHAP importance features The functionality is similar to sklearn.feature_selection.RFECV , yet, it removes the lowest importance features based on SHAP features importance and optimizes the hyperparameters of the model at each round, to tune the model for each features set. The main advantages of using this routine are: - High computation speed compared compared to o. - Uses SHAP importance, which is one of the most reliable ways to estimate features importance. Unlike many other techniques, it works with missing values and categorical variables. - Uses Cross-validation to optimize hyperparameters for each features set. This way you can assess if removal of a given feature reduces the predictive power, or simply requires additional tuning of the model. The disadvantages are: - One needs to manually select how many features to keep at the end of the routine, based on how the performance of the model changes between rounds. - Removing lowest SHAP importance feature does not always translate to choosing the feature with lowest impact on model's performance. Shap importance illustrates how strongly a given feature affects the output of the model, while disregarding correctness of this prediction. - The current implementation works with tree-based models only, due to much lower speed of SHAP importance computation for other types of models. Setup the dataset \u00b6 In order to use the functionality, let's set up an example dataset with: - numerical features - 1 categorical feature - 1 static feature - 1 feature with missing values Probatus can handle such dataset if you use an appropriate binary model that handles the above issues in the data e.g. LightGBM. from probatus.feature_elimination import ShapRFECV from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split import numpy as np import pandas as pd import lightgbm feature_names = [ 'f1_categorical' , 'f2_missing' , 'f3_static' , 'f4' , 'f5' , 'f6' , 'f7' , 'f8' , 'f9' , 'f10' , 'f11' , 'f12' , 'f13' , 'f14' , 'f15' , 'f16' , 'f17' , 'f18' , 'f19' , 'f20' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , class_sep = 0.05 , n_informative = 6 , n_features = 20 , random_state = 0 , n_redundant = 10 , n_clusters_per_class = 1 ) X = pd . DataFrame ( X , columns = feature_names ) X [ 'f1_categorical' ] = X [ 'f1_categorical' ] . apply ( lambda x : str ( np . round ( x * 10 ))) X [ 'f2_missing' ] = X [ 'f2_missing' ] . apply ( lambda x : x if np . random . rand () < 0.8 else np . nan ) X [ 'f3_static' ] = 0 X . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f1_categorical f2_missing f3_static f4 f5 f6 f7 f8 f9 f10 f11 f12 f13 f14 f15 f16 f17 f18 f19 f20 0 34.0 NaN 0 0.037207 -0.211075 2.378358 0.474059 -0.580471 2.523367 1.265063 -0.698129 0.320310 0.373186 1.828942 -3.116881 -0.960251 -0.031511 -1.294803 0.115819 -2.727467 1 -25.0 0.772855 0 0.302824 0.729950 0.815054 1.157228 1.347449 -1.880812 0.222902 2.293246 0.286349 0.063909 0.036967 -1.016085 0.035576 -0.439491 0.823261 2.284586 0.674484 2 -7.0 1.350847 0 1.837895 -0.745689 0.327826 0.755258 1.499059 -1.987181 0.093551 0.734911 0.579087 0.216668 -1.523017 -1.940202 0.763642 -0.764415 1.879374 1.239255 0.767797 3 -53.0 4.559465 0 -1.277930 3.688404 -2.369522 0.927847 -0.155483 -2.434361 0.448693 2.021587 -0.609161 -1.699683 -2.938755 1.995516 -2.558037 -1.103081 3.084860 6.632615 0.754824 4 -10.0 1.505766 0 -0.576209 -0.790525 -0.585126 -0.499129 1.324540 -0.077793 0.352424 -1.728604 0.987153 -0.754924 0.257670 1.381853 1.751669 -1.043428 -0.848575 -3.581535 2.810744 X . dtypes . head () f1_categorical object f2_missing float64 f3_static int64 f4 float64 f5 float64 dtype: object Set up the model and model tuning \u00b6 You need to set up the model and it's hyperparameter optimization space/grid for the Backwards Feature Elimination. Probatus requires a tree-based binary classifier in order to speed up the computation of SHAP feature importance at each step. We recommend using LGBMClassifier, which by default handles missing values and categorical features. clf = lightgbm . LGBMClassifier ( max_depth = 5 , class_weight = 'balanced' ) param_grid = { 'n_estimators' : [ 5 , 7 , 10 ], 'num_leaves' : [ 3 , 5 , 7 ], } Apply ShapRFECV \u00b6 The ShapRFECV class in probatus implements the Backwards Recursive Feature Elimination, in which, at each step the Cross-Validation based hyperparameter optimization is performed, and the lowest SHAP importance features are removed. shap_elimination = ShapRFECV ( clf = clf , search_space = param_grid , search_schema = 'grid' , step = 0.2 , cv = 20 , scoring = 'roc_auc' , n_jobs = 3 ) report = shap_elimination . fit_compute ( X , y ) Removing static features ['f3_static']. The following variables contain missing values ['f2_missing']. Make sure to imputemissing or apply a model that handles them automatically. Changing dtype of ['f1_categorical'] from \"object\" to \"category\". Treating it as categorical variable. Make sure that the model handles categorical variables, or encode them first. Round: 1, Current number of features: 19, Current performance: Train 0.942 +/- 0.008, CV Validation 0.888 +/- 0.032. Num of features left: 16. Removed features at the end of the round: ['f6', 'f17', 'f4'] Round: 2, Current number of features: 16, Current performance: Train 0.942 +/- 0.008, CV Validation 0.887 +/- 0.032. Num of features left: 13. Removed features at the end of the round: ['f2_missing', 'f13', 'f7'] Round: 3, Current number of features: 13, Current performance: Train 0.942 +/- 0.008, CV Validation 0.887 +/- 0.032. Num of features left: 11. Removed features at the end of the round: ['f18', 'f10'] Round: 4, Current number of features: 11, Current performance: Train 0.941 +/- 0.008, CV Validation 0.883 +/- 0.039. Num of features left: 9. Removed features at the end of the round: ['f20', 'f12'] Round: 5, Current number of features: 9, Current performance: Train 0.938 +/- 0.006, CV Validation 0.884 +/- 0.042. Num of features left: 8. Removed features at the end of the round: ['f11'] Round: 6, Current number of features: 8, Current performance: Train 0.937 +/- 0.006, CV Validation 0.883 +/- 0.039. Num of features left: 7. Removed features at the end of the round: ['f14'] Round: 7, Current number of features: 7, Current performance: Train 0.93 +/- 0.008, CV Validation 0.878 +/- 0.038. Num of features left: 6. Removed features at the end of the round: ['f15'] Round: 8, Current number of features: 6, Current performance: Train 0.925 +/- 0.006, CV Validation 0.874 +/- 0.042. Num of features left: 5. Removed features at the end of the round: ['f8'] Round: 9, Current number of features: 5, Current performance: Train 0.916 +/- 0.006, CV Validation 0.869 +/- 0.044. Num of features left: 4. Removed features at the end of the round: ['f5'] Round: 10, Current number of features: 4, Current performance: Train 0.9 +/- 0.005, CV Validation 0.854 +/- 0.045. Num of features left: 3. Removed features at the end of the round: ['f1_categorical'] Round: 11, Current number of features: 3, Current performance: Train 0.889 +/- 0.006, CV Validation 0.86 +/- 0.038. Num of features left: 2. Removed features at the end of the round: ['f9'] Round: 12, Current number of features: 2, Current performance: Train 0.848 +/- 0.005, CV Validation 0.811 +/- 0.042. Num of features left: 1. Removed features at the end of the round: ['f19'] Round: 13, Current number of features: 1, Current performance: Train 0.746 +/- 0.004, CV Validation 0.723 +/- 0.05. Num of features left: 1. Removed features at the end of the round: [] At the end of the process, you can investigate the results for each iteration. report . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } num_features features_set eliminated_features train_metric_mean train_metric_std val_metric_mean val_metric_std param_n_estimators param_num_leaves 1 19 [f1_categorical, f2_missing, f4, f5, f6, f7, f... [f6, f17, f4] 0.942 0.008 0.888 0.032 10 7 2 16 [f16, f1_categorical, f15, f19, f8, f9, f10, f... [f2_missing, f13, f7] 0.942 0.008 0.887 0.032 10 7 3 13 [f16, f1_categorical, f15, f19, f8, f9, f10, f... [f18, f10] 0.942 0.008 0.887 0.032 10 7 4 11 [f16, f1_categorical, f12, f14, f15, f19, f8, ... [f20, f12] 0.941 0.008 0.883 0.039 10 7 5 9 [f16, f1_categorical, f14, f15, f19, f8, f9, f... [f11] 0.938 0.006 0.884 0.042 10 7 Once the process is completed, you can visualize the results. Let's investigate the performance plot. Clearly, the Validation score starts decreasing between 13 and 11 features, and has a sharper drop between 3 and 2 features. Therefore, you can safely remove 12 out of 20 features without loss of performance, or discard 17 out of 20 in order to keep only the essential ones and have a very lightweight model. performance_plot = shap_elimination . plot () The final features set for 3 features would be the following: shap_elimination . get_reduced_features_set ( num_features = 3 ) ['f16', 'f19', 'f9'] Now let's visualize how the optimal hyperparameter's values are affected by the feature elimination. For both parameters n_estimators and num_leaves , the optimal values for most of the rounds are equal to the maximum values of these parameters in the search grid. This indicates that you can possibly further improve the results by increasing the search ranges for these parameters. However, it is crucial to ensure that the model does not overfit. Currently, there is a visible gap between Train and Validation scores for features sets having more than 3 features, therefore, one should also consider further regularizing the model. param_plots = shap_elimination . plot ( 'parameter' , param_names = [ 'n_estimators' , 'num_leaves' ])","title":"Recursive Feature Elimination using SHAP importance and CV"},{"location":"tutorials/nb_shap_feature_elimination.html#recursive-feature-elimination-using-shap-importance-and-cv","text":"Backwards Recursive Feature Elimination allows to efficiently reduce the number of features in your dataset, without losing the predictive power of the model. Probatus implements the following feature elimination routine for tree-based models : While any features left, iterate: 1. Optimize hyperparameters and fit a model for a given features set. You can either use GridSearchCV of RandomSearchCV from sklearn. 2. Remove n lowest SHAP importance features The functionality is similar to sklearn.feature_selection.RFECV , yet, it removes the lowest importance features based on SHAP features importance and optimizes the hyperparameters of the model at each round, to tune the model for each features set. The main advantages of using this routine are: - High computation speed compared compared to o. - Uses SHAP importance, which is one of the most reliable ways to estimate features importance. Unlike many other techniques, it works with missing values and categorical variables. - Uses Cross-validation to optimize hyperparameters for each features set. This way you can assess if removal of a given feature reduces the predictive power, or simply requires additional tuning of the model. The disadvantages are: - One needs to manually select how many features to keep at the end of the routine, based on how the performance of the model changes between rounds. - Removing lowest SHAP importance feature does not always translate to choosing the feature with lowest impact on model's performance. Shap importance illustrates how strongly a given feature affects the output of the model, while disregarding correctness of this prediction. - The current implementation works with tree-based models only, due to much lower speed of SHAP importance computation for other types of models.","title":"Recursive Feature Elimination using SHAP importance and CV"},{"location":"tutorials/nb_shap_feature_elimination.html#setup-the-dataset","text":"In order to use the functionality, let's set up an example dataset with: - numerical features - 1 categorical feature - 1 static feature - 1 feature with missing values Probatus can handle such dataset if you use an appropriate binary model that handles the above issues in the data e.g. LightGBM. from probatus.feature_elimination import ShapRFECV from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split import numpy as np import pandas as pd import lightgbm feature_names = [ 'f1_categorical' , 'f2_missing' , 'f3_static' , 'f4' , 'f5' , 'f6' , 'f7' , 'f8' , 'f9' , 'f10' , 'f11' , 'f12' , 'f13' , 'f14' , 'f15' , 'f16' , 'f17' , 'f18' , 'f19' , 'f20' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , class_sep = 0.05 , n_informative = 6 , n_features = 20 , random_state = 0 , n_redundant = 10 , n_clusters_per_class = 1 ) X = pd . DataFrame ( X , columns = feature_names ) X [ 'f1_categorical' ] = X [ 'f1_categorical' ] . apply ( lambda x : str ( np . round ( x * 10 ))) X [ 'f2_missing' ] = X [ 'f2_missing' ] . apply ( lambda x : x if np . random . rand () < 0.8 else np . nan ) X [ 'f3_static' ] = 0 X . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f1_categorical f2_missing f3_static f4 f5 f6 f7 f8 f9 f10 f11 f12 f13 f14 f15 f16 f17 f18 f19 f20 0 34.0 NaN 0 0.037207 -0.211075 2.378358 0.474059 -0.580471 2.523367 1.265063 -0.698129 0.320310 0.373186 1.828942 -3.116881 -0.960251 -0.031511 -1.294803 0.115819 -2.727467 1 -25.0 0.772855 0 0.302824 0.729950 0.815054 1.157228 1.347449 -1.880812 0.222902 2.293246 0.286349 0.063909 0.036967 -1.016085 0.035576 -0.439491 0.823261 2.284586 0.674484 2 -7.0 1.350847 0 1.837895 -0.745689 0.327826 0.755258 1.499059 -1.987181 0.093551 0.734911 0.579087 0.216668 -1.523017 -1.940202 0.763642 -0.764415 1.879374 1.239255 0.767797 3 -53.0 4.559465 0 -1.277930 3.688404 -2.369522 0.927847 -0.155483 -2.434361 0.448693 2.021587 -0.609161 -1.699683 -2.938755 1.995516 -2.558037 -1.103081 3.084860 6.632615 0.754824 4 -10.0 1.505766 0 -0.576209 -0.790525 -0.585126 -0.499129 1.324540 -0.077793 0.352424 -1.728604 0.987153 -0.754924 0.257670 1.381853 1.751669 -1.043428 -0.848575 -3.581535 2.810744 X . dtypes . head () f1_categorical object f2_missing float64 f3_static int64 f4 float64 f5 float64 dtype: object","title":"Setup the dataset"},{"location":"tutorials/nb_shap_feature_elimination.html#set-up-the-model-and-model-tuning","text":"You need to set up the model and it's hyperparameter optimization space/grid for the Backwards Feature Elimination. Probatus requires a tree-based binary classifier in order to speed up the computation of SHAP feature importance at each step. We recommend using LGBMClassifier, which by default handles missing values and categorical features. clf = lightgbm . LGBMClassifier ( max_depth = 5 , class_weight = 'balanced' ) param_grid = { 'n_estimators' : [ 5 , 7 , 10 ], 'num_leaves' : [ 3 , 5 , 7 ], }","title":"Set up the model and model tuning"},{"location":"tutorials/nb_shap_feature_elimination.html#apply-shaprfecv","text":"The ShapRFECV class in probatus implements the Backwards Recursive Feature Elimination, in which, at each step the Cross-Validation based hyperparameter optimization is performed, and the lowest SHAP importance features are removed. shap_elimination = ShapRFECV ( clf = clf , search_space = param_grid , search_schema = 'grid' , step = 0.2 , cv = 20 , scoring = 'roc_auc' , n_jobs = 3 ) report = shap_elimination . fit_compute ( X , y ) Removing static features ['f3_static']. The following variables contain missing values ['f2_missing']. Make sure to imputemissing or apply a model that handles them automatically. Changing dtype of ['f1_categorical'] from \"object\" to \"category\". Treating it as categorical variable. Make sure that the model handles categorical variables, or encode them first. Round: 1, Current number of features: 19, Current performance: Train 0.942 +/- 0.008, CV Validation 0.888 +/- 0.032. Num of features left: 16. Removed features at the end of the round: ['f6', 'f17', 'f4'] Round: 2, Current number of features: 16, Current performance: Train 0.942 +/- 0.008, CV Validation 0.887 +/- 0.032. Num of features left: 13. Removed features at the end of the round: ['f2_missing', 'f13', 'f7'] Round: 3, Current number of features: 13, Current performance: Train 0.942 +/- 0.008, CV Validation 0.887 +/- 0.032. Num of features left: 11. Removed features at the end of the round: ['f18', 'f10'] Round: 4, Current number of features: 11, Current performance: Train 0.941 +/- 0.008, CV Validation 0.883 +/- 0.039. Num of features left: 9. Removed features at the end of the round: ['f20', 'f12'] Round: 5, Current number of features: 9, Current performance: Train 0.938 +/- 0.006, CV Validation 0.884 +/- 0.042. Num of features left: 8. Removed features at the end of the round: ['f11'] Round: 6, Current number of features: 8, Current performance: Train 0.937 +/- 0.006, CV Validation 0.883 +/- 0.039. Num of features left: 7. Removed features at the end of the round: ['f14'] Round: 7, Current number of features: 7, Current performance: Train 0.93 +/- 0.008, CV Validation 0.878 +/- 0.038. Num of features left: 6. Removed features at the end of the round: ['f15'] Round: 8, Current number of features: 6, Current performance: Train 0.925 +/- 0.006, CV Validation 0.874 +/- 0.042. Num of features left: 5. Removed features at the end of the round: ['f8'] Round: 9, Current number of features: 5, Current performance: Train 0.916 +/- 0.006, CV Validation 0.869 +/- 0.044. Num of features left: 4. Removed features at the end of the round: ['f5'] Round: 10, Current number of features: 4, Current performance: Train 0.9 +/- 0.005, CV Validation 0.854 +/- 0.045. Num of features left: 3. Removed features at the end of the round: ['f1_categorical'] Round: 11, Current number of features: 3, Current performance: Train 0.889 +/- 0.006, CV Validation 0.86 +/- 0.038. Num of features left: 2. Removed features at the end of the round: ['f9'] Round: 12, Current number of features: 2, Current performance: Train 0.848 +/- 0.005, CV Validation 0.811 +/- 0.042. Num of features left: 1. Removed features at the end of the round: ['f19'] Round: 13, Current number of features: 1, Current performance: Train 0.746 +/- 0.004, CV Validation 0.723 +/- 0.05. Num of features left: 1. Removed features at the end of the round: [] At the end of the process, you can investigate the results for each iteration. report . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } num_features features_set eliminated_features train_metric_mean train_metric_std val_metric_mean val_metric_std param_n_estimators param_num_leaves 1 19 [f1_categorical, f2_missing, f4, f5, f6, f7, f... [f6, f17, f4] 0.942 0.008 0.888 0.032 10 7 2 16 [f16, f1_categorical, f15, f19, f8, f9, f10, f... [f2_missing, f13, f7] 0.942 0.008 0.887 0.032 10 7 3 13 [f16, f1_categorical, f15, f19, f8, f9, f10, f... [f18, f10] 0.942 0.008 0.887 0.032 10 7 4 11 [f16, f1_categorical, f12, f14, f15, f19, f8, ... [f20, f12] 0.941 0.008 0.883 0.039 10 7 5 9 [f16, f1_categorical, f14, f15, f19, f8, f9, f... [f11] 0.938 0.006 0.884 0.042 10 7 Once the process is completed, you can visualize the results. Let's investigate the performance plot. Clearly, the Validation score starts decreasing between 13 and 11 features, and has a sharper drop between 3 and 2 features. Therefore, you can safely remove 12 out of 20 features without loss of performance, or discard 17 out of 20 in order to keep only the essential ones and have a very lightweight model. performance_plot = shap_elimination . plot () The final features set for 3 features would be the following: shap_elimination . get_reduced_features_set ( num_features = 3 ) ['f16', 'f19', 'f9'] Now let's visualize how the optimal hyperparameter's values are affected by the feature elimination. For both parameters n_estimators and num_leaves , the optimal values for most of the rounds are equal to the maximum values of these parameters in the search grid. This indicates that you can possibly further improve the results by increasing the search ranges for these parameters. However, it is crucial to ensure that the model does not overfit. Currently, there is a visible gap between Train and Validation scores for features sets having more than 3 features, therefore, one should also consider further regularizing the model. param_plots = shap_elimination . plot ( 'parameter' , param_names = [ 'n_estimators' , 'num_leaves' ])","title":"Apply ShapRFECV"},{"location":"tutorials/nb_shap_model_interpreter.html","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Tree Model Interpretation using SHAP \u00b6 There are many techniques, each with advantages and disadvantages that can be suitable for different situations. SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model, and is well-suited for exploring feature importances. Pros: Mathematical theory behind explanation of the model. Very wide application and ease of use. Explanations on single sample and global level, and a number of graphs that can be very easily computed and understood. Feature interactions taken into account by the method. High computation speed, especially for the TreeExplainer Cons: Documentation is often lacking. Different API when you use sklearn models e.g. RandomForestClassifier. Slow computation for some explainers e.g. KernelExplainer. Let's assume we want to analyse the following model: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split import numpy as np import pandas as pd feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 ) X = pd . DataFrame ( X , columns = feature_names ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Prepare and fit model. Remember about class_weight=\"balanced\" or an equivalent. clf = RandomForestClassifier ( class_weight = 'balanced' , n_estimators = 100 , max_depth = 2 , random_state = 0 ) clf = clf . fit ( X_train , y_train ) ShapModelInterpreter \u00b6 The ShapModelInterpreter class in Probatus, is a convenience wrapper class that allows to easily interpret the ML models. It implements multiple plotting methods, for tree-based models . Feature importance \u00b6 Firstly, lets compute the feature importance. In SHAP, feature importance can be computed using mean absolute shap value per feature on the test set. It can be interpreted as strength of influence of a given feature on output probabilities. from probatus.interpret import ShapModelInterpreter shap_interpreter = ShapModelInterpreter ( clf ) feature_importance = shap_interpreter . fit_compute ( X_train , X_test , y_train , y_test , approximate = False ) feature_importance Shap values are related to the output probabilities of class 1 for this model, instead of log odds. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_abs_shap_value mean_shap_value f1 0.312133 0.008057 f4 0.087395 -0.002653 f3 0.050422 -0.000949 f2 0.006234 -0.000982 The plot can be generated as follows: ax = shap_interpreter . plot ( 'importance' ) The AUC on train and test sets is illustrated in each plot, in order to illustrate if the model overfits. If you see that Test AUC is significantly lower than Train AUC, this is a sign that the model might be overfitting. In such case, the interpretation of the model might be misleading. In such situations we recommend retraining the model with more regularization. Summary plot \u00b6 Summary plot gives you more insights into how different feature values affect predictions made. This is a very crucial plot to make for every model. Each dot on the X-axis represents a sample in the data, and strongly it affected the prediction (together with predictions direction). The colours of the dots, present the values of that feature. For each model try to analyse this plot with Subject Matter Expert, in order to make sure that the relations that the model has learned make sense. ax = shap_interpreter . plot ( 'summary' ) Dependence Plot \u00b6 This plot allows you to understand how the model reacts for different feature values. You can plot it for each feature in your model, or at least the top 10 features. This can provide you with further insights on how the model uses each of the features. Moreover, one can detect anomalies, as well as the effect of the outliers on the model. As an addition, the bottom plot, presents the feature distribution histogram, and the target rate for different buckets within that feature values. This allows to further analyse how the feature correlates with the target variable. ax = shap_interpreter . plot ( 'dependence' , target_columns = [ 'f1' ]) Sample explanation \u00b6 In order to explain predictions for specific samples from your test set, you can use sample plot. For a given sample, the plot presents the force and direction of prediction shift that each feature value causes. ax = shap_interpreter . plot ( 'sample' , samples_index = [ 521 , 78 ]) Tips for using the interpreter \u00b6 Before using the ShapModelInterpreter consider the following tips: Fit your model using class weights set to 'balanced' setting, in order to make sure that Shap values are balanced between classes. If this setting is not available, try to balance the classes (under- or oversample before fitting the model). Make sure you do not underfit or overfit the model. Underfitting will cause only the most important relations in the data to be visible, while overfitting will present relationships that do not generalize. Perform feature selection process before fitting the final model. This way, many it will be easier to interpret the explanation. Moreover, highly-correlated features will affect the explanation less. Preferably use model that handles NaNs e.g. LightGBM or impute them before using SHAP. When imputing also extract a MissingIndicator to get insights into when NaNs are meaningful for the model. For categorical features either use a model that handles them e.g. LightGBM, or apply one-hot encoding. Keep in mind that with One-hot encoding the importance of categorical feature might be spread over multiple encoded features.","title":"Tree-based Model Interpretation with SHAP"},{"location":"tutorials/nb_shap_model_interpreter.html#tree-model-interpretation-using-shap","text":"There are many techniques, each with advantages and disadvantages that can be suitable for different situations. SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model, and is well-suited for exploring feature importances. Pros: Mathematical theory behind explanation of the model. Very wide application and ease of use. Explanations on single sample and global level, and a number of graphs that can be very easily computed and understood. Feature interactions taken into account by the method. High computation speed, especially for the TreeExplainer Cons: Documentation is often lacking. Different API when you use sklearn models e.g. RandomForestClassifier. Slow computation for some explainers e.g. KernelExplainer. Let's assume we want to analyse the following model: from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split import numpy as np import pandas as pd feature_names = [ 'f1' , 'f2' , 'f3' , 'f4' ] # Prepare two samples X , y = make_classification ( n_samples = 1000 , n_features = 4 , random_state = 0 ) X = pd . DataFrame ( X , columns = feature_names ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Prepare and fit model. Remember about class_weight=\"balanced\" or an equivalent. clf = RandomForestClassifier ( class_weight = 'balanced' , n_estimators = 100 , max_depth = 2 , random_state = 0 ) clf = clf . fit ( X_train , y_train )","title":"Tree Model Interpretation using SHAP"},{"location":"tutorials/nb_shap_model_interpreter.html#shapmodelinterpreter","text":"The ShapModelInterpreter class in Probatus, is a convenience wrapper class that allows to easily interpret the ML models. It implements multiple plotting methods, for tree-based models .","title":"ShapModelInterpreter"},{"location":"tutorials/nb_shap_model_interpreter.html#feature-importance","text":"Firstly, lets compute the feature importance. In SHAP, feature importance can be computed using mean absolute shap value per feature on the test set. It can be interpreted as strength of influence of a given feature on output probabilities. from probatus.interpret import ShapModelInterpreter shap_interpreter = ShapModelInterpreter ( clf ) feature_importance = shap_interpreter . fit_compute ( X_train , X_test , y_train , y_test , approximate = False ) feature_importance Shap values are related to the output probabilities of class 1 for this model, instead of log odds. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_abs_shap_value mean_shap_value f1 0.312133 0.008057 f4 0.087395 -0.002653 f3 0.050422 -0.000949 f2 0.006234 -0.000982 The plot can be generated as follows: ax = shap_interpreter . plot ( 'importance' ) The AUC on train and test sets is illustrated in each plot, in order to illustrate if the model overfits. If you see that Test AUC is significantly lower than Train AUC, this is a sign that the model might be overfitting. In such case, the interpretation of the model might be misleading. In such situations we recommend retraining the model with more regularization.","title":"Feature importance"},{"location":"tutorials/nb_shap_model_interpreter.html#summary-plot","text":"Summary plot gives you more insights into how different feature values affect predictions made. This is a very crucial plot to make for every model. Each dot on the X-axis represents a sample in the data, and strongly it affected the prediction (together with predictions direction). The colours of the dots, present the values of that feature. For each model try to analyse this plot with Subject Matter Expert, in order to make sure that the relations that the model has learned make sense. ax = shap_interpreter . plot ( 'summary' )","title":"Summary plot"},{"location":"tutorials/nb_shap_model_interpreter.html#dependence-plot","text":"This plot allows you to understand how the model reacts for different feature values. You can plot it for each feature in your model, or at least the top 10 features. This can provide you with further insights on how the model uses each of the features. Moreover, one can detect anomalies, as well as the effect of the outliers on the model. As an addition, the bottom plot, presents the feature distribution histogram, and the target rate for different buckets within that feature values. This allows to further analyse how the feature correlates with the target variable. ax = shap_interpreter . plot ( 'dependence' , target_columns = [ 'f1' ])","title":"Dependence Plot"},{"location":"tutorials/nb_shap_model_interpreter.html#sample-explanation","text":"In order to explain predictions for specific samples from your test set, you can use sample plot. For a given sample, the plot presents the force and direction of prediction shift that each feature value causes. ax = shap_interpreter . plot ( 'sample' , samples_index = [ 521 , 78 ])","title":"Sample explanation"},{"location":"tutorials/nb_shap_model_interpreter.html#tips-for-using-the-interpreter","text":"Before using the ShapModelInterpreter consider the following tips: Fit your model using class weights set to 'balanced' setting, in order to make sure that Shap values are balanced between classes. If this setting is not available, try to balance the classes (under- or oversample before fitting the model). Make sure you do not underfit or overfit the model. Underfitting will cause only the most important relations in the data to be visible, while overfitting will present relationships that do not generalize. Perform feature selection process before fitting the final model. This way, many it will be easier to interpret the explanation. Moreover, highly-correlated features will affect the explanation less. Preferably use model that handles NaNs e.g. LightGBM or impute them before using SHAP. When imputing also extract a MissingIndicator to get insights into when NaNs are meaningful for the model. For categorical features either use a model that handles them e.g. LightGBM, or apply one-hot encoding. Keep in mind that with One-hot encoding the importance of categorical feature might be spread over multiple encoded features.","title":"Tips for using the interpreter"}]}